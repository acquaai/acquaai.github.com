<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Acqua</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://acquaai.github.io/"/>
  <updated>2019-09-20T14:56:25.558Z</updated>
  <id>https://acquaai.github.io/</id>
  
  <author>
    <name>Acqua</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Install Portworx Cluster on Kubernetes(on-premise)</title>
    <link href="https://acquaai.github.io/2019/09/19/install-ha-portworx-k8s/"/>
    <id>https://acquaai.github.io/2019/09/19/install-ha-portworx-k8s/</id>
    <published>2019-09-19T14:22:13.000Z</published>
    <updated>2019-09-20T14:56:25.558Z</updated>
    
    <content type="html"><![CDATA[<h2 id="prepare-hosts-with-storage"><a class="markdownIt-Anchor" href="#prepare-hosts-with-storage"></a> Prepare hosts with storage</h2><p>Portworx (PX) requires at least some nodes in the cluster to have dedicated storage for Portworx to use. PX will then carve out virtual volumes from these storage pools. In this example, we use a 3.3T block device that exists on each node.</p><p>List block devices on worker nodes</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ lsblk</span><br><span class="line">NAME                              MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda                                 8:0    0  1.1T  0 disk </span><br><span class="line">├─sda1                              8:1    0  512M  0 part /boot/efi</span><br><span class="line">└─sda2                              8:2    0  1.1T  0 part </span><br><span class="line">  ├─w--192--31--16--16--vg-root   253:0    0  1.1T  0 lvm  /</span><br><span class="line">  └─w--192--31--16--16--vg-swap_1 253:1    0  976M  0 lvm  </span><br><span class="line">sdb                                 8:16   0  3.3T  0 disk</span><br></pre></td></tr></table></figure><p>Note the storage device sdb, which will be used by PX as one of it’s raw block disks. All the nodes in this setup have the sdb device.</p><a id="more"></a><h2 id="generate-the-specs"><a class="markdownIt-Anchor" href="#generate-the-specs"></a> Generate the specs</h2><ul><li><strong><a href="https://docs.portworx.com/portworx-install-with-kubernetes/on-premise/other/#" target="_blank" rel="noopener">Generating the Portworx specs</a></strong></li><li><strong><a href="https://github.com/acquaai/Kubernetes/blob/master/CSI/Portworx/spec.yaml" target="_blank" rel="noopener">My spec</a></strong></li></ul><p>Get Kubernetes Version:</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl version --short | awk -Fv <span class="string">'/Server Version: / &#123;print $3&#125;'</span></span><br></pre></td></tr></table></figure><p>Portworx will create and manage an internal key-value store (kvdb) cluster.</p><p>You can restrict the nodes that will run the key-value store by labelling your nodes with the label px/metadata-node=true. Only the nodes with the label will participate in the kvdb cluster. This allows you to use nodes with dedicated hardware for the key-value store.</p><p><strong>For example</strong>:</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl label nodes node1 node2 node3 px/metadata-node=<span class="literal">true</span>   (labeled)</span><br><span class="line"></span><br><span class="line">kubectl label nodes node1 node2 node3 px/metadata-node-   (remove label)</span><br></pre></td></tr></table></figure><h2 id="apply-the-specs"><a class="markdownIt-Anchor" href="#apply-the-specs"></a> Apply the specs</h2><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl create secret generic registrykey --from-file=.dockerconfigjson=.docker/config.json --<span class="built_in">type</span>=kubernetes.io/dockerconfigjson -n kube-system</span><br><span class="line">kubectl apply -f spec.yaml</span><br></pre></td></tr></table></figure><p><code>NOTE</code>: <strong>portworx/px-enterprise:2.1.5</strong> image is need to  be.</p><h2 id="monitor-the-portworx-pods"><a class="markdownIt-Anchor" href="#monitor-the-portworx-pods"></a> Monitor the portworx pods</h2><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -o wide -n kube-system -l name=portworx</span><br></pre></td></tr></table></figure><h2 id="monitor-portworx-cluster-status"><a class="markdownIt-Anchor" href="#monitor-portworx-cluster-status"></a> Monitor Portworx cluster status</h2><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PX_POD=$(kubectl get pods -l name=portworx -n kube-system -o jsonpath=<span class="string">'&#123;.items[0].metadata.name&#125;'</span>)</span><br><span class="line">kubectl <span class="built_in">exec</span> <span class="variable">$PX_POD</span> -n kube-system -- /opt/pwx/bin/pxctl status</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Status: PX is operational</span><br><span class="line">License: Trial (expires in 31 days)</span><br><span class="line">Node ID: 4078b64e-2e1c-4865-aeab-2da8afa86d43</span><br><span class="line">        IP: 192.31.16.16 </span><br><span class="line">        Local Storage Pool: 1 pool</span><br><span class="line">        POOL    IO_PRIORITY     RAID_LEVEL      USABLE  USED    STATUS  ZONE    REGION</span><br><span class="line">        0       LOW             raid0           3.3 TiB 10 GiB  Online  default default</span><br><span class="line">        Local Storage Devices: 1 device</span><br><span class="line">        Device  Path            Media Type              Size            Last-Scan</span><br><span class="line">        0:1     /dev/sdb2       STORAGE_MEDIUM_MAGNETIC 3.3 TiB         19 Sep 19 09:07 UTC</span><br><span class="line">        total                   -                       3.3 TiB</span><br><span class="line">        Journal Device: </span><br><span class="line">        1       /dev/sdb1       STORAGE_MEDIUM_MAGNETIC</span><br><span class="line">Cluster Summary</span><br><span class="line">        Cluster ID: px-cluster-d620cbe1-ad1c-4b44-be2e-043929a7080f</span><br><span class="line">        Cluster UUID: c50eb3ec-d4ba-416a-8a66-643efb0cb312</span><br><span class="line">        Scheduler: kubernetes</span><br><span class="line">        Nodes: 3 node(s) with storage (3 online)</span><br><span class="line">        IP              ID                                      SchedulerNodeName       StorageNode     Used    Capacity        Status  StorageStatus   Version         Kernel        OS</span><br><span class="line">        192.31.16.14    bb755d05-2c28-4a57-9126-4032ae29ad56    w-192-31-16-14          Yes             10 GiB  3.3 TiB         Online  Up              2.1.5.0-3b73452 4.15.0-58-generic      Ubuntu 18.04.3 LTS</span><br><span class="line">        192.31.16.16    4078b64e-2e1c-4865-aeab-2da8afa86d43    w-192-31-16-16          Yes             10 GiB  3.3 TiB         Online  Up (This node)  2.1.5.0-3b73452 4.15.0-58-generic      Ubuntu 18.04.3 LTS</span><br><span class="line">        192.31.16.15    00b67c6d-d4c1-4eab-9a71-fe07040fd98a    w-192-31-16-15          Yes             10 GiB  3.3 TiB         Online  Up              2.1.5.0-3b73452 4.15.0-58-generic      Ubuntu 18.04.3 LTS</span><br><span class="line">Global Storage Pool</span><br><span class="line">        Total Used      :  31 GiB</span><br><span class="line">        Total Capacity  :  9.8 TiB</span><br></pre></td></tr></table></figure><ul><li><a href="https://docs.portworx.com/reference/knowledge-base/px-licensing/" target="_blank" rel="noopener">Portworx Licensing</a></li><li><a href="https://github.com/portworx/px-dev" target="_blank" rel="noopener">PX-Developer</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;prepare-hosts-with-storage&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#prepare-hosts-with-storage&quot;&gt;&lt;/a&gt; Prepare hosts with storage&lt;/h2&gt;
&lt;p&gt;Portworx (PX) requires at least some nodes in the cluster to have dedicated storage for Portworx to use. PX will then carve out virtual volumes from these storage pools. In this example, we use a 3.3T block device that exists on each node.&lt;/p&gt;
&lt;p&gt;List block devices on worker nodes&lt;/p&gt;
&lt;figure class=&quot;highlight zsh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ lsblk&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;NAME                              MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sda                                 8:0    0  1.1T  0 disk &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;├─sda1                              8:1    0  512M  0 part /boot/efi&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;└─sda2                              8:2    0  1.1T  0 part &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  ├─w--192--31--16--16--vg-root   253:0    0  1.1T  0 lvm  /&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  └─w--192--31--16--16--vg-swap_1 253:1    0  976M  0 lvm  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sdb                                 8:16   0  3.3T  0 disk&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Note the storage device sdb, which will be used by PX as one of it’s raw block disks. All the nodes in this setup have the sdb device.&lt;/p&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>Using Ceph RBD for persistent storage</title>
    <link href="https://acquaai.github.io/2019/09/16/using-ceph-rbd-persistent-storage/"/>
    <id>https://acquaai.github.io/2019/09/16/using-ceph-rbd-persistent-storage/</id>
    <published>2019-09-16T14:34:02.000Z</published>
    <updated>2019-09-20T14:55:14.658Z</updated>
    
    <content type="html"><![CDATA[<h2 id="overview"><a class="markdownIt-Anchor" href="#overview"></a> Overview</h2><p>This topic provides a complete example of using an existing Ceph cluster for OKD persistent storage. It is assumed that a working Ceph cluster is already set up. If not, consult the <a href="https://access.redhat.com/products/red-hat-ceph-storage" target="_blank" rel="noopener">Overview of Red Hat Ceph Storage</a>.</p><p>Persistent Storage Using Ceph Rados Block Device provides an explanation of persistent volumes (PVs), persistent volume claims (PVCs), and using Ceph RBD as persistent storage.</p><h2 id="using-an-existing-ceph-cluster-for-persistent-storage"><a class="markdownIt-Anchor" href="#using-an-existing-ceph-cluster-for-persistent-storage"></a> Using an existing Ceph cluster for persistent storage</h2><a id="more"></a><ul><li>Install the ceph-common package same as ceph-cluster.</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">- name:</span> <span class="string">update</span> <span class="string">apt</span> <span class="string">cache</span></span><br><span class="line"><span class="attr">  apt:</span></span><br><span class="line">     <span class="string">update_cache=yes</span></span><br><span class="line"></span><br><span class="line"><span class="attr">- name:</span> <span class="string">install</span> <span class="string">ceph-common</span></span><br><span class="line"><span class="attr">  apt:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">ceph-common</span></span><br><span class="line"><span class="attr">  state:</span> <span class="string">present</span></span><br></pre></td></tr></table></figure><p><code>NOTE</code>: The <strong>ceph-common</strong> library must be installed on <strong>all nodes</strong>.</p><ul><li>Create the keyring for the client: <strong>(On Ceph Node)</strong></li></ul><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph auth get-or-create client.qemu mon <span class="string">'allow *'</span> osd <span class="string">'allow rwx pool=rbd'</span> -o /etc/ceph/ceph.client.qemu.keyring</span><br></pre></td></tr></table></figure><ul><li>Convert the keyring to base64: <strong>(On Ceph Node)</strong></li></ul><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ grep key /etc/ceph/ceph.client.qemu.keyring | awk <span class="string">'&#123;printf "%s", $NF&#125;'</span> | base64</span><br></pre></td></tr></table></figure><p><code>NOTE</code>: This base64 key is generated on one of the Ceph MON nodes using the <strong>ceph auth get-key client.admin | base64</strong> command, then copying the output and pasting it as the secret key’s value.</p><h2 id="create-storageclassdynamic-volume-provisioning-using-ceph-rbd"><a class="markdownIt-Anchor" href="#create-storageclassdynamic-volume-provisioning-using-ceph-rbd"></a> Create StorageClass(<code>Dynamic Volume Provisioning</code>) using Ceph RBD</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">elk</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># define admin secret on cluster level, create PV</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-secret-admin</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  key:</span> <span class="string">QVFEQUNWWmRmTUxBSkJBQXlSV88rUm11RzJSb0J2Tk9SVllSaGc9PQ==</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># define user secret on namespace level, create PVC</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-secret-qemu</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">elk</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  key:</span> <span class="string">QVFCNldYTmRtZ29iREJBQSt4dXorNHp0Wi33RWluR1J4U1hWcnc9PQ==</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-rbd</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">ceph.com/rbd</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line"><span class="attr">  monitors:</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.2</span><span class="string">:6789,192.168.0.3:6789</span></span><br><span class="line"><span class="attr">  pool:</span> <span class="string">rbd</span></span><br><span class="line"><span class="attr">  adminId:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">  adminSecretNamespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">  adminSecretName:</span> <span class="string">ceph-secret-admin</span></span><br><span class="line"><span class="attr">  userId:</span> <span class="string">qemu</span></span><br><span class="line"><span class="attr">  userSecretName:</span> <span class="string">ceph-secret-qemu</span></span><br><span class="line"><span class="attr">  userSecretNamespace:</span> <span class="string">elk</span></span><br><span class="line"><span class="attr">  fsType:</span> <span class="string">ext4</span></span><br><span class="line"><span class="attr">  imageFormat:</span> <span class="string">"2"</span></span><br><span class="line"><span class="attr">  imageFeatures:</span> <span class="string">"layering"</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># StatefulSet</span></span><br><span class="line"><span class="string">......</span></span><br><span class="line"><span class="attr">  volumeClaimTemplates:</span></span><br><span class="line"><span class="attr">  - metadata:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">elasticsearch</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      accessModes:</span> <span class="string">[</span> <span class="string">"ReadWriteOnce"</span> <span class="string">]</span></span><br><span class="line"><span class="attr">      resources:</span></span><br><span class="line"><span class="attr">        requests:</span></span><br><span class="line"><span class="attr">          storage:</span> <span class="number">10</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">      storageClassName:</span> <span class="string">ceph-rbd</span></span><br></pre></td></tr></table></figure><p><strong><a href="https://github.com/kubernetes/kubernetes/issues/38923#issuecomment-315255075" target="_blank" rel="noopener">bug &amp; fix</a></strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Warning ProvisioningFailed 31s (x16 over 19m) persistentvolume-controller Failed to provision volume with StorageClass &quot;ceph-elk&quot;: failed to create rbd image: executable file not found in $PATH, command output:</span><br></pre></td></tr></table></figure><p>Please check that <code>rbac/deployment.yaml</code> <strong><a href="http://quay.io/external_storage/rbd-provisioner:latest" target="_blank" rel="noopener">quay.io/external_storage/rbd-provisioner:latest</a></strong> image has the same Ceph version installed as your Ceph cluster. You can check it like this on any machine running docker:</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ceph-cluster:~$ ceph version</span><br><span class="line">ceph version 12.2.12 (1436006594665279fe734b4c15d7e08c13ebd777) luminous (stable)</span><br><span class="line"></span><br><span class="line">$ docker <span class="built_in">history</span> quay.io/external_storage/rbd-provisioner:v1.0.0-k8s1.10 | grep CEPH_VERSION</span><br><span class="line">&lt;missing&gt;           15 months ago       /bin/sh -c <span class="comment">#(nop)  ENV CEPH_VERSION=luminous    0B</span></span><br></pre></td></tr></table></figure><h2 id="create-static-pv-using-ceph-rbd"><a class="markdownIt-Anchor" href="#create-static-pv-using-ceph-rbd"></a> Create Static PV using Ceph RBD</h2><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-cluster:~$ rbd create vmd0 -s 64G</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">elk</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-secret-qemu</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">elk</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line"><span class="attr">  key:</span> <span class="string">QVFCNldYTmRtZ29iREJBQSt4dXorNHp0Wi33RWluR1J4U1hWcnc9PQ==</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-elk-pv0</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">elk</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    storage:</span> <span class="number">64</span><span class="string">Gi</span></span><br><span class="line">  <span class="comment"># The 'accessModes' are used as labels to match a PV and a PVC. They currently do not define any form of access control. All block storage is defined to be single user (non-</span></span><br><span class="line"><span class="string">shared</span> <span class="string">storage).</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  rbd:</span></span><br><span class="line"><span class="attr">    monitors:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.2</span><span class="string">:6789</span></span><br><span class="line"><span class="bullet">    -</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.3</span><span class="string">:6789</span></span><br><span class="line"><span class="attr">    pool:</span> <span class="string">rbd</span></span><br><span class="line">    <span class="comment"># The 'vmd0' must be created on the Ceph cluster.</span></span><br><span class="line"><span class="attr">    image:</span> <span class="string">vmd0</span></span><br><span class="line"><span class="attr">    user:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">    secretRef:</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">ceph-secret-qemu</span></span><br><span class="line"><span class="attr">    fsType:</span> <span class="string">ext4</span></span><br><span class="line"><span class="attr">    readOnly:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">ceph-elk-claim0</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">elk</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="comment"># The 'accessModes' do not enforce access rights but instead act as labels to match a PV to a PVC.</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">64</span><span class="string">Gi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment"># pod</span></span><br><span class="line"><span class="string">......</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">data</span></span><br><span class="line"><span class="attr">      persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">        claimName:</span> <span class="string">ceph-elk-claim0</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;overview&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#overview&quot;&gt;&lt;/a&gt; Overview&lt;/h2&gt;
&lt;p&gt;This topic provides a complete example of using an existing Ceph cluster for OKD persistent storage. It is assumed that a working Ceph cluster is already set up. If not, consult the &lt;a href=&quot;https://access.redhat.com/products/red-hat-ceph-storage&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Overview of Red Hat Ceph Storage&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Persistent Storage Using Ceph Rados Block Device provides an explanation of persistent volumes (PVs), persistent volume claims (PVCs), and using Ceph RBD as persistent storage.&lt;/p&gt;
&lt;h2 id=&quot;using-an-existing-ceph-cluster-for-persistent-storage&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#using-an-existing-ceph-cluster-for-persistent-storage&quot;&gt;&lt;/a&gt; Using an existing Ceph cluster for persistent storage&lt;/h2&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>Creating Kubernetes HA cluster with kubeadm</title>
    <link href="https://acquaai.github.io/2019/08/28/k8s-ha-with-kubeadm/"/>
    <id>https://acquaai.github.io/2019/08/28/k8s-ha-with-kubeadm/</id>
    <published>2019-08-28T15:22:27.000Z</published>
    <updated>2019-08-28T15:35:42.630Z</updated>
    
    <content type="html"><![CDATA[<h1 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> tl;dr</h1><p><strong>here to view full document</strong>:</p><ul><li><a href="https://github.com/acquaai/Kubernetes" target="_blank" rel="noopener">Creating HA cluster 1.15 with kubeadm on Ubuntu 18.04 LTS</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;tldr&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#tldr&quot;&gt;&lt;/a&gt; tl;dr&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;here to view full document&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href
      
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>Harbor (Updated)</title>
    <link href="https://acquaai.github.io/2019/08/06/harbor/"/>
    <id>https://acquaai.github.io/2019/08/06/harbor/</id>
    <published>2019-08-06T07:05:45.000Z</published>
    <updated>2019-09-16T14:32:05.365Z</updated>
    
    <content type="html"><![CDATA[<p>Harbor是由VMware公司开源的基于Docker的企业级容器注册服务器，它包括权限管理(RBAC)、LDAP、日志审核、管理界面、自我注册、镜像复制和中文支持等功能。</p><h2 id="install-docker"><a class="markdownIt-Anchor" href="#install-docker"></a> Install Docker</h2><p><strong>Pass it over</strong>.</p><h2 id="install-docker-compose"><a class="markdownIt-Anchor" href="#install-docker-compose"></a> <a href="https://docs.docker.com/compose/install/#install-compose" target="_blank" rel="noopener">Install Docker Compose</a></h2><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo curl -L <span class="string">"https://github.com/docker/compose/releases/download/1.24.1/docker-compose-<span class="variable">$(uname -s)</span>-<span class="variable">$(uname -m)</span>"</span> -o /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line">$ sudo chmod +x /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line">$ docker-compose --version</span><br><span class="line">docker-compose version 1.24.1, build 4667896b</span><br></pre></td></tr></table></figure><h2 id="install-harbor-with-https"><a class="markdownIt-Anchor" href="#install-harbor-with-https"></a> Install Harbor with https</h2><p><a href="http://github.com/vmware/harbor" target="_blank" rel="noopener">Prerequisites for the Harbor</a></p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.2.tgz</span><br><span class="line">$ sudo tar xzvf harbor-offline-installer-v1.8.2.tgz  -C /</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="gen-harbor-ssl"><a class="markdownIt-Anchor" href="#gen-harbor-ssl"></a> Gen Harbor SSL</h3><p><a href="https://github.com/fishdrowned/ssl" target="_blank" rel="noopener">自签泛域名证书</a></p><h3 id="configure-harbor"><a class="markdownIt-Anchor" href="#configure-harbor"></a> Configure Harbor</h3><p><strong>ssl</strong>:</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ls -l /data/cert/</span><br><span class="line">total 8</span><br><span class="line">-rw-r--r-- 1 root root 2863 Sep 11 17:07 xxx.com.bundle.crt</span><br><span class="line">-rw------- 1 root root 1675 Sep 11 17:08 xxx.com.key.pem</span><br><span class="line">-rw-r--r-- 1 root root 1350 Sep 11 17:12 root.crt</span><br></pre></td></tr></table></figure><p><strong>configuration</strong>:</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ grep -Ev <span class="string">"^$|#"</span> /harbor/harbor.yml</span><br></pre></td></tr></table></figure><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Required parameters</span></span><br><span class="line"><span class="attr">hostname:</span> <span class="string">reg.xxx.com</span></span><br><span class="line"></span><br><span class="line"><span class="attr">data_volume:</span> <span class="string">/data</span></span><br><span class="line"></span><br><span class="line"><span class="attr">harbor_admin_password:</span> <span class="string">Harbor12345</span></span><br><span class="line"></span><br><span class="line"><span class="attr">database:</span></span><br><span class="line"><span class="attr">  password:</span> <span class="string">root123</span></span><br><span class="line"></span><br><span class="line"><span class="attr">jobservice:</span></span><br><span class="line"><span class="attr">  max_job_workers:</span> <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="attr">log:</span></span><br><span class="line"><span class="attr">  level:</span> <span class="string">info</span></span><br><span class="line"><span class="attr">  rotate_count:</span> <span class="number">50</span></span><br><span class="line"><span class="attr">  rotate_size:</span> <span class="number">200</span><span class="string">M</span></span><br><span class="line"><span class="attr">  location:</span> <span class="string">/var/log/harbor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># optional parameters</span></span><br><span class="line"><span class="attr">http:</span></span><br><span class="line"><span class="attr">  port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">https:</span></span><br><span class="line"><span class="attr">  port:</span> <span class="number">443</span></span><br><span class="line"><span class="attr">  certificate:</span> <span class="string">/data/cert/xxx.com.bundle.crt</span></span><br><span class="line"><span class="attr">  private_key:</span> <span class="string">/data/cert/xxx.com.key.pem</span></span><br><span class="line"><span class="attr">clair:</span></span><br><span class="line"><span class="attr">  updaters_interval:</span> <span class="number">12</span></span><br><span class="line"><span class="attr">  http_proxy:</span></span><br><span class="line"><span class="attr">  https_proxy:</span></span><br><span class="line"><span class="attr">  no_proxy:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span><span class="string">,localhost,core,registry</span></span><br><span class="line"><span class="attr">chart:</span></span><br><span class="line"><span class="attr">  absolute_url:</span> <span class="string">enabled</span></span><br><span class="line"><span class="attr">uaa:</span></span><br><span class="line"><span class="attr">  ca_file:</span> <span class="string">/data/cert/root.crt</span></span><br></pre></td></tr></table></figure><p><strong>installing</strong>:</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /harbor</span><br><span class="line">$ sudo ./install.sh --with-clair</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...... installing log ......</span><br><span class="line">✔ ----Harbor has been installed and started successfully.----</span><br><span class="line"></span><br><span class="line">Now you should be able to visit the admin portal at https://x.x.x.x.</span><br><span class="line">For more details, please visit https://github.com/goharbor/harbor .</span><br></pre></td></tr></table></figure><p><strong>–with-notary</strong>: 镜像签名<br><strong>–with-clair</strong>: 漏扫<br><strong>–with-chartmuseum</strong>: Helm chart</p><h2 id="access-harbor"><a class="markdownIt-Anchor" href="#access-harbor"></a> Access Harbor</h2><h3 id="client-access"><a class="markdownIt-Anchor" href="#client-access"></a> Client Access</h3><p>将自签名的 <code>root.crt</code> 证书拷贝到需要访问 Harbor 的 docker 主机的 /etc/docker/certs.d/<code>reg.xxx.com</code>/。</p><blockquote><p>If the Docker registry is accessed without a port number, do not add the port to the directory name. The following shows the configuration for a registry on default port 443 which is accessed with:</p></blockquote><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ ll /etc/docker/certs.d/reg.xxx.com/</span><br><span class="line">root.crt</span><br><span class="line"></span><br><span class="line">$ docker login -u admin reg.xxx.com</span><br><span class="line">Password:</span><br><span class="line">WARNING! Your password will be stored unencrypted <span class="keyword">in</span> /home/k8s/.docker/config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/login/<span class="comment">#credentials-store</span></span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br></pre></td></tr></table></figure><h3 id="kubernetes-access"><a class="markdownIt-Anchor" href="#kubernetes-access"></a> <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/" target="_blank" rel="noopener">Kubernetes Access</a></h3><h2 id="managing-harbors-lifecycle"><a class="markdownIt-Anchor" href="#managing-harbors-lifecycle"></a> Managing Harbors lifecycle</h2><p>Stopping/Starting Harbor:</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /harbor/</span><br><span class="line">$ sudo docker-compose stop</span><br><span class="line">$ sudo docker-compose start</span><br></pre></td></tr></table></figure><p>To change Harbor’s configuration:</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker-compose down -v</span><br><span class="line">$ sudo vim harbor.yml</span><br><span class="line">$ sudo ./prepare --with-notary --with-clair --with-chartmuseum</span><br><span class="line">$ sudo docker-compose up -d</span><br></pre></td></tr></table></figure><p>Removing Harbor’s containers while keeping the image data and Harbor’s database files on the file system:</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker-compose down -v</span><br></pre></td></tr></table></figure><p>Removing Harbor’s database and image data (for a clean re-installation):</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo rm -r /data/database</span><br><span class="line">$ sudo rm -r /data/registry</span><br></pre></td></tr></table></figure><ul><li>[x] Clair 同步更新时注意容器 DNS 的配置 (/etc/resolv.conf)</li><li>[x] <a href="https://github.com/theupdateframework/notary" target="_blank" rel="noopener">Notray</a> 目前并不好用，删除已经签名的镜像时仍需要借助<code>Notary CLI</code></li></ul><h2 id="projects"><a class="markdownIt-Anchor" href="#projects"></a> Projects</h2><p><strong>Project: acqua</strong></p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker tag SOURCE_IMAGE[:TAG] reg.xxx.com/acqua/IMAGE[:TAG]</span><br><span class="line">$ docker push reg.xxx.com/acqua/IMAGE[:TAG]</span><br></pre></td></tr></table></figure><h2 id="how-to-process-that-forget-harbors-admin-password"><a class="markdownIt-Anchor" href="#how-to-process-that-forget-harbors-admin-password"></a> How to process that forget Harbors admin password</h2><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ docker <span class="built_in">exec</span> -it harbor-db bash</span><br><span class="line"></span><br><span class="line">root [ / ]<span class="comment"># psql -h postgresql -d postgres -U postgres</span></span><br><span class="line">Password <span class="keyword">for</span> user postgres: </span><br><span class="line">psql (9.6.14)</span><br><span class="line">Type <span class="string">"help"</span> <span class="keyword">for</span> <span class="built_in">help</span>.</span><br><span class="line"></span><br><span class="line">postgres=<span class="comment">#</span></span><br></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">postgres=# \l</span><br><span class="line">postgres=# \c registry</span><br><span class="line">You are now connected to database "registry" as user "postgres".</span><br><span class="line"></span><br><span class="line">registry=# \d</span><br><span class="line"></span><br><span class="line">registry=# select user_id, username, password, realname, salt from harbor_user;</span><br><span class="line"> user_id | username  |             password             |    realname    |               salt               </span><br><span class="line"><span class="comment">---------+-----------+----------------------------------+----------------+----------------------------------</span></span><br><span class="line">       2 | anonymous |                                  | anonymous user | </span><br><span class="line">       3 | ybcard    | f5f92a94bfc48c36a539a644167476e0 | ybcard         | uc6dbjkwh178i1aycisi26q8y8mo593v</span><br><span class="line">       1 | admin     | 4f16b74b68178d0c83d00af80ddb7d10 | system admin   | vsq9qbd0jgu3236iz0beat43yl9av11a</span><br><span class="line"></span><br><span class="line"># pbkdf2 algorithm, "Admin123"</span><br><span class="line"></span><br><span class="line"><span class="keyword">update</span> harbor_user <span class="keyword">set</span> <span class="keyword">password</span>=<span class="string">'e7c0331ebb021d64713c0515f6dad38f'</span>, <span class="keyword">salt</span>=<span class="string">'pa4mmop0v9lhnv2vpvmkuv941it72ku6'</span> <span class="keyword">where</span> username=<span class="string">'admin'</span>;</span><br><span class="line"></span><br><span class="line">registry=# \q</span><br><span class="line">root [ / ]# exit</span><br></pre></td></tr></table></figure><p><strong>Ref</strong></p><ul><li><a href="https://github.com/goharbor/harbor/blob/master/docs/configure_https.md" target="_blank" rel="noopener">configure_https</a></li><li><a href="https://github.com/goharbor/harbor/blob/master/docs/user_guide.md#user-guide" target="_blank" rel="noopener">User Guide</a></li><li><a href="https://coreos.com/os/docs/latest/generate-self-signed-certificates.html" target="_blank" rel="noopener">Generate self-signed certificates</a></li><li><a href="https://github.com/mitsuhiko/python-pbkdf2/blob/master/pbkdf2.py" target="_blank" rel="noopener">pbkdf2</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Harbor是由VMware公司开源的基于Docker的企业级容器注册服务器，它包括权限管理(RBAC)、LDAP、日志审核、管理界面、自我注册、镜像复制和中文支持等功能。&lt;/p&gt;
&lt;h2 id=&quot;install-docker&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#install-docker&quot;&gt;&lt;/a&gt; Install Docker&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Pass it over&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&quot;install-docker-compose&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#install-docker-compose&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://docs.docker.com/compose/install/#install-compose&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Install Docker Compose&lt;/a&gt;&lt;/h2&gt;
&lt;figure class=&quot;highlight zsh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ sudo curl -L &lt;span class=&quot;string&quot;&gt;&quot;https://github.com/docker/compose/releases/download/1.24.1/docker-compose-&lt;span class=&quot;variable&quot;&gt;$(uname -s)&lt;/span&gt;-&lt;span class=&quot;variable&quot;&gt;$(uname -m)&lt;/span&gt;&quot;&lt;/span&gt; -o /usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/bin/docker-compose&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sudo chmod +x /usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/bin/docker-compose&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ docker-compose --version&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;docker-compose version 1.24.1, build 4667896b&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;install-harbor-with-https&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#install-harbor-with-https&quot;&gt;&lt;/a&gt; Install Harbor with https&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://github.com/vmware/harbor&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Prerequisites for the Harbor&lt;/a&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight zsh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ wget https://storage.googleapis.com/harbor-releases/release-1.8.0/harbor-offline-installer-v1.8.2.tgz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sudo tar xzvf harbor-offline-installer-v1.8.2.tgz  -C /&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>Nexus Docker Registry</title>
    <link href="https://acquaai.github.io/2019/08/06/nexus-docker-registry/"/>
    <id>https://acquaai.github.io/2019/08/06/nexus-docker-registry/</id>
    <published>2019-08-06T03:41:11.000Z</published>
    <updated>2019-09-11T03:58:40.317Z</updated>
    
    <content type="html"><![CDATA[<p>Sonatype Nexus OSS 3 开始支持 Docker 镜像仓库，当拉取的 images 在 docker(hosted) 仓库中不存在时，会自动从配置的 docker(proxy) 仓库中拉取。</p><h2 id="deploy-nexus"><a class="markdownIt-Anchor" href="#deploy-nexus"></a> Deploy Nexus</h2><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir /nexus-data</span><br><span class="line">$ sudo chmod -Rv 200 /nexus-data/</span><br></pre></td></tr></table></figure><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -dti \</span><br><span class="line">        --net=host \</span><br><span class="line">        --name=nexus \</span><br><span class="line">        --privileged=<span class="literal">true</span> \</span><br><span class="line">        --restart=always \</span><br><span class="line">        -e INSTALL4J_ADD_VM_PARAMS=<span class="string">"-Xms16g -Xmx16g -XX:MaxDirectMemorySize=24g"</span> \</span><br><span class="line">        -v /etc/localtime:/etc/localtime \</span><br><span class="line">        -v /nexus-data:/nexus-data \</span><br><span class="line">        sonatype/nexus3:3.18.1</span><br></pre></td></tr></table></figure><a id="more"></a><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ll /nexus-data/admin.password</span><br><span class="line">-rw-r--r-- 1 200 200 36 Sep  8 23:58 /nexus-data/admin.password</span><br><span class="line">$ cat /nexus-data/admin.password</span><br><span class="line">545cb562-a03f-4dc8-a78f-d9a15fa2442a</span><br></pre></td></tr></table></figure><p><code>http://ip:8081</code></p><h2 id="configure-blob-stores"><a class="markdownIt-Anchor" href="#configure-blob-stores"></a> Configure Blob Stores</h2><ul><li>aliyun-hub</li><li>default</li><li>docker-hub</li><li>google-hub</li><li>ybcard</li></ul><h2 id="configure-repositories"><a class="markdownIt-Anchor" href="#configure-repositories"></a> Configure Repositories</h2><ul><li>docker(group): hub-group 8082</li></ul><ul><li>docker(proxy): docker-hub 8083</li><li>docker(proxy): google-hub 8084</li><li>docker(proxy): aliyun-hub 8085</li><li>docker(hosted): ybcard 8086</li></ul><h3 id="create-repository-docker-proxy"><a class="markdownIt-Anchor" href="#create-repository-docker-proxy"></a> Create Repository: docker (proxy)</h3><ul><li>Name: docker-hub</li></ul><ul><li>[x] Online:</li></ul><ul><li>HTTP: 8083</li></ul><ul><li>[x] Enable Docker V1 API:</li></ul><ul><li>Remote storage: |</li><li><a href="https://registry-1.docker.io" target="_blank" rel="noopener">https://registry-1.docker.io</a></li><li><a href="https://k8s.gcr.io" target="_blank" rel="noopener">https://k8s.gcr.io</a></li><li><a href="https://registry.cn-hangzhou.aliyuncs.com" target="_blank" rel="noopener">https://registry.cn-hangzhou.aliyuncs.com</a></li><li>Docker Index: Use Docker Hub</li></ul><ul><li>[x] Auto blocking enabled:</li></ul><ul><li>Maximum component age: 1440</li><li>Maximum metadata age: 1440</li><li>Blob store: docker-hub</li></ul><ul><li>[x] Strict Content Type Validattion:</li><li>[x] Not found cache enabled:</li></ul><ul><li>Not found cache TTL: 1440</li></ul><h3 id="create-repository-docker-hosted"><a class="markdownIt-Anchor" href="#create-repository-docker-hosted"></a> Create Repository: docker (hosted)</h3><ul><li>Name: ybcard</li></ul><ul><li>[x] Online:</li></ul><ul><li>HTTP: 8086</li></ul><ul><li>[x] Enable Docker V1 API:</li></ul><ul><li>Blob store: ybcard</li></ul><ul><li>[x] Strict Content Type Validattion:</li></ul><ul><li>Deployment policy: Allow redeploy</li></ul><h3 id="create-repository-docker-group"><a class="markdownIt-Anchor" href="#create-repository-docker-group"></a> Create Repository: docker (group)</h3><ul><li>Name: hub-group</li></ul><ul><li>[x] Online:</li></ul><ul><li>HTTP: 8082</li></ul><ul><li>[x] Enable Docker V1 API:</li></ul><ul><li>Blob store: default</li></ul><ul><li>[x] Strict Content Type Validattion:</li></ul><ul><li>Members: docker-hub, aliyun-hub, google-hub, ybcard</li></ul><p><code>NOTE:</code> group 仓库并不能推送镜像，需要 <strong>docker(hosted): ybcard 8086</strong> 才能推送。</p><h2 id="configure-nginx"><a class="markdownIt-Anchor" href="#configure-nginx"></a> Configure Nginx</h2><p>为了 <strong>pull</strong> 不携带端口，使用<a href="https://github.com/fishdrowned/ssl" target="_blank" rel="noopener">自签名泛域名证书</a>。</p><p>Nexus WebUI: <a href="http://nexus.xxx.com" target="_blank" rel="noopener">nexus.xxx.com</a><br>Docker registry: <a href="http://registry.xxx.com" target="_blank" rel="noopener">registry.xxx.com</a></p><p><strong><code>nginx.conf</code></strong>:</p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">worker_processes</span> auto;</span><br><span class="line"><span class="attribute">pid</span> /run/nginx.pid;</span><br><span class="line"><span class="attribute">include</span> /etc/nginx/modules-enabled/<span class="regexp">*.conf</span>;</span><br><span class="line"></span><br><span class="line"><span class="section">events</span> &#123;</span><br><span class="line"><span class="attribute">worker_connections</span> <span class="number">768</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">http</span> &#123;</span><br><span class="line"><span class="attribute">sendfile</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">tcp_nopush</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">tcp_nodelay</span> <span class="literal">on</span>;</span><br><span class="line"><span class="comment">#keepalive_timeout 5 5;</span></span><br><span class="line">   <span class="attribute">proxy_buffering</span> <span class="literal">off</span>;</span><br><span class="line"></span><br><span class="line"><span class="attribute">include</span> /etc/nginx/mime.types;</span><br><span class="line"><span class="attribute">default_type</span> application/octet-stream;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">upstream</span> nexus_web &#123;</span><br><span class="line">            <span class="attribute">server</span> <span class="number">172.31.16.10:8081</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">upstream</span> nexus_docker_pull &#123;</span><br><span class="line">            <span class="attribute">server</span> <span class="number">172.31.16.10:8082</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">upstream</span> nexus_docker_push &#123;</span><br><span class="line">            <span class="attribute">server</span> <span class="number">172.31.16.10:8086</span>;</span><br><span class="line">        &#125;  </span><br><span class="line"></span><br><span class="line">    <span class="section">server</span> &#123;</span><br><span class="line">        <span class="attribute">listen</span> <span class="number">443</span> ssl;</span><br><span class="line"> </span><br><span class="line">        <span class="attribute">server_name</span> registry.acqua.com;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">access_log</span> /data/wwwlogs/registry.acqua.com.log;</span><br><span class="line">        <span class="attribute">error_log</span> /data/wwwlogs/registry.<span class="literal">error</span>.log;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># allow large uploads of files - refer to nginx documentation</span></span><br><span class="line">        <span class="attribute">client_max_body_size</span> <span class="number">1G</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># optimize downloading files larger than 1G - refer to nginx doc before adjusting</span></span><br><span class="line">        <span class="comment">#proxy_max_temp_file_size 2048m;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># required to avoid HTTP 411: see Issue #1486 (https://github.com/docker/docker/issues/1486)</span></span><br><span class="line">        <span class="attribute">chunked_transfer_encoding</span> <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">ssl</span> <span class="literal">on</span>;</span><br><span class="line">        <span class="attribute">ssl_certificate</span>     /etc/nginx/ssl/acqua.com.bundle.crt;</span><br><span class="line">        <span class="attribute">ssl_certificate_key</span> /etc/nginx/ssl/acqua.com.key.pem;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># default push</span></span><br><span class="line">        <span class="attribute">set</span> <span class="variable">$upstream</span> <span class="string">"nexus_docker_push"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># docker pull</span></span><br><span class="line">        <span class="attribute">if</span> ( <span class="variable">$request_method</span> <span class="regexp">~* 'GET')</span> &#123;</span><br><span class="line">             <span class="attribute">set</span> <span class="variable">$upstream</span> <span class="string">"nexus_docker_pull"</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># only docker(hosted)</span></span><br><span class="line">        <span class="attribute">if</span> (<span class="variable">$request_uri</span> <span class="regexp">~ '/search')</span> &#123;</span><br><span class="line">            <span class="attribute">set</span> <span class="variable">$upstream</span> <span class="string">"nexus_docker_push"</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">location</span> / &#123;</span><br><span class="line">            <span class="attribute">proxy_pass</span> http://<span class="variable">$upstream</span>;</span><br><span class="line">            <span class="attribute">proxy_set_header</span> Host <span class="variable">$host</span>;</span><br><span class="line">            <span class="attribute">proxy_send_timeout</span> <span class="number">3600</span>;</span><br><span class="line">            <span class="attribute">proxy_read_timeout</span> <span class="number">3600</span>;</span><br><span class="line">            <span class="attribute">proxy_connect_timeout</span> <span class="number">3600</span>;</span><br><span class="line">            <span class="attribute">proxy_set_header</span> X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">            <span class="attribute">proxy_set_header</span> X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">            <span class="attribute">proxy_set_header</span> X-Forwarded-Proto <span class="string">"https"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="section">server</span> &#123;</span><br><span class="line">        <span class="attribute">listen</span> <span class="number">443</span> ssl;</span><br><span class="line">        <span class="attribute">server_name</span> nexus.cs.acqua.com;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">access_log</span> /data/wwwlogs/nexus.cs.acqua.com.log;</span><br><span class="line">        <span class="attribute">error_log</span> /data/wwwlogs/nexus.<span class="literal">error</span>.log;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># allow large uploads of files - refer to nginx documentation</span></span><br><span class="line">        <span class="attribute">client_max_body_size</span> <span class="number">1G</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># optimize downloading files larger than 1G - refer to nginx doc before adjusting</span></span><br><span class="line">        <span class="comment">#proxy_max_temp_file_size 2048m;</span></span><br><span class="line"></span><br><span class="line">        <span class="attribute">ssl</span> <span class="literal">on</span>;</span><br><span class="line">        <span class="attribute">ssl_certificate</span>     /etc/nginx/ssl/acqua.com.bundle.crt;</span><br><span class="line">        <span class="attribute">ssl_certificate_key</span> /etc/nginx/ssl/acqua.com.key.pem;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">location</span> /download &#123;</span><br><span class="line">            <span class="attribute">root</span> /data/wwwroot;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="attribute">location</span> / &#123;</span><br><span class="line">            <span class="attribute">proxy_pass</span> http://nexus_web;</span><br><span class="line">            <span class="attribute">proxy_set_header</span> Host <span class="variable">$host</span>;</span><br><span class="line">            <span class="attribute">proxy_send_timeout</span> <span class="number">120</span>;</span><br><span class="line">            <span class="attribute">proxy_read_timeout</span> <span class="number">300</span>;</span><br><span class="line">            <span class="attribute">proxy_set_header</span> X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">            <span class="attribute">proxy_set_header</span> X-Forwarded-For <span class="variable">$proxy_add_x_forwarded_for</span>;</span><br><span class="line">            <span class="attribute">proxy_set_header</span> X-Forwarded-Proto <span class="string">"https"</span>;</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="attribute">gzip</span> <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">## Virtual Host Configs ##</span></span><br><span class="line"><span class="attribute">include</span> /etc/nginx/conf.d/<span class="regexp">*.conf</span>;</span><br><span class="line"><span class="attribute">include</span> /etc/nginx/sites-enabled/*;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="导入证书"><a class="markdownIt-Anchor" href="#导入证书"></a> 导入证书</h2><ul><li>将自签名的根证书 <code>out/root.crt</code>:</li></ul><ul><li>导入客户端；</li><li>导入所有需要访问镜像仓库的 <code>control plane</code> 和 <code>worker</code> 节点。</li></ul><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir -p /etc/docker/certs.d/registry.xxx.com</span><br><span class="line">$ <span class="built_in">cd</span> /etc/docker/certs.d/registry.xxx.com</span><br><span class="line">$ sudo curl -k https://nexus.cs.xxx.com/download/ca.crt -o ca.crt</span><br><span class="line"></span><br><span class="line">k8s@cp-172-31-16-11:~$ docker login -u admin https://registry.xxx.com</span><br><span class="line">Password:</span><br><span class="line">WARNING! Your password will be stored unencrypted <span class="keyword">in</span> /home/k8s/.docker/config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/login/<span class="comment">#credentials-store</span></span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br></pre></td></tr></table></figure><h2 id="warning"><a class="markdownIt-Anchor" href="#warning"></a> <strong>WARNING!</strong></h2><p>Docker 利用 docker login 命令来校验用户镜像仓库的登录凭证，并不 Web Login，仅仅是一种登录试探校验 <strong>用户名、密码</strong> 是否正确，正确情况下 Docker 会把 <em>仓库域名、用户名、密码</em> 等信息进行 base64 编码后保存在 <code>$HOME/.docker/config.json</code> 文件中。</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cat /home/k8s/.docker/config.json</span><br><span class="line">&#123;</span><br><span class="line"><span class="string">"auths"</span>: &#123;</span><br><span class="line"><span class="string">"registry.xxx.com"</span>: &#123;</span><br><span class="line"><span class="string">"auth"</span>: <span class="string">"cGFzc3dvcmQ="</span></span><br></pre></td></tr></table></figure><p>针对每一个镜像仓库，只保存最近一次有效登录的用户名和密码。docker logout 时从文件中删除相应信息。</p><p>用户密码解码：</p><pre class="highlight"><code class="zsh">$ <span class="built_in">echo</span> <span class="string">'cGFzc3dvcmQ='</span> | base64 --decodepassword</code></pre><h3 id="安全存储-docker-login-登录凭证"><a class="markdownIt-Anchor" href="#安全存储-docker-login-登录凭证"></a> 安全存储 docker login 登录凭证</h3><p>Docker 官方为不同平台提供了相应的解决方案: 详见 <a href="https://github.com/docker/docker-credential-helpers" target="_blank" rel="noopener">docker-credential-helpers</a></p><p><strong>Ref</strong></p><ul><li><a href="https://blog.sonatype.com/running-the-nexus-platform-behind-nginx-using-docker" target="_blank" rel="noopener">Running The Nexus Platform Behind Nginx Using Docker</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Sonatype Nexus OSS 3 开始支持 Docker 镜像仓库，当拉取的 images 在 docker(hosted) 仓库中不存在时，会自动从配置的 docker(proxy) 仓库中拉取。&lt;/p&gt;
&lt;h2 id=&quot;deploy-nexus&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#deploy-nexus&quot;&gt;&lt;/a&gt; Deploy Nexus&lt;/h2&gt;
&lt;figure class=&quot;highlight zsh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ sudo mkdir /nexus-data&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sudo chmod -Rv 200 /nexus-data/&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight zsh&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ docker run -dti \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        --net=host \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        --name=nexus \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        --privileged=&lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt; \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        --restart=always \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        -e INSTALL4J_ADD_VM_PARAMS=&lt;span class=&quot;string&quot;&gt;&quot;-Xms16g -Xmx16g -XX:MaxDirectMemorySize=24g&quot;&lt;/span&gt; \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        -v /etc/localtime:/etc/localtime \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        -v /nexus-data:/nexus-data \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        sonatype/nexus3:3.18.1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>部署 Tornado 开发环境</title>
    <link href="https://acquaai.github.io/2018/05/22/depoly-tornado-dev/"/>
    <id>https://acquaai.github.io/2018/05/22/depoly-tornado-dev/</id>
    <published>2018-05-22T06:54:48.000Z</published>
    <updated>2019-08-28T14:00:52.037Z</updated>
    
    <content type="html"><![CDATA[<p><code>Tornado</code>：<a href="http://www.tornadoweb.org/en/stable/" target="_blank" rel="noopener">Tornado</a> 是 Python 编写出来的一个极轻量级、高可伸缩性和非阻塞IO的Web服务器软件，例如前 Friendfeed 网站。</p><p><code>Supervisor</code>：一个服务（进程）管理工具，主要用于监控服务器上的服务，并且在出现问题时自动重启。</p><p><code>Nginx</code>：在这里作为反向代理。</p><a id="more"></a><h2 id="centos-系统基础环境"><a class="markdownIt-Anchor" href="#centos-系统基础环境"></a> CentOS 系统基础环境</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum update -y</span><br></pre></td></tr></table></figure><p>因为目前 Supervisor 仅支持 python2，但又需要使用 Python3 来开发，所以它们需要共存。为了不同环境互不干扰，将使用<code>virtualenv</code>来达到这一目的。</p><h3 id="安装-python-3"><a class="markdownIt-Anchor" href="#安装-python-3"></a> 安装 Python 3</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python2的软链接</span></span><br><span class="line"></span><br><span class="line">$ ll /usr/bin/python*</span><br><span class="line">lrwxrwxrwx 1 root root    7 May 21 10:30 /usr/bin/python -&gt; python2</span><br><span class="line">lrwxrwxrwx 1 root root    9 May 21 10:30 /usr/bin/python2 -&gt; python2.7</span><br><span class="line">-rwxr-xr-x 1 root root 7216 Apr 11 15:36 /usr/bin/python2.7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装Python3的依赖</span></span><br><span class="line">$ yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make </span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装Python3.6</span></span><br><span class="line">$ wget https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz</span><br><span class="line">$ tar xzvf Python-3.6.5.tgz</span><br><span class="line">$ <span class="built_in">cd</span> Python-3.6.5</span><br><span class="line">$ ./configure perfix=/usr/<span class="built_in">local</span>/python3</span><br><span class="line">$ make &amp;&amp; make install</span><br><span class="line">$ python -V</span><br><span class="line"></span><br><span class="line"><span class="comment"># （可选）设置Python3为系统默认版本</span></span><br><span class="line"></span><br><span class="line">$ mv /usr/bin/python /usr/bin/python.bak</span><br><span class="line">$ ln -s /usr/<span class="built_in">local</span>/python3/bin/python3.6 /usr/bin/python</span><br><span class="line">$ python -V</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置 yum（yum包管理工具使用Python2）</span></span><br><span class="line">$ vi /usr/bin/yum</span><br><span class="line"></span><br><span class="line"><span class="comment">#! /usr/bin/python ===&gt; #! /usr/bin/python2</span></span><br><span class="line"></span><br><span class="line">$ vi /usr/libexec/urlgrabber-ext-down</span><br><span class="line"><span class="comment">#! /usr/bin/python ===&gt; #! /usr/bin/python2</span></span><br></pre></td></tr></table></figure><h2 id="安装-tornado"><a class="markdownIt-Anchor" href="#安装-tornado"></a> 安装 Tornado</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ pip3 install tornado</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建示例页面</span></span><br><span class="line">$ mkdir /var/www</span><br><span class="line">$ vi /var/www/index.py</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tornado.ioloop</span><br><span class="line"><span class="keyword">import</span> tornado.web</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MainHandler</span><span class="params">(tornado.web.RequestHandler)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.write(<span class="string">"Hello, Tornado!"</span>)</span><br><span class="line"> </span><br><span class="line">application = tornado.web.Application([</span><br><span class="line">    (<span class="string">r"/"</span>, MainHandler),</span><br><span class="line">    (<span class="string">r"/index.py"</span>, MainHandler),</span><br><span class="line">])</span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    application.listen(<span class="number">8006</span>)</span><br><span class="line">    tornado.ioloop.IOLoop.instance().start()</span><br></pre></td></tr></table></figure><h3 id="test"><a class="markdownIt-Anchor" href="#test"></a> Test</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ python3 index.py</span><br><span class="line"></span><br><span class="line">http://10.0.77.119:8006</span><br></pre></td></tr></table></figure><h2 id="安装-nginx"><a class="markdownIt-Anchor" href="#安装-nginx"></a> 安装 Nginx</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/yum.repos.d/nginx.repo</span><br><span class="line">[nginx]</span><br><span class="line">name=nginx repo</span><br><span class="line">baseurl=http://nginx.org/packages/centos/7/<span class="variable">$basearch</span>/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">$ yum install nginx</span><br></pre></td></tr></table></figure><p><a href="http://nginx.org/keys/nginx_signing.key" target="_blank" rel="noopener">nginx signing key</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/nginx/nginx.conf          </span><br><span class="line"></span><br><span class="line">user  nginx;</span><br><span class="line">worker_processes  1;</span><br><span class="line"></span><br><span class="line">error_log  /var/<span class="built_in">log</span>/nginx/error.log warn;</span><br><span class="line">pid        /var/run/nginx.pid;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">    use epoll;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    upstream tornado &#123;</span><br><span class="line">        server 10.0.77.119:8006;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        listen 80;</span><br><span class="line">        root /var/www;</span><br><span class="line">        index index.py index.html;</span><br><span class="line"></span><br><span class="line">        server_name server;</span><br><span class="line"></span><br><span class="line">        location / &#123;</span><br><span class="line">            <span class="keyword">if</span> (!-e <span class="variable">$request_filename</span>) &#123;</span><br><span class="line">                rewrite ^/(.*)$ /index.py/<span class="variable">$1</span> last;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        location ~ /index\.py &#123;</span><br><span class="line">            proxy_pass_header Server;</span><br><span class="line">            proxy_set_header Host <span class="variable">$http_host</span>;</span><br><span class="line">            proxy_set_header X-Real-IP <span class="variable">$remote_addr</span>;</span><br><span class="line">            proxy_set_header X-Scheme <span class="variable">$scheme</span>;</span><br><span class="line">            proxy_pass http://tornado;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/images/test_Tornado.png" alt></p><h2 id="安装-supervisor"><a class="markdownIt-Anchor" href="#安装-supervisor"></a> 安装 Supervisor</h2><h3 id="pip2"><a class="markdownIt-Anchor" href="#pip2"></a> pip2</h3><ul><li>安装 pip</li><li>yum 安装</li></ul> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ yum install epel-release</span><br><span class="line">$ yum install python-pip</span><br></pre></td></tr></table></figure><ul><li>curl 安装</li></ul> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl <span class="string">"https://bootstrap.pypa.io/get-pip.py"</span> -o <span class="string">"get-pip.py"</span></span><br><span class="line">$ python get-pip.py</span><br></pre></td></tr></table></figure><ul><li>检测安装</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip --<span class="built_in">help</span></span><br><span class="line">pip -V</span><br></pre></td></tr></table></figure><h3 id="virtualenv"><a class="markdownIt-Anchor" href="#virtualenv"></a> virtualenv</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip2 install virtualenv</span><br></pre></td></tr></table></figure><h3 id="配置-supervisor"><a class="markdownIt-Anchor" href="#配置-supervisor"></a> 配置 supervisor</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建 virtualenv 环境</span></span><br><span class="line">$ mkdir /etc/supervisor</span><br><span class="line">$ virtualenv --distribute -p /usr/bin/python2 supervisor</span><br><span class="line"></span><br><span class="line">Already using interpreter /usr/bin/python2</span><br><span class="line">New python executable <span class="keyword">in</span> /etc/supervisor/bin/python2</span><br><span class="line">Also creating executable <span class="keyword">in</span> /etc/supervisor/bin/python</span><br><span class="line">Installing setuptools, pip, wheel...done.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入 virtualenv 环境</span></span><br><span class="line">$ <span class="built_in">cd</span> supervisor/</span><br><span class="line">$ <span class="built_in">source</span> bin/activate</span><br><span class="line">(supervisor) [root@tornado-web supervisor]<span class="comment"># ./bin/pip install supervisor</span></span><br><span class="line">...</span><br><span class="line">Successfully installed meld3-1.0.2 supervisor-3.3.4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">(supervisor) [root@tornado-web supervisor]<span class="comment"># echo_supervisord_conf &gt; /etc/supervisord.conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改如下内容：</span></span><br><span class="line">[unix_http_server]</span><br><span class="line">file=/var/run/supervisor.sock</span><br><span class="line">chmod=0700</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启Web管理功能</span></span><br><span class="line">[inet_http_server]</span><br><span class="line">port=10.0.77.119:9001iface</span><br><span class="line">username=user</span><br><span class="line">password=123</span><br><span class="line"></span><br><span class="line">[supervisord]</span><br><span class="line">logfile=/var/<span class="built_in">log</span>/supervisor/supervisord.log</span><br><span class="line">pidfile=/var/run/supervisord.pid</span><br><span class="line"></span><br><span class="line">[supervisorctl]</span><br><span class="line">serverurl=unix:///var/run/supervisor.sock</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在文件末尾增加如下内容：</span></span><br><span class="line"><span class="comment"># 主要作用就是在Supervisor启动的时候自动启动 hello 应用对应的</span></span><br><span class="line"><span class="comment"># Tornado Web Server进程并纳入管理。</span></span><br><span class="line">[program:hello]</span><br><span class="line"><span class="built_in">command</span>=/usr/<span class="built_in">local</span>/bin/python3.6 /var/www/index.py --port=8006</span><br><span class="line">directory=/var/www</span><br><span class="line">autorestart=<span class="literal">true</span></span><br><span class="line">redirect_stderr=<span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动 Supervisord</span></span><br><span class="line">(supervisor) [root@tornado-web supervisor]<span class="comment"># supervisord  -c /etc/supervisord.conf</span></span><br><span class="line">(supervisor) [root@tornado-web supervisor]<span class="comment"># supervisorctl</span></span><br><span class="line">hello                            RUNNING   pid 5107, uptime 0:05:04</span><br><span class="line">supervisor&gt; <span class="built_in">help</span></span><br><span class="line"></span><br><span class="line">default commands (<span class="built_in">type</span> <span class="built_in">help</span> &lt;topic&gt;):</span><br><span class="line">=====================================</span><br><span class="line">add    <span class="built_in">exit</span>      open  reload  restart   start   tail   </span><br><span class="line">avail  <span class="built_in">fg</span>        pid   remove  shutdown  status  update </span><br><span class="line">clear  maintail  quit  reread  signal    stop    version</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置和运行 Supervisord 后，就可以退出 virtulenv</span></span><br><span class="line">(supervisor) [root@tornado-web supervisor]<span class="comment"># deactivate</span></span><br></pre></td></tr></table></figure><p><img src="/images/supervisor.png" alt></p><p><strong>Reference</strong></p><ul><li><a href="http://tornado-zh.readthedocs.io/zh/latest/guide/running.html" target="_blank" rel="noopener">Tornado Docs</a></li><li><a href="http://supervisord.org/" target="_blank" rel="noopener">Supervisor Docs</a></li><li><a href="http://einverne.github.io/post/2017/10/nginx-conf.html" target="_blank" rel="noopener">Nginx 配置详解</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;Tornado&lt;/code&gt;：&lt;a href=&quot;http://www.tornadoweb.org/en/stable/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Tornado&lt;/a&gt; 是 Python 编写出来的一个极轻量级、高可伸缩性和非阻塞IO的Web服务器软件，例如前 Friendfeed 网站。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Supervisor&lt;/code&gt;：一个服务（进程）管理工具，主要用于监控服务器上的服务，并且在出现问题时自动重启。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Nginx&lt;/code&gt;：在这里作为反向代理。&lt;/p&gt;
    
    </summary>
    
      <category term="Python" scheme="https://acquaai.github.io/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>Create a bootable USB using macOS</title>
    <link href="https://acquaai.github.io/2018/05/04/bootable-usb/"/>
    <id>https://acquaai.github.io/2018/05/04/bootable-usb/</id>
    <published>2018-05-04T07:06:39.000Z</published>
    <updated>2019-08-28T14:00:52.036Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ls CentOS-7*</span><br><span class="line">CentOS-7-x86_64-DVD-1708.iso</span><br><span class="line"></span><br><span class="line">➜  ~ hdiutil convert -format UDRW -o CentOS74.img CentOS-7-x86_64-DVD-1708.iso</span><br><span class="line"></span><br><span class="line">➜  ~ diskutil list</span><br><span class="line">/dev/disk0 (internal, physical):</span><br><span class="line">   #:      TYPE NAME                   SIZE       IDENTIFIER</span><br><span class="line">   0:      GUID_partition_scheme        *1.0 TB   disk0</span><br><span class="line">   1:      EFI EFI                     209.7 MB   disk0s1</span><br><span class="line">   2:      Apple_HFS Macintosh HD      999.3 GB   disk0s2</span><br><span class="line">   3:      Apple_Boot Recovery HD      650.0 MB   disk0s3</span><br><span class="line"></span><br><span class="line">/dev/disk1 (external, physical):</span><br><span class="line">   #:      TYPE NAME                     SIZE       IDENTIFIER</span><br><span class="line">   0:      GUID_partition_scheme         *15.5 GB   disk1</span><br><span class="line">   1:      EFI EFI                       209.7 MB   disk1s1</span><br><span class="line">   2:      Microsoft Basic Data KINGSTON  15.3 GB   disk1s2</span><br><span class="line">   </span><br><span class="line">➜  ~ diskutil unmountDisk /dev/disk1</span><br><span class="line">Unmount of all volumes on disk1 was successful</span><br><span class="line"></span><br><span class="line">➜  ~ sudo dd if=CentOS74.img.dmg of=/dev/disk1 bs=1m</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class
      
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>Helm deploy TiDB on Kubernetes</title>
    <link href="https://acquaai.github.io/2018/04/04/helm-tidb-k8s/"/>
    <id>https://acquaai.github.io/2018/04/04/helm-tidb-k8s/</id>
    <published>2018-04-04T10:24:11.000Z</published>
    <updated>2019-08-28T14:00:52.039Z</updated>
    
    <content type="html"><![CDATA[<h1 id="tldr"><a class="markdownIt-Anchor" href="#tldr"></a> tl;dr</h1><p>本文将使用 <code>Helm</code> 包管理器在 <code>Kubernetes</code> 集群上部署 <code>TiDB</code>。</p><h2 id="tidb-chart"><a class="markdownIt-Anchor" href="#tidb-chart"></a> TiDB Chart</h2><blockquote><p><a href="https://pingcap.com" target="_blank" rel="noopener">TiDB</a> 是新一代开源分布式 NewSQL 数据库，模型受 Google Spanner / F1 论文的启发，实现了自动的水平伸缩，强一致性的分布式事务，基于 Raft 算法的多副本复制等重要 NewSQL 特性。TiDB 结合了 RDBMS 和 NoSQL 的优点，部署简单，在线弹性扩容和异步表结构变更不影响业务， 真正的异地多活及自动故障恢复保障数据安全，同时兼容 MySQL 协议，使迁移使用成本降到极低。</p></blockquote><a id="more"></a><h3 id="create-chart"><a class="markdownIt-Anchor" href="#create-chart"></a> Create <a href="https://docs.helm.sh/developing_charts/#charts" target="_blank" rel="noopener">Chart</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ helm create tidb</span><br><span class="line">Creating tidb</span><br><span class="line"></span><br><span class="line">$ yum -y install tree</span><br><span class="line">$ tree tidb</span><br><span class="line">tidb</span><br><span class="line">├── charts</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   └── service.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">2 directories, 7 files</span><br></pre></td></tr></table></figure><ul><li>charts: 本 TiDB Chart 依赖的 chart，当前为空</li><li>Chart.yaml: 描述 Chart 的基本信息</li><li>templates: k8s manifest的模板文件目录。模板使用 chart 配置的值生成 manifest 文件，遵守 Go 语言的语法。</li><li>templates/NOTES.txt: 描述 chart 的使用说明</li><li>values.yaml: chart 配置的默认值</li></ul><h3 id="config-tidb-chart"><a class="markdownIt-Anchor" href="#config-tidb-chart"></a> Config TiDB Chart</h3><p>创建 TiKV 的存储类，TiKV 动态请求 PVC。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create secret generic ceph-secret --<span class="built_in">type</span>=<span class="string">"kubernetes.io/rbd"</span> \</span><br><span class="line">  --from-literal=key=<span class="string">'QVFDd2hLZGFJYktSSHhBQVlmQ21vaitWUnNmUVhTczA3ODRLb3c9PQ=='</span></span><br><span class="line"></span><br><span class="line">$ kubectl create -f tikv-storageclass.yaml</span><br></pre></td></tr></table></figure><p>Chart 的文件在<a href="https://github.com/acquaai/K8S/tree/master/charts/incubator/tidb" target="_blank" rel="noopener">这里</a>。</p><h3 id="test-tidb-chart"><a class="markdownIt-Anchor" href="#test-tidb-chart"></a> Test TiDB Chart</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --dry-run --debug tidb</span><br><span class="line">[debug] Created tunnel using <span class="built_in">local</span> port: <span class="string">'38158'</span></span><br><span class="line"></span><br><span class="line">[debug] SERVER: <span class="string">"127.0.0.1:38158"</span></span><br><span class="line"></span><br><span class="line">[debug] Original chart version: <span class="string">""</span></span><br><span class="line">[debug] CHART PATH: /root/tidb</span><br><span class="line"></span><br><span class="line">NAME:   invisible-pika</span><br><span class="line">REVISION: 1</span><br><span class="line">RELEASED: Wed Apr  4 17:19:31 2018</span><br><span class="line">CHART: tidb-v2.0.0</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="install-chart"><a class="markdownIt-Anchor" href="#install-chart"></a> Install Chart</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --name pingcap tidb</span><br><span class="line">NAME:   pingcap</span><br><span class="line">LAST DEPLOYED: Sat Apr  7 15:57:13 2018</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1beta1/Deployment</span><br><span class="line">NAME                 DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">pingcap-tidb-db      1        1        1           0          1s</span><br><span class="line">pingcap-tidb-vision  1        1        1           0          1s</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/StatefulSet</span><br><span class="line">NAME             DESIRED  CURRENT  AGE</span><br><span class="line">pingcap-tidb-pd  3        1        1s</span><br><span class="line">pingcap-tidb-kv  3        1        1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                  READY  STATUS             RESTARTS  AGE</span><br><span class="line">pingcap-tidb-db-846cdff8c6-frb22      0/1    ContainerCreating  0         1s</span><br><span class="line">pingcap-tidb-vision-76bd48b96f-pmqnl  0/1    ContainerCreating  0         1s</span><br><span class="line">pingcap-tidb-pd-0                     0/1    ContainerCreating  0         1s</span><br><span class="line">pingcap-tidb-kv-0                     0/1    Init:0/1           0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/StorageClass</span><br><span class="line">NAME                 PROVISIONER        AGE</span><br><span class="line">ceph-tidb (default)  kubernetes.io/rbd  1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                 TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)                         AGE</span><br><span class="line">pingcap-tidb-pd      ClusterIP  None           &lt;none&gt;       2379/TCP,2380/TCP               1s</span><br><span class="line">pingcap-tidb-db      NodePort   10.254.164.24  &lt;none&gt;       4000:31975/TCP,10080:30608/TCP  1s</span><br><span class="line">pingcap-tidb-vision  NodePort   10.254.30.87   &lt;none&gt;       8010:30895/TCP                  1s</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ helm status pingcap</span><br><span class="line">LAST DEPLOYED: Sat Apr  7 15:57:13 2018</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                  READY  STATUS   RESTARTS  AGE</span><br><span class="line">pingcap-tidb-db-846cdff8c6-frb22      1/1    Running  0         5m</span><br><span class="line">pingcap-tidb-vision-76bd48b96f-pmqnl  1/1    Running  0         5m</span><br><span class="line">pingcap-tidb-pd-0                     1/1    Running  0         5m</span><br><span class="line">pingcap-tidb-pd-1                     1/1    Running  1         5m</span><br><span class="line">pingcap-tidb-pd-2                     1/1    Running  1         5m</span><br><span class="line">pingcap-tidb-kv-0                     1/1    Running  1         5m</span><br><span class="line">pingcap-tidb-kv-1                     1/1    Running  0         4m</span><br><span class="line">pingcap-tidb-kv-2                     1/1    Running  0         3m</span><br><span class="line"></span><br><span class="line">==&gt; v1/StorageClass</span><br><span class="line">NAME                 PROVISIONER        AGE</span><br><span class="line">ceph-tidb (default)  kubernetes.io/rbd  5m</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                 TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)                         AGE</span><br><span class="line">pingcap-tidb-pd      ClusterIP  None           &lt;none&gt;       2379/TCP,2380/TCP               5m</span><br><span class="line">pingcap-tidb-db      NodePort   10.254.164.24  &lt;none&gt;       4000:31975/TCP,10080:30608/TCP  5m</span><br><span class="line">pingcap-tidb-vision  NodePort   10.254.30.87   &lt;none&gt;       8010:30895/TCP                  5m</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/Deployment</span><br><span class="line">NAME                 DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">pingcap-tidb-db      1        1        1           1          5m</span><br><span class="line">pingcap-tidb-vision  1        1        1           1          5m</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/StatefulSet</span><br><span class="line">NAME             DESIRED  CURRENT  AGE</span><br><span class="line">pingcap-tidb-pd  3        3        5m</span><br><span class="line">pingcap-tidb-kv  3        3        5m</span><br></pre></td></tr></table></figure><blockquote><p>注意<code>Chart.yaml</code>文件中 <strong>name 和 version</strong> 的值及<code>--name</code> <strong>release name</strong>，否则会报错：</p><p>Error: release PingCAP failed: Service “PingCAP-TiDB-pd” is invalid: <a href="http://metadata.name" target="_blank" rel="noopener">metadata.name</a>: Invalid value: “PingCAP-TiDB-pd”: a DNS-1035 label must consist of lower case alphanumeric characters or ‘-’, start with an alphabetic character, and end with an alphanumeric character (e.g. ‘my-name’,  or ‘abc-123’, regex used for validation is ‘<a href="%5B-a-z0-9%5D*%5Ba-z0-9%5D">a-z</a>?’)</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -h 10.0.77.17 -P 31975 -u root -D test</span><br><span class="line">Welcome to the MySQL monitor.  Commands <span class="keyword">end</span> <span class="keyword">with</span> ; or \g.</span><br><span class="line">Your MySQL connection id is 2</span><br><span class="line">Server version: 5.7.10-TiDB-v2.0.0-rc.3 MySQL Community Server (Apache License 2.0)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type '<span class="keyword">help</span>;' or '\h' for help. <span class="keyword">Type</span> <span class="string">'\c'</span> <span class="keyword">to</span> <span class="keyword">clear</span> the <span class="keyword">current</span> <span class="keyword">input</span> statement.</span><br><span class="line"></span><br><span class="line">mysql&gt; <span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| Database           |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">| INFORMATION_SCHEMA |</span><br><span class="line">| PERFORMANCE_SCHEMA |</span><br><span class="line">| mysql              |</span><br><span class="line">| test               |</span><br><span class="line">+<span class="comment">--------------------+</span></span><br><span class="line">4 rows in <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ helm list</span><br><span class="line">NAME    REVISION        UPDATED                         STATUS          CHART           NAMESPACE</span><br><span class="line">pingcap 1               Sat Apr  7 15:57:13 2018        DEPLOYED        tidb-v2.0.0     default  </span><br><span class="line"></span><br><span class="line">$ helm delete pingcap</span><br><span class="line">release <span class="string">"pingcap"</span> deleted</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --name pingcap tidb</span><br><span class="line">Error: a release named pingcap already exists.</span><br><span class="line">Run: helm ls --all pingcap; to check the status of the release</span><br><span class="line">Or run: helm del --purge pingcap; to delete it</span><br><span class="line"></span><br><span class="line">$ helm ls --all</span><br><span class="line">NAME                    REVISION        UPDATED                         STATUS  CHART                   NAMESPACE</span><br><span class="line">pingcap                 1               Tue Apr  7 17:20:31 2018        DELETED tidb-v2.0.0             default  </span><br><span class="line"></span><br><span class="line">$ helm del --purge pingcap</span><br><span class="line">release <span class="string">"pingcap"</span> deleted</span><br></pre></td></tr></table></figure><h3 id="deploy-chart-into-artifactory"><a class="markdownIt-Anchor" href="#deploy-chart-into-artifactory"></a> Deploy Chart into Artifactory</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ helm package tidb</span><br><span class="line">Successfully packaged chart and saved it to: /root/tidb-v2.0.0.tgz</span><br><span class="line"></span><br><span class="line">$ curl -uadmin:AP412AQbN3DGPWuWXvrXvJrvzPX -T tidb-v2.0.0.tgz "http://10.0.77.17:30809/artifactory/helm-virtual/tidb-v2.0.0.tgz"</span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">"repo"</span> : <span class="string">"helm-local"</span>,</span><br><span class="line">  <span class="attr">"path"</span> : <span class="string">"/tidb-v2.0.0.tgz"</span>,</span><br><span class="line">  <span class="attr">"created"</span> : <span class="string">"2018-04-04T09:54:26.368Z"</span>,</span><br><span class="line">  <span class="attr">"createdBy"</span> : <span class="string">"admin"</span>,</span><br><span class="line">  <span class="attr">"downloadUri"</span> : <span class="string">"http://10.0.77.17:30809/artifactory/helm-local/tidb-v2.0.0.tgz"</span>,</span><br><span class="line">  <span class="attr">"mimeType"</span> : <span class="string">"application/x-gzip"</span>,</span><br><span class="line">  <span class="attr">"size"</span> : <span class="string">"3176"</span>,</span><br><span class="line">  <span class="attr">"checksums"</span> : &#123;</span><br><span class="line">    <span class="attr">"sha1"</span> : <span class="string">"c43f6fef95ce9c6fec22f7856cb28f1630499f13"</span>,</span><br><span class="line">    <span class="attr">"md5"</span> : <span class="string">"33e0d37e351587f61124c04aeb8b87a0"</span>,</span><br><span class="line">    <span class="attr">"sha256"</span> : <span class="string">"bacb3d6001794b09d554d57b6fb0e8b00acd00bf4ef1af0d0a120583dd9c4704"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"originalChecksums"</span> : &#123;</span><br><span class="line">    <span class="attr">"sha256"</span> : <span class="string">"bacb3d6001794b09d554d57b6fb0e8b00acd00bf4ef1af0d0a120583dd9c4704"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">"uri"</span> : <span class="string">"http://10.0.77.17:30809/artifactory/helm-local/tidb-v2.0.0.tgz"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="qa"><a class="markdownIt-Anchor" href="#qa"></a> QA</h3><blockquote><p>2018/04/04 03:10:07.591 <a href="http://tikv-server.rs:135" target="_blank" rel="noopener">tikv-server.rs:135</a>: [ERROR] Limit(“the maximum number of open file descriptors is too small, got 65536, expect greater or equal to 82920”)</p></blockquote><p>Linux 有文件句柄限制，默认1024，系统总限制可以查看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/sys/fs/file-max</span><br><span class="line">807856</span><br></pre></td></tr></table></figure><p>系统当前使用的文件句柄数量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/sys/fs/file-nr </span><br><span class="line">3424    0       807856</span><br></pre></td></tr></table></figure><p>查看进程开启的句柄：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lsof -n |awk <span class="string">'&#123;print $2&#125;'</span>|sort|uniq -c |sort -nr|more</span><br></pre></td></tr></table></figure><blockquote><p>修改<code>Docker</code>(/usr/lib/systemd/system/docker.service)的服务并重启：</p></blockquote><p><s>LimitNOFILE=infinity</s></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ps -ef|grep docker</span><br><span class="line">root      3400  3390  1 07:56 ?        00:00:00 docker-containerd --config /var/run/docker/containerd/containerd.toml</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">$ cat /proc/3400/limits |grep <span class="string">"Max open files"</span></span><br><span class="line">Max open files            65536                65536                files</span><br></pre></td></tr></table></figure><p><strong><code>LimitNOFILE=1000000</code></strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/1393/limits |grep <span class="string">"Max open files"</span></span><br><span class="line">Max open files            1000000              1000000              files</span><br></pre></td></tr></table></figure><p><s>DOCKER_NOFILE=1000000 在我的CentOS Linux release 7.3.1611 (Core)中未生效。</s></p><p><strong>option</strong><br>limits.conf 文件实际是 Linux PAM（插入式认证模块，Pluggable Authentication Modules）中 pam_limits.so 的配置文件，突破系统的默认限制，对系统访问资源有一定保护作用。limits.conf 和 sysctl.conf 区别在于 limits.conf 是针对用户，而 sysctl.conf 是针对整个系统参数配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /etc/security/limits.conf</span><br><span class="line">root     -     nofile     1000000</span><br></pre></td></tr></table></figure><p><strong>References</strong></p><ul><li><a href="https://pingcap.com/docs-cn/op-guide/docker-compose/#%E8%87%AA%E5%AE%9A%E4%B9%89%E9%9B%86%E7%BE%A4" target="_blank" rel="noopener">TiDB快速部署</a></li><li><a href="https://github.com/pingcap" target="_blank" rel="noopener">PingCAP ∙ GitHub</a></li><li><a href="https://github.com/pingcap/docs/blob/master/FAQ.md" target="_blank" rel="noopener">TiDB FAQ</a></li><li><a href="https://banzaicloud.com/blog/tidb-kubernetes/" target="_blank" rel="noopener">Running TiDB on Kubernetes</a></li><li><a href="https://github.com/banzaicloud/banzai-charts/tree/master/incubator/tidb" target="_blank" rel="noopener">banzaicloud/banzai-charts</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;tldr&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#tldr&quot;&gt;&lt;/a&gt; tl;dr&lt;/h1&gt;
&lt;p&gt;本文将使用 &lt;code&gt;Helm&lt;/code&gt; 包管理器在 &lt;code&gt;Kubernetes&lt;/code&gt; 集群上部署 &lt;code&gt;TiDB&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&quot;tidb-chart&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#tidb-chart&quot;&gt;&lt;/a&gt; TiDB Chart&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://pingcap.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TiDB&lt;/a&gt; 是新一代开源分布式 NewSQL 数据库，模型受 Google Spanner / F1 论文的启发，实现了自动的水平伸缩，强一致性的分布式事务，基于 Raft 算法的多副本复制等重要 NewSQL 特性。TiDB 结合了 RDBMS 和 NoSQL 的优点，部署简单，在线弹性扩容和异步表结构变更不影响业务， 真正的异地多活及自动故障恢复保障数据安全，同时兼容 MySQL 协议，使迁移使用成本降到极低。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>Helm on Kubernetes</title>
    <link href="https://acquaai.github.io/2018/04/02/helm/"/>
    <id>https://acquaai.github.io/2018/04/02/helm/</id>
    <published>2018-04-02T12:25:47.000Z</published>
    <updated>2019-08-28T14:00:52.040Z</updated>
    
    <content type="html"><![CDATA[<h2 id="helm"><a class="markdownIt-Anchor" href="#helm"></a> Helm</h2><p><a href="https://helm.sh" target="_blank" rel="noopener">Helm</a> 作为 Kubernetes 的包管理平台，类似 CentOS 的 yum 或 Ubuntu 的 apt-get 来部署管理 k8s 的应用。Helm Charts 能够帮你定义、安装、升级最复杂的 Kubernetes 应用集合，它容易创建，做版本化，共享和发布。在K8s上部署一些通用应用（组件）时，利用 Helm 可以获得业界统一的<code>最佳实践</code>的配置，从而减少时间成本。</p><p>Helm Dictionary (概念):</p><ul><li>chart: a package; bundle of Kubernetes <a href="https://github.com/kubernetes/charts" target="_blank" rel="noopener">resources</a> (能够描述最复杂的应用，提供可重复，幂等性的安装，以及提供统一的认证中心服务。)</li><li>Release: a chart instance is loaded into Kubernetes (提供实时的镜像升级，以及自定义 webhook，解决镜像升级的痛点。)<ul><li>same chart can be installed several times into the same cluster; each will have it’s own Release</li></ul></li><li>Repository: a repository of published Charts (版本化、共享、Heml 私有仓库。)</li><li>Template: a Kubernetes configuration file mixed with Go/Sprig template</li></ul><a id="more"></a><h3 id="helm-components"><a class="markdownIt-Anchor" href="#helm-components"></a> Helm Components</h3><ul><li><p>Helm Client: Helm 是用户命令行工具，常与 CI/CD 部署在一起。它用来创建，拉取，搜索和验证 Charts，初始化 Tiller 服务。</p></li><li><p>Tiller: 是一个部署在Kubernetes集群内部的 Server，与 Helm client、Kubernetes APIServer 进行交互，管理这些应用的发布。</p></li></ul><p><a href="https://www.slideshare.net/alexLM/helm-application-deployment-management-for-kubernetes" target="_blank" rel="noopener">Alexei Ledenev’s Helm - Application deployment management for Kubernetes</a></p><p><img src="/images/helm-arch.png" alt></p><h3 id="install-helm"><a class="markdownIt-Anchor" href="#install-helm"></a> Install Helm</h3><p><strong>安装 Client:</strong></p><p>4Mac:</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ brew install kubernetes-helm</span><br></pre></td></tr></table></figure><p>4Linux:</p><p><a href="https://github.com/kubernetes/helm" target="_blank" rel="noopener">下载</a> Helm</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://storage.googleapis.com/kubernetes-helm/helm-v2.8.2-linux-amd64.tar.gz</span><br><span class="line">$ tar xzvf helm-v2.8.2-linux-amd64.tar.gz</span><br><span class="line">$ mv linux-amd64/helm /usr/<span class="built_in">local</span>/bin/</span><br><span class="line"></span><br><span class="line">$ helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:<span class="string">"v2.8.2"</span>, GitCommit:<span class="string">"a80231648a1473929271764b920a8e346f6de844"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br><span class="line">Error: cannot connect to Tiller</span><br></pre></td></tr></table></figure><p><strong>安装 Tiller:</strong></p><p><code>$ helm init</code></p><blockquote><p>在安装 tiller 之前，需要在 10.0.77.16(ss,u know) 上配置好 kubectl 工具和 kubeconfig 文件，并确保可以正常访问 Kubernetes APIServer。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ scp root@n1.k8s.com:/usr/<span class="built_in">local</span>/bin/kubectl /usr/<span class="built_in">local</span>/bin/</span><br><span class="line">$ mkdir <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">$ scp root@n1.k8s.com:/root/.kube/config /root/.kube/</span><br><span class="line"></span><br><span class="line">$ kubectl get cs</span><br><span class="line">NAME                 STATUS    MESSAGE             ERROR</span><br><span class="line">scheduler            Healthy   ok                  </span><br><span class="line">controller-manager   Healthy   ok                  </span><br><span class="line">etcd-0               Healthy   &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;   </span><br><span class="line">etcd-1               Healthy   &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;   </span><br><span class="line">etcd-2               Healthy   &#123;<span class="string">"health"</span>:<span class="string">"true"</span>&#125;</span><br></pre></td></tr></table></figure><p>因为 k8s cluster(v1.9.2) 中开启了 <code>RBAC</code> 访问控制，所以需要创建 <code>service account: tiller</code> 并分配合适的角色。详情请参考 <a href="https://docs.helm.sh/using_helm/#tiller-and-role-based-access-control" target="_blank" rel="noopener">TILLER AND ROLE-BASED ACCESS CONTROL</a>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create serviceaccount tiller --namespace kube-system</span><br></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ cat helm-rbac.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: tiller</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: tiller</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: tiller</span><br><span class="line">    namespace: kube-system</span><br><span class="line">    </span><br><span class="line">$ kubectl create -f helm-rbac.yaml</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm init --service-account tiller --skip-refresh</span><br></pre></td></tr></table></figure><blockquote><p>GFW again，可以使用 <code>--tiller-image</code> 或<code>-i</code>参数来指定 tiller 要使用到的镜像。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gcr.io/kubernetes-helm/tiller:v2.8.2</span><br><span class="line">---&gt;</span><br><span class="line">10.0.77.16/library/tiller:v2.8.2</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ helm init --service-account tiller --tiller-image 10.0.77.16/library/tiller:v2.8.2 --skip-refresh [--kube-context string]</span><br><span class="line"></span><br><span class="line">Creating /root/.helm </span><br><span class="line">Creating /root/.helm/repository </span><br><span class="line">Creating /root/.helm/repository/cache </span><br><span class="line">Creating /root/.helm/repository/<span class="built_in">local</span> </span><br><span class="line">Creating /root/.helm/plugins </span><br><span class="line">Creating /root/.helm/starters </span><br><span class="line">Creating /root/.helm/cache/archive </span><br><span class="line">Creating /root/.helm/repository/repositories.yaml </span><br><span class="line">Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com </span><br><span class="line">Adding <span class="built_in">local</span> repo with URL: http://127.0.0.1:8879/charts </span><br><span class="line"><span class="variable">$HELM_HOME</span> has been configured at /root/.helm.</span><br><span class="line"></span><br><span class="line">Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.</span><br><span class="line"></span><br><span class="line">Please note: by default, Tiller is deployed with an insecure <span class="string">'allow unauthenticated users'</span> policy.</span><br><span class="line">For more information on securing your installation see: https://docs.helm.sh/using_helm/<span class="comment">#securing-your-helm-installation</span></span><br><span class="line">Happy Helming!</span><br></pre></td></tr></table></figure><p><code>--kube-context</code> - default ‘~/.kube/config’</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n kube-system -l app=helm</span><br><span class="line">NAME                            READY     STATUS    RESTARTS   AGE</span><br><span class="line">tiller-deploy-b56587494-ctd9c   1/1       Running   0          1m</span><br><span class="line"></span><br><span class="line">$ helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:<span class="string">"v2.8.2"</span>, GitCommit:<span class="string">"a80231648a1473929271764b920a8e346f6de844"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:<span class="string">"v2.8.2"</span>, GitCommit:<span class="string">"a80231648a1473929271764b920a8e346f6de844"</span>, GitTreeState:<span class="string">"clean"</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">更新仓库:</span><br><span class="line"></span><br><span class="line">$ helm repo update</span><br><span class="line">Hang tight <span class="keyword">while</span> we grab the latest from your chart repositories...</span><br><span class="line">...Skip <span class="built_in">local</span> chart repository</span><br><span class="line">...Successfully got an update from the <span class="string">"stable"</span> chart repository</span><br><span class="line">Update Complete. ⎈ Happy Helming!⎈ </span><br><span class="line"></span><br><span class="line">设置helm命令自动补全:</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">source</span> &lt;(helm completion zsh)</span><br><span class="line">OR</span><br><span class="line">$ <span class="built_in">source</span> &lt;(helm completion bash)</span><br></pre></td></tr></table></figure><h2 id="helm-chart-repository"><a class="markdownIt-Anchor" href="#helm-chart-repository"></a> Helm Chart Repository</h2><blockquote><p>当研发人员达到一定数量后，Docker 原生的镜像中心或 Harbor 成为瓶颈。<a href="https://www.jfrog.com" target="_blank" rel="noopener">JFrog Artifactory</a> 提供了企业内部的高可用 Docker 注册中心集群，能够提供高并发 Docker Pull 的拉取。</p></blockquote><p>Artifactory 的虚拟 Helm Chart 仓库能够聚合公司本地和远程的仓库成为一个仓库，为开发者解析和安装 Charts 时提供唯一的 URL。如下图所示：</p><p><img src="/images/repo.png" alt></p><p>JFrog Helm Client 目前支持2个版本：</p><ul><li><a href="https://github.com/kubernetes/helm" target="_blank" rel="noopener">匿名访问</a> Artifactory</li><li><a href="https://github.com/JFrogDev/helm" target="_blank" rel="noopener">认证访问</a> Artifactory</li></ul><h2 id="artifactory-on-k8s"><a class="markdownIt-Anchor" href="#artifactory-on-k8s"></a> Artifactory on K8s</h2><p><img src="/images/artifactory-k8s.png" alt></p><p><a href="https://github.com/JFrogDev/artifactory-docker-examples/tree/master/kubernetes" target="_blank" rel="noopener">官方参考</a></p><p>Yaml 文件在<a href="https://github.com/acquaai/K8S/tree/master/yaml/JFrog-Artifactory" target="_blank" rel="noopener">这里</a>。</p><h3 id="persistent-storage"><a class="markdownIt-Anchor" href="#persistent-storage"></a> Persistent Storage</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cd artifactory/</span><br><span class="line">$ kubectl create secret generic ceph-secret --type=&quot;kubernetes.io/rbd&quot;   --from-literal=key=&apos;AQCwhKdaIbKRHxAAYfCmoj+VRsfQXSs0784Kow==&apos;</span><br><span class="line">$ kubectl create -f art-storageclass.yaml</span><br></pre></td></tr></table></figure><h3 id="database-driver"><a class="markdownIt-Anchor" href="#database-driver"></a> Database Driver</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker build -t 10.0.77.16/library/artifactory-pro-mysql:5.10.1 -f Dockerfile.mysql .</span><br><span class="line">$ docker push 10.0.77.16/library/artifactory-pro-mysql:5.10.1</span><br></pre></td></tr></table></figure><h3 id="mysql-database"><a class="markdownIt-Anchor" href="#mysql-database"></a> MySQL Database</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f mysql.yaml</span><br></pre></td></tr></table></figure><h3 id="artifactory-pro"><a class="markdownIt-Anchor" href="#artifactory-pro"></a> Artifactory Pro</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f artifactory.yaml</span><br><span class="line">$ kubectl logs -f artifactory-k8s-deployment-667d8847d-mrj2m</span><br><span class="line">...</span><br><span class="line"><span class="comment">###########################################################</span></span><br><span class="line"><span class="comment">### Artifactory successfully started (93.102 seconds)   ###</span></span><br><span class="line"><span class="comment">###########################################################</span></span><br></pre></td></tr></table></figure><h3 id="nginx-option"><a class="markdownIt-Anchor" href="#nginx-option"></a> Nginx (option)</h3><p><strong>SSL secret</strong></p><p>使用<a href="https://github.com/michaelliao/itranswarp.js/blob/master/conf/ssl/gencert.sh" target="_blank" rel="noopener">michaelliao</a>的脚本生成 demo SSL secret。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sh gen-ssl-cert.sh </span><br><span class="line">Enter your domain [www.example.com]: artifactory</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">$ kubectl create secret tls art-tls --cert=artifactory.crt --key=artifactory.key</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f nginx.yaml</span><br></pre></td></tr></table></figure><h3 id="accessing-artifactory-pro"><a class="markdownIt-Anchor" href="#accessing-artifactory-pro"></a> Accessing Artifactory Pro</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods</span><br><span class="line">NAME                                         READY     STATUS    RESTARTS   AGE</span><br><span class="line">artifactory-k8s-deployment-667d8847d-mrj2m   1/1       Running   0          7m</span><br><span class="line">mysql-k8s-deployment-5746f95457-rpzc6        1/1       Running   0          10m</span><br><span class="line">nginx-k8s-deployment-5cb5fdb4c4-fvn4s        1/1       Running   0          2m</span><br><span class="line"></span><br><span class="line">$ kubectl get svc</span><br><span class="line">NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE</span><br><span class="line">artifactory         NodePort    10.254.85.186   &lt;none&gt;        8081:30809/TCP               8m</span><br><span class="line">kubernetes          ClusterIP   10.254.0.1      &lt;none&gt;        443/TCP                      39d</span><br><span class="line">mysql-k8s-service   ClusterIP   10.254.91.68    &lt;none&gt;        3306/TCP                     10m</span><br><span class="line">nginx-k8s-service   NodePort    10.254.131.58   &lt;none&gt;        80:30021/TCP,443:31086/TCP   3m</span><br><span class="line"></span><br><span class="line">$ kubectl describe svc/nginx-k8s-service</span><br><span class="line">Name:                     nginx-k8s-service</span><br><span class="line">Namespace:                default</span><br><span class="line">Labels:                   app=nginx-k8s-service</span><br><span class="line">                          group=artifactory-k8s</span><br><span class="line">Annotations:              &lt;none&gt;</span><br><span class="line">Selector:                 app=nginx-k8s-deployment</span><br><span class="line">Type:                     NodePort</span><br><span class="line">IP:                       10.254.131.58</span><br><span class="line">Port:                     port-1  80/TCP</span><br><span class="line">TargetPort:               80/TCP</span><br><span class="line">NodePort:                 port-1  30021/TCP</span><br><span class="line">Endpoints:                172.30.13.5:80</span><br><span class="line">Port:                     port-2  443/TCP</span><br><span class="line">TargetPort:               443/TCP</span><br><span class="line">NodePort:                 port-2  31086/TCP</span><br><span class="line">Endpoints:                172.30.13.5:443</span><br><span class="line">Session Affinity:         None</span><br><span class="line">External Traffic Policy:  Cluster</span><br><span class="line">Events:                   &lt;none&gt;</span><br></pre></td></tr></table></figure><h3 id="config-artifactory-repository"><a class="markdownIt-Anchor" href="#config-artifactory-repository"></a> Config Artifactory Repository</h3><p><a href="https://jfrog.com/blog/master-helm-chart-repositories-artifactory/" target="_blank" rel="noopener">Master Your Helm Chart Repositories in Artifactory</a></p><p><code>admin</code> 用户登录<code>http://10.0.77.18:30809/</code>新建：</p><p><code>Local Repository</code>，<code>Remote Repository</code>，然后新建<code>Virtual Repository</code>。注意，<code>Virtual Repository</code> 包含<code>本地和远程仓库</code></p><h3 id="helm-client-registry-repository"><a class="markdownIt-Anchor" href="#helm-client-registry-repository"></a> Helm Client Registry Repository</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ helm repo add helm-virtual http://10.0.77.17:30809/artifactory/helm-virtual admin AP412AQbN3DGPWuWXvrXvJrvzPX</span><br><span class="line"><span class="string">"helm-virtual"</span> has been added to your repositories</span><br><span class="line"></span><br><span class="line">$ helm repo update</span><br><span class="line">Hang tight <span class="keyword">while</span> we grab the latest from your chart repositories...</span><br><span class="line">...Skip <span class="built_in">local</span> chart repository</span><br><span class="line">...Successfully got an update from the <span class="string">"helm-virtual"</span> chart repository</span><br><span class="line">...Unable to get an update from the <span class="string">"stable"</span> chart repository (https://kubernetes-charts.storage.googleapis.com):</span><br><span class="line">        Get https://kubernetes-charts.storage.googleapis.com/index.yaml: dial tcp 172.217.160.112:443: i/o timeout</span><br><span class="line">Update Complete. ⎈ Happy Helming!⎈ </span><br><span class="line"></span><br><span class="line">$ helm repo list</span><br><span class="line">NAME            URL                                             </span><br><span class="line">stable          https://kubernetes-charts.storage.googleapis.com</span><br><span class="line"><span class="built_in">local</span>           http://127.0.0.1:8879/charts                    </span><br><span class="line">helm-virtual    http://10.0.77.18:30809/artifactory/helm-virtual</span><br><span class="line"></span><br><span class="line"><span class="comment">#To deploy a Helm Chart into an Artifactory repository you need to use Artifactory's REST API.</span></span><br><span class="line">$ curl -uadmin:AP412AQbN3DGPWuWXvrXvJrvzPX -T &lt;PATH_TO_FILE&gt; <span class="string">"http://10.0.77.17:30809/artifactory/helm-virtual/&lt;TARGET_FILE_PATH&gt;"</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;helm&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#helm&quot;&gt;&lt;/a&gt; Helm&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://helm.sh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Helm&lt;/a&gt; 作为 Kubernetes 的包管理平台，类似 CentOS 的 yum 或 Ubuntu 的 apt-get 来部署管理 k8s 的应用。Helm Charts 能够帮你定义、安装、升级最复杂的 Kubernetes 应用集合，它容易创建，做版本化，共享和发布。在K8s上部署一些通用应用（组件）时，利用 Helm 可以获得业界统一的&lt;code&gt;最佳实践&lt;/code&gt;的配置，从而减少时间成本。&lt;/p&gt;
&lt;p&gt;Helm Dictionary (概念):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;chart: a package; bundle of Kubernetes &lt;a href=&quot;https://github.com/kubernetes/charts&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;resources&lt;/a&gt; (能够描述最复杂的应用，提供可重复，幂等性的安装，以及提供统一的认证中心服务。)&lt;/li&gt;
&lt;li&gt;Release: a chart instance is loaded into Kubernetes (提供实时的镜像升级，以及自定义 webhook，解决镜像升级的痛点。)
&lt;ul&gt;
&lt;li&gt;same chart can be installed several times into the same cluster; each will have it’s own Release&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Repository: a repository of published Charts (版本化、共享、Heml 私有仓库。)&lt;/li&gt;
&lt;li&gt;Template: a Kubernetes configuration file mixed with Go/Sprig template&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>Spark on Kubernetes</title>
    <link href="https://acquaai.github.io/2018/03/29/spark-k8s/"/>
    <id>https://acquaai.github.io/2018/03/29/spark-k8s/</id>
    <published>2018-03-29T07:35:28.000Z</published>
    <updated>2019-08-28T14:00:52.044Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Apache Spark™ is a fast and general engine for large-scale data processing.</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">gcr.io/google_containers/spark:1.5.2_v1</span><br><span class="line">---&gt;</span><br><span class="line">10.0.77.16/library/spark:1.5.2_v1</span><br><span class="line"></span><br><span class="line">elsonrodriguez/spark-ui-proxy:1.0</span><br><span class="line">---&gt;</span><br><span class="line">10.0.77.16/library/spark-ui-proxy:1.0</span><br><span class="line"></span><br><span class="line">gcr.io/google_containers/zeppelin:v0.5.6_v1</span><br><span class="line">---&gt;</span><br><span class="line">10.0.77.16/library/zeppelin:v0.5.6_v1</span><br></pre></td></tr></table></figure><p><a href="https://github.com/acquaai/K8S/tree/master/yaml/Spark-K8s" target="_blank" rel="noopener">YAML Files</a></p><a id="more"></a><h2 id="create-namespace"><a class="markdownIt-Anchor" href="#create-namespace"></a> Create Namespace</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f spark-namespace.yaml</span><br></pre></td></tr></table></figure><h2 id="deploy-master"><a class="markdownIt-Anchor" href="#deploy-master"></a> Deploy Master</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f spark-master-rc.yaml</span><br><span class="line">$ kubectl create -f spark-master-svc.yaml</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -n spark-cluster</span><br><span class="line">NAME                            READY     STATUS    RESTARTS   AGE</span><br><span class="line">spark-master-controller-h9rvd   1/1       Running   0          1m</span><br><span class="line"></span><br><span class="line">$ kubectl logs spark-master-controller-h9rvd -n spark-cluster</span><br><span class="line">18/03/29 03:32:09 INFO Master: Registered signal handlers <span class="keyword">for</span> [TERM, HUP, INT]</span><br><span class="line">18/03/29 03:32:10 INFO SecurityManager: Changing view acls to: root</span><br><span class="line">18/03/29 03:32:10 INFO SecurityManager: Changing modify acls to: root</span><br><span class="line">18/03/29 03:32:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)</span><br><span class="line">18/03/29 03:32:11 INFO Slf4jLogger: Slf4jLogger started</span><br><span class="line">18/03/29 03:32:11 INFO Remoting: Starting remoting</span><br><span class="line">18/03/29 03:32:11 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkMaster@spark-master:7077]</span><br><span class="line">18/03/29 03:32:11 INFO Utils: Successfully started service <span class="string">'sparkMaster'</span> on port 7077.</span><br><span class="line">18/03/29 03:32:11 INFO Master: Starting Spark master at spark://spark-master:7077</span><br><span class="line">18/03/29 03:32:11 INFO Master: Running Spark version 1.5.2</span><br><span class="line">18/03/29 03:32:22 INFO Utils: Successfully started service <span class="string">'MasterUI'</span> on port 8080.</span><br><span class="line">18/03/29 03:32:22 INFO MasterWebUI: Started MasterWebUI at http://172.30.48.2:8080</span><br><span class="line">18/03/29 03:32:22 INFO Utils: Successfully started service on port 6066.</span><br><span class="line">18/03/29 03:32:22 INFO StandaloneRestServer: Started REST server <span class="keyword">for</span> submitting applications on port 6066</span><br><span class="line">18/03/29 03:32:22 INFO Master: I have been elected leader! New state: ALIVE</span><br></pre></td></tr></table></figure><p><a href="https://github.com/aseigneurin/spark-ui-proxy" target="_blank" rel="noopener">Spark UI Proxy</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f spark-ui-proxy-rc.yaml</span><br><span class="line">$ kubectl create -f spark-ui-proxy-svc.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl get svc -n spark-cluster</span><br><span class="line">NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE</span><br><span class="line">spark-master     ClusterIP   10.254.137.145   &lt;none&gt;        7077/TCP,8080/TCP   1h</span><br><span class="line">spark-ui-proxy   NodePort    10.254.48.115    &lt;none&gt;        80:30080/TCP        2m</span><br></pre></td></tr></table></figure><p><img src="/images/spark1-k8s.png" alt></p><h2 id="deploy-workers"><a class="markdownIt-Anchor" href="#deploy-workers"></a> Deploy Workers</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f spark-worker-rc.yaml</span><br></pre></td></tr></table></figure><h2 id="zeppelin"><a class="markdownIt-Anchor" href="#zeppelin"></a> <a href="http://zeppelin.apache.org" target="_blank" rel="noopener">Zeppelin</a></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f zeppelin-rc.yaml</span><br><span class="line">$ kubectl create -f zeppelin-svc.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl <span class="built_in">exec</span> zeppelin-controller-pnn5h -it -n spark-cluster pyspark</span><br><span class="line"></span><br><span class="line">Python 2.7.9 (default, Mar  1 2015, 12:57:24) </span><br><span class="line">[GCC 4.9.2] on linux2</span><br><span class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> or <span class="string">"license"</span> <span class="keyword">for</span> more information.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  <span class="string">'_/</span></span><br><span class="line"><span class="string">   /__ / .__/\_,_/_/ /_/\_\   version 1.5.2</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Using Python version 2.7.9 (default, Mar  1 2015 12:57:24)</span></span><br><span class="line"><span class="string">SparkContext available as sc, HiveContext available as sqlContext.</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span></span><br></pre></td></tr></table></figure><p><img src="/images/spark2-k8s.png" alt></p><p><strong>References</strong></p><ul><li><a href="https://feisky.gitbooks.io/kubernetes/zh/machine-learning/spark.html" target="_blank" rel="noopener">Spark</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;Apache Spark™ is a fast and general engine for large-scale data processing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;gcr.io/google_containers/spark:1.5.2_v1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;---&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10.0.77.16/library/spark:1.5.2_v1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;elsonrodriguez/spark-ui-proxy:1.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;---&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10.0.77.16/library/spark-ui-proxy:1.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;gcr.io/google_containers/zeppelin:v0.5.6_v1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;---&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10.0.77.16/library/zeppelin:v0.5.6_v1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/acquaai/K8S/tree/master/yaml/Spark-K8s&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;YAML Files&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>Jenkins on Kubernetes</title>
    <link href="https://acquaai.github.io/2018/03/28/jenkins-k8s/"/>
    <id>https://acquaai.github.io/2018/03/28/jenkins-k8s/</id>
    <published>2018-03-28T09:00:11.000Z</published>
    <updated>2019-08-28T14:00:52.040Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/jenkinsonk8s1.png" alt></p><a id="more"></a><blockquote><p>由于部分 Jenkins Plugins 安装需要越墙，加之 Plugins 之间的依赖关系若全手动安装会把人逼疯掉，此时使用官方 jenkinsci/blueocean 镜像成为一个明智选择，此镜像已包含官方推荐的插件。Jenkins Slave 使用 jenkins/jnlp-slave:latest 镜像。</p></blockquote><h2 id="部署-jenkins"><a class="markdownIt-Anchor" href="#部署-jenkins"></a> 部署 Jenkins</h2><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">$ cat jenkins-pvc.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-secret</span><br><span class="line">  namespace: jenkins</span><br><span class="line">type: "kubernetes.io/rbd"  </span><br><span class="line">data:</span><br><span class="line">  key: QVFDd2hLZGFJYktSSHhBQVlmQ21vaitWUnNmUVhTczA3ODRLb3c9PQ==</span><br><span class="line">---</span><br><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">   name: ceph-jenkins</span><br><span class="line">   namespace: jenkins</span><br><span class="line">provisioner: kubernetes.io/rbd</span><br><span class="line">parameters:</span><br><span class="line">  monitors: 10.0.77.17:6789</span><br><span class="line">  adminId: admin</span><br><span class="line">  adminSecretName: ceph-secret</span><br><span class="line">  adminSecretNamespace: jenkins</span><br><span class="line">  pool: k8s</span><br><span class="line">  userId: admin</span><br><span class="line">  userSecretName: ceph-secret</span><br><span class="line">  fsType: xfs</span><br><span class="line">  imageFormat: "2"</span><br><span class="line">  imageFeatures: "layering"</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins-master-pvc</span><br><span class="line">  namespace: jenkins</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  storageClassName: ceph-jenkins  </span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 50Gi</span><br></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">$ cat jenkins-svc.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: jenkins</span><br><span class="line">  name: jenkins-admin</span><br><span class="line">  namespace: jenkins</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins-default</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: ["","extensions","app"]</span><br><span class="line">  resources: ["pods","pods/exec","deployments","replicasets"]</span><br><span class="line">  verbs: ["get","list","watch","create","update","patch","delete"]</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins-admin</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: jenkins</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: jenkins-default</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: jenkins-admin</span><br><span class="line">  namespace: jenkins</span><br><span class="line">---</span><br><span class="line">#Generally, don't need to create a service, the service here only for jnlp connect.</span><br><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: jenkins-master</span><br><span class="line">  name: jenkins-service</span><br><span class="line">  namespace: jenkins</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort   #attention</span><br><span class="line">  ports:</span><br><span class="line">    - port: 8080</span><br><span class="line">      name: jenkins</span><br><span class="line">    - port: 50000</span><br><span class="line">      name: agent</span><br><span class="line">  selector:</span><br><span class="line">    app: jenkins-master</span><br></pre></td></tr></table></figure><blockquote><p>Kubernetes Service Type 目前仅支持三种方式:</p></blockquote><ul><li><code>ClusterIP</code>: k8s集群内访问</li><li><code>LoadBalancer</code>: 依赖IaaS服务商（如Google Cloud、AWS）或自建负载均衡器</li><li><code>NodePort</code>: 通过 nodeIP:nodePORT 来访问</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">$ cat jenkins-deployment.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins</span><br><span class="line">  namespace: jenkins</span><br><span class="line">  labels:</span><br><span class="line">    app: jenkins-master</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: jenkins-master</span><br><span class="line">    spec:</span><br><span class="line">      securityContext:</span><br><span class="line">        runAsUser: 1000</span><br><span class="line">        fsGroup: 1000</span><br><span class="line">      serviceAccountName: jenkins-admin</span><br><span class="line">      containers:</span><br><span class="line">      - name: jenkins-master</span><br><span class="line">        image: 10.0.77.16/library/jenkinsci/blueocean</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          name: jenkins</span><br><span class="line">        - containerPort: 50000</span><br><span class="line">          name: agent</span><br><span class="line">          protocol: TCP</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1</span><br><span class="line">            memory: 1Gi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 0.5</span><br><span class="line">            memory: 500Mi</span><br><span class="line">        volumeMounts:</span><br><span class="line">          - name: docker</span><br><span class="line">            mountPath: /var/run/docker.sock</span><br><span class="line">          - name: jenkins-persistent-storage</span><br><span class="line">            mountPath: /var/jenkins_home</span><br><span class="line">        env:</span><br><span class="line">          - name: JAVA_OPTS</span><br><span class="line">            value: "-Duser.timezone=Asia/Shanghai"</span><br><span class="line">      volumes:</span><br><span class="line">      - name: docker</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /var/run/docker.sock</span><br><span class="line">      - name: jenkins-persistent-storage</span><br><span class="line">        persistentVolumeClaim:</span><br><span class="line">          claimName: jenkins-master-pvc</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get svc -n jenkins</span><br><span class="line">NAME              TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                          AGE</span><br><span class="line">jenkins-service   NodePort   10.254.112.72   &lt;none&gt;        8080:30514/TCP,50000:32589/TCP   4h</span><br><span class="line"></span><br><span class="line">$ kubectl get pods -n jenkins</span><br><span class="line">NAME                       READY     STATUS    RESTARTS   AGE</span><br><span class="line">jenkins-5d659f6b99-nwpxk   1/1       Running   0          3m</span><br><span class="line"></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -n jenkins jenkins-5d659f6b99-nwpxk -it -- bash</span><br><span class="line">bash-4.4$ cat /var/jenkins_home/secrets/initialAdminPassword</span><br><span class="line">5ad866a38c674a66a2a2fd6adc9702cd</span><br></pre></td></tr></table></figure><h2 id="配置-kubernetes-插件"><a class="markdownIt-Anchor" href="#配置-kubernetes-插件"></a> 配置 Kubernetes 插件</h2><p>手动下载<strong>kubernetes、kubernetes-credentials</strong>插件并安装。</p><p><img src="/images/jenkinsonk8s2.png" alt></p><h2 id="创建pipeline-job"><a class="markdownIt-Anchor" href="#创建pipeline-job"></a> 创建Pipeline Job</h2><p>Bypass for now.</p><p><strong>References</strong></p><ul><li><a href="https://zhangchenchen.github.io/2017/12/17/achieve-cicd-in-kubernetes-with-jenkins/" target="_blank" rel="noopener">Kuberbetes-- 利用Jenkins在Kubernetes中实践CI/CD</a></li><li><a href="https://kevinguo.me/2017/12/27/jenkins-on-kubernetes-with-pipeline/" target="_blank" rel="noopener">jenkins with pipeline on kubernetes</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/images/jenkinsonk8s1.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>Linux性能分析</title>
    <link href="https://acquaai.github.io/2018/03/26/linuxperf/"/>
    <id>https://acquaai.github.io/2018/03/26/linuxperf/</id>
    <published>2018-03-26T03:53:58.000Z</published>
    <updated>2019-08-28T14:00:52.042Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://p564fpez5.bkt.clouddn.com/image/linuxlinux_perf_tools_full.png" alt></p><a id="more"></a><p><a href="http://brendangregg.com" target="_blank" rel="noopener">Brendan Gregg</a> (Netflix Senior Performance Architect), 的<a href="https://medium.com/netflix-techblog/netflix-at-velocity-2015-linux-performance-tools-51964ddb81cf" target="_blank" rel="noopener">文章</a>讲述了 NETFLIX 大规模应用在 EC2 上的 Linux 性能分析（Use Methods）<strong><code>in 60,000 Milliseconds（1分钟）</code></strong>。</p><p>1分钟内如何做性能分析？</p><p>这就不得不提到运维人员吃饭干活，居家旅行必备的10 个命令（<strong><code>sysstat package</code></strong>）。</p><h2 id="10-commands"><a class="markdownIt-Anchor" href="#10-commands"></a> 10 Commands</h2><h3 id="uptime"><a class="markdownIt-Anchor" href="#uptime"></a> uptime</h3><p>了解系统负载趋势。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ uptime</span><br><span class="line"> 09:36:24 up 4 days, 18:56,  1 user,  load average: 0.22, 0.15, 0.18</span><br></pre></td></tr></table></figure><h3 id="dmesg-tail"><a class="markdownIt-Anchor" href="#dmesg-tail"></a> dmesg | tail</h3><p>显示系统最新的10条信息。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ dmesg |tail</span><br><span class="line">[183879.841702] device vethcb3685f entered promiscuous mode</span><br><span class="line">[183879.841838] IPv6: ADDRCONF(NETDEV_UP): vethcb3685f: link is not ready</span><br><span class="line">[183879.841847] docker0: port 7(vethcb3685f) entered forwarding state</span><br><span class="line">[183879.841856] docker0: port 7(vethcb3685f) entered forwarding state</span><br><span class="line">[183879.857773] docker0: port 7(vethcb3685f) entered disabled state</span><br><span class="line">[183879.953447] IPVS: Creating netns size=2040 id=10</span><br><span class="line">[183880.039549] IPv6: ADDRCONF(NETDEV_CHANGE): vethcb3685f: link becomes ready</span><br><span class="line">[183880.039595] docker0: port 7(vethcb3685f) entered forwarding state</span><br><span class="line">[183880.039604] docker0: port 7(vethcb3685f) entered forwarding state</span><br><span class="line">[183895.093600] docker0: port 7(vethcb3685f) entered forwarding state</span><br></pre></td></tr></table></figure><h3 id="vmstat-1"><a class="markdownIt-Anchor" href="#vmstat-1"></a> vmstat 1</h3><p>Virtual Memory Stat.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ vmstat 1</span><br><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   <span class="keyword">in</span>   cs us sy id wa st</span><br><span class="line"> 1  0      0 1393704   2104 2889104    0    0     1    21    2    6  1  0 98  0  0</span><br><span class="line"> 0  0      0 1393688   2104 2889140    0    0     0   119 3009 5868  1  1 98  1  0</span><br><span class="line"> 0  0      0 1393588   2104 2889148    0    0     0     8 2856 5327  1  1 98  0  0</span><br><span class="line"> 2  0      0 1393720   2104 2889148    0    0     0    68 2763 5495  1  0 99  0  0</span><br><span class="line"> 0  0      0 1393688   2104 2889148    0    0     0     8 2288 4503  1  0 99  0  0</span><br><span class="line"> 0  0      0 1393720   2104 2889148    0    0     0    56 2919 5640  1  0 99  0  0</span><br><span class="line">^C</span><br></pre></td></tr></table></figure><h3 id="mpstat-p-all-1"><a class="markdownIt-Anchor" href="#mpstat-p-all-1"></a> mpstat -P ALL 1</h3><p>查看多核CPU的负载状态，应用是否存在单线工作。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ mpstat -P ALL 1</span><br><span class="line">Linux 3.10.0-514.el7.x86_64 (n1.k8s.com)        03/26/2018      _x86_64_        (8 CPU)</span><br><span class="line"></span><br><span class="line">09:57:37 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle</span><br><span class="line">09:57:38 AM  all    0.75    0.00    0.38    2.76    0.00    0.00    0.00    0.00    0.00   96.11</span><br><span class="line">09:57:38 AM    0    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00</span><br><span class="line">09:57:38 AM    1    1.00    0.00    1.00    0.00    0.00    0.00    0.00    0.00    0.00   98.00</span><br><span class="line">09:57:38 AM    2    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00</span><br><span class="line">09:57:38 AM    3    1.98    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   98.02</span><br><span class="line">09:57:38 AM    4    1.01    0.00    0.00   22.22    0.00    0.00    0.00    0.00    0.00   76.77</span><br><span class="line">09:57:38 AM    5    1.01    0.00    1.01    0.00    0.00    0.00    0.00    0.00    0.00   97.98</span><br><span class="line">09:57:38 AM    6    0.00    0.00    1.01    0.00    0.00    0.00    0.00    0.00    0.00   98.99</span><br><span class="line">09:57:38 AM    7    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure><h3 id="pidstat-1"><a class="markdownIt-Anchor" href="#pidstat-1"></a> pidstat 1</h3><p>类似top，但持续输出信息，不覆盖。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ pidstat 1</span><br><span class="line">Linux 3.10.0-514.el7.x86_64 (n1.k8s.com)        03/26/2018      _x86_64_        (8 CPU)</span><br><span class="line"></span><br><span class="line">10:02:50 AM   UID       PID    %usr %system  %guest    %CPU   CPU  Command</span><br><span class="line">10:02:51 AM     0       712    0.00    0.98    0.00    0.98     7  kube-scheduler</span><br><span class="line">10:02:51 AM     0       978    0.98    0.98    0.00    1.96     7  etcd</span><br><span class="line">10:02:51 AM     0      1238    0.98    0.00    0.00    0.98     7  kube-apiserver</span><br><span class="line">10:02:51 AM     0      1450    0.98    0.98    0.00    1.96     6  kubelet</span><br><span class="line">10:02:51 AM     0     14585    0.98    0.98    0.00    1.96     1  pidstat</span><br><span class="line">10:02:51 AM     0     23261    0.98    0.00    0.00    0.98     7  fluentd</span><br><span class="line"></span><br><span class="line">10:02:51 AM   UID       PID    %usr %system  %guest    %CPU   CPU  Command</span><br><span class="line">10:02:52 AM     0       712    1.00    0.00    0.00    1.00     2  kube-scheduler</span><br><span class="line">10:02:52 AM   167       976    0.00    1.00    0.00    1.00     7  ceph-mon</span><br><span class="line">10:02:52 AM     0       978    1.00    1.00    0.00    2.00     7  etcd</span><br><span class="line">10:02:52 AM     0      1238    2.00    0.00    0.00    2.00     7  kube-apiserver</span><br><span class="line">10:02:52 AM     0      1286    5.00    0.00    0.00    5.00     4  dockerd</span><br><span class="line">10:02:52 AM     0      1301    1.00    0.00    0.00    1.00     6  docker-containe</span><br><span class="line">10:02:52 AM     0      1450    5.00    0.00    0.00    5.00     6  kubelet</span><br><span class="line">10:02:52 AM   999      6656    0.00    1.00    0.00    1.00     0  java</span><br><span class="line">10:02:52 AM   999      6879    1.00    0.00    0.00    1.00     7  java</span><br><span class="line">10:02:52 AM     0     14585    0.00    1.00    0.00    1.00     1  pidstat</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure><h3 id="iostat-xz-1"><a class="markdownIt-Anchor" href="#iostat-xz-1"></a> iostat -xz 1</h3><p><code>r/s，w/s，rkB/s，wkB/s:</code>分别表示每秒设备读次数，写次数，读的KB数，写的KB数。它们描述了磁盘的工作负载。也许性能问题就是由过高的负载所造成的。</p><p><code>await:</code>I/O平均时间，以毫秒作单位。它是应用中I/O处理所实际消耗的时间，因为其中既包括排队用时也包括处理用时。如果它比预期的大，就意味着设备饱和了，或者设备出了问题。</p><p><code>avgqu-sz:</code>分配给设备的平均请求数。大于1表示设备已经饱和了。（不过有些设备可以并行处理请求，比如由多个磁盘组成的虚拟设备）</p><p><code>%util:</code>设备使用率。这个值显示了设备每秒内工作时间的百分比，一般都处于高位。低于60%通常是低性能的表现（也可以从await中看出），不过这个得看设备的类型。接近100%通常意味着饱和。</p><p><code>Attention:</code>disk I/O性能低不一定是问题。应用的I/O往往是异步的（比如预读（read-ahead）和写缓冲（buffering for writes）），所以不一定会被阻塞并遭受延迟。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ iostat -xz 1</span><br><span class="line">Linux 3.10.0-514.el7.x86_64 (n1.k8s.com)        03/26/2018      _x86_64_        (8 CPU)</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           1.04    0.00    0.43    0.14    0.00   98.38</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sda               0.00     2.08    0.05   13.81     4.61   163.14    24.22     0.07    5.16   15.64    5.12   1.26   1.74</span><br><span class="line">dm-0              0.00     0.00    0.04   15.89     4.58   163.13    21.05     0.11    6.93   16.69    6.90   1.10   1.75</span><br><span class="line">dm-1              0.00     0.00    0.00    0.00     0.01     0.00    33.58     0.00    3.39    3.39    0.00   2.71   0.00</span><br><span class="line">dm-2              0.00     0.00    0.00    0.00     0.01     0.00    41.74     0.00    6.74    4.59  143.25   4.86   0.00</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure><h3 id="free-m"><a class="markdownIt-Anchor" href="#free-m"></a> free -m</h3><p><code>Attention:</code> buffer（<strong>缓冲</strong>）用于块设备，cache（<strong>缓存</strong>）用于文件系统。使用 ZFS 文件系统可能会造成一种缺少空闲内存的假象，ZFS 自己的文件系统缓存不计算在<code>free -m</code>内。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ free -m</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:           7983        3746        1255           1        2981        3879</span><br><span class="line">Swap:             0           0           0</span><br></pre></td></tr></table></figure><h3 id="sar-n-dev-1"><a class="markdownIt-Anchor" href="#sar-n-dev-1"></a> sar -n DEV 1</h3><p>用于查看网络流量。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ sar -n DEV 1</span><br><span class="line">Linux 3.10.0-514.el7.x86_64 (n1.k8s.com)        03/26/2018      _x86_64_        (8 CPU)</span><br><span class="line"></span><br><span class="line">10:26:58 AM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s</span><br><span class="line">10:26:59 AM vethdd9260f      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">10:26:59 AM vethcb3685f      6.00      4.00      0.63      0.42      0.00      0.00      0.00</span><br><span class="line">10:26:59 AM vethb280b46      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">10:26:59 AM vethb9914b2      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">10:26:59 AM vetha23f71e      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">10:26:59 AM        lo     45.00     45.00     10.41     10.41      0.00      0.00      0.00</span><br><span class="line">10:26:59 AM veth1a67b80      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">10:26:59 AM    ens160     89.00     88.00     10.67     16.42      0.00      0.00      0.00</span><br><span class="line">10:26:59 AM flannel.1      4.00      6.00      0.37      0.55      0.00      0.00      0.00</span><br><span class="line">10:26:59 AM   docker0      6.00      4.00      0.55      0.42      0.00      0.00      0.00</span><br><span class="line">10:26:59 AM veth1bb7d82      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure><h3 id="sar-n-tcpetcp-1"><a class="markdownIt-Anchor" href="#sar-n-tcpetcp-1"></a> sar -n TCP,ETCP 1</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ sar -n TCP,ETCP 1</span><br><span class="line">Linux 3.10.0-514.el7.x86_64 (n1.k8s.com)        03/26/2018      _x86_64_        (8 CPU)</span><br><span class="line"></span><br><span class="line">10:31:09 AM  active/s passive/s    iseg/s    oseg/s</span><br><span class="line">10:31:10 AM      1.00      0.00    202.00    206.00</span><br><span class="line"></span><br><span class="line">10:31:09 AM  atmptf/s  estres/s retrans/s isegerr/s   orsts/s</span><br><span class="line">10:31:10 AM      0.00      0.00      0.00      0.00      1.00</span><br><span class="line"></span><br><span class="line">10:31:10 AM  active/s passive/s    iseg/s    oseg/s</span><br><span class="line">10:31:11 AM      0.00      0.00     70.00     69.00</span><br><span class="line">[...]</span><br></pre></td></tr></table></figure><h3 id="top"><a class="markdownIt-Anchor" href="#top"></a> top</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ top</span><br><span class="line">top - 10:33:44 up 4 days, 19:53,  2 users,  load average: 0.30, 0.25, 0.21</span><br><span class="line">Tasks: 203 total,   1 running, 202 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  0.7 us,  0.5 sy,  0.0 ni, 98.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">KiB Mem :  8175148 total,  1275752 free,  3840864 used,  3058532 buff/cache</span><br><span class="line">KiB Swap:        0 total,        0 free,        0 used.  3967860 avail Mem </span><br><span class="line"></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                                   </span><br><span class="line"> 1450 root      20   0  990376  82964  29064 S   2.3  1.0 190:36.23 kubelet                                                                                                                   </span><br><span class="line"> 1238 root      20   0  404012 263748  32680 S   2.0  3.2 172:04.16 kube-apiserver                                                                                                            </span><br><span class="line">  978 root      20   0 10.213g 107832  13040 S   1.7  1.3 171:20.35 etcd                                                                                                                      </span><br><span class="line"> 1286 root      20   0  909376  60972  18820 S   1.7  0.7 153:23.73 dockerd                                                                                                                   </span><br><span class="line">  712 root      20   0   54924  27172  13376 S   1.0  0.3  53:16.15 kube-scheduler                                                                                                            </span><br><span class="line"> 6723 systemd+  20   0 6320356 867680  16256 S   0.7 10.6  11:21.24 java                                                                                                                      </span><br><span class="line">   22 root      rt   0       0      0      0 S   0.3  0.0   0:24.12 migration/3                                                                                                               </span><br><span class="line">  971 root      20   0   54948  23564  13252 S   0.3  0.3  13:46.43 kube-proxy                                                                                                                </span><br><span class="line">  976 ceph      20   0  400784  53644  11012 S   0.3  0.7  24:56.87 ceph-mon                                                                                                                  </span><br><span class="line"> 1301 root      20   0  904872  28140  13036 S   0.3  0.3  24:26.56 docker-containe                                                                                                           </span><br><span class="line"> 6656 systemd+  20   0 7463428 765872  16348 S   0.3  9.4   6:11.82 java                                                                                                                      </span><br><span class="line">12235 root       0 -20       0      0      0 S   0.3  0.0   0:00.58 kworker/5:1H                                                                                                              </span><br><span class="line">23261 root      20   0 1722644 321224   7456 S   0.3  3.9  43:57.80 fluentd                                                                                                                   </span><br><span class="line">27263 root      20   0       0      0      0 S   0.3  0.0   0:11.16 kworker/1:2                                                                                                               </span><br><span class="line">    1 root      20   0  191092   4164   2436 S   0.0  0.1   0:09.49 systemd                                                                                                                   </span><br><span class="line">    2 root      20   0       0      0      0 S   0.0  0.0   0:00.36 kthreadd   </span><br><span class="line">[...]</span><br></pre></td></tr></table></figure><h2 id="tools"><a class="markdownIt-Anchor" href="#tools"></a> Tools</h2><ul><li><a href="http://brendangregg.com/perf.html" target="_blank" rel="noopener">perf_events</a></li><li><a href="https://github.com/brendangregg/perf-tools" target="_blank" rel="noopener">perf-tools</a></li><li><a href="http://brendangregg.com/linuxperf.html" target="_blank" rel="noopener">even more …</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ ll perf-tools/bin</span><br><span class="line">total 0</span><br><span class="line">lrwxrwxrwx 1 root root 16 Mar 26 09:02 bitesize -&gt; ../disk/bitesize</span><br><span class="line">lrwxrwxrwx 1 root root 15 Mar 26 09:02 cachestat -&gt; ../fs/cachestat</span><br><span class="line">lrwxrwxrwx 1 root root 12 Mar 26 09:02 execsnoop -&gt; ../execsnoop</span><br><span class="line">lrwxrwxrwx 1 root root 19 Mar 26 09:02 funccount -&gt; ../kernel/funccount</span><br><span class="line">lrwxrwxrwx 1 root root 19 Mar 26 09:02 funcgraph -&gt; ../kernel/funcgraph</span><br><span class="line">lrwxrwxrwx 1 root root 20 Mar 26 09:02 funcslower -&gt; ../kernel/funcslower</span><br><span class="line">lrwxrwxrwx 1 root root 19 Mar 26 09:02 functrace -&gt; ../kernel/functrace</span><br><span class="line">lrwxrwxrwx 1 root root 12 Mar 26 09:02 iolatency -&gt; ../iolatency</span><br><span class="line">lrwxrwxrwx 1 root root 10 Mar 26 09:02 iosnoop -&gt; ../iosnoop</span><br><span class="line">lrwxrwxrwx 1 root root 12 Mar 26 09:02 killsnoop -&gt; ../killsnoop</span><br><span class="line">lrwxrwxrwx 1 root root 16 Mar 26 09:02 kprobe -&gt; ../kernel/kprobe</span><br><span class="line">lrwxrwxrwx 1 root root 12 Mar 26 09:02 opensnoop -&gt; ../opensnoop</span><br><span class="line">lrwxrwxrwx 1 root root 22 Mar 26 09:02 perf-stat-hist -&gt; ../misc/perf-stat-hist</span><br><span class="line">lrwxrwxrwx 1 root root 21 Mar 26 09:02 reset-ftrace -&gt; ../tools/reset-ftrace</span><br><span class="line">lrwxrwxrwx 1 root root 11 Mar 26 09:02 syscount -&gt; ../syscount</span><br><span class="line">lrwxrwxrwx 1 root root 17 Mar 26 09:02 tcpretrans -&gt; ../net/tcpretrans</span><br><span class="line">lrwxrwxrwx 1 root root 16 Mar 26 09:02 tpoint -&gt; ../system/tpoint</span><br><span class="line">lrwxrwxrwx 1 root root 14 Mar 26 09:02 uprobe -&gt; ../user/uprobe</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://p564fpez5.bkt.clouddn.com/image/linuxlinux_perf_tools_full.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>SonarQube on Kubernetes</title>
    <link href="https://acquaai.github.io/2018/03/23/sonar/"/>
    <id>https://acquaai.github.io/2018/03/23/sonar/</id>
    <published>2018-03-23T11:18:41.000Z</published>
    <updated>2019-09-20T14:51:55.140Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文中PostgreSQL使用 <a href="https://acquaai.github.io/2019/09/16/using-ceph-rbd-persistent-storage/">Ceph RBD</a> 作为持久卷，SonarQube使用 NFS 作为持久卷。</p></blockquote><h2 id="安装-postgresql"><a class="markdownIt-Anchor" href="#安装-postgresql"></a> 安装 PostgreSQL</h2><h3 id="创建-postgresql-secret"><a class="markdownIt-Anchor" href="#创建-postgresql-secret"></a> 创建 PostgreSQL Secret</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> -n sonar | base64</span><br><span class="line">c29uYXI=</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> -n Passw0rd | base64</span><br><span class="line">UGFzc3cwcmQ=</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> -n sonardb | base64</span><br><span class="line">c29uYXJkYg==</span><br><span class="line"></span><br><span class="line"><span class="comment">#与 echo -n 作用相同</span></span><br><span class="line">$ tr --delete <span class="string">'\n'</span> &lt; mysql-root-pwd.txt &gt; .strippedmysql-root-pwd.txt &amp;&amp; mv .strippedmysql-root-pwd.txt mysql-root-pwd.txt</span><br><span class="line"></span><br><span class="line">OR</span><br><span class="line">$ kubectl create secret generic mysql-secrets --from-literal=root-user=root --from-literal=root-password=Passw0rd --from-literal=mysql-database=sonardb --from-literal=mysql-user=sonar --from-literal=mysql-password=sonar -n sonar</span><br><span class="line"></span><br><span class="line">OR</span><br><span class="line">$ kubectl create secret generic mysql-root-pwd --from-file=mysql-root-pwd.txt -n sonar</span><br></pre></td></tr></table></figure><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ cat pg-secrets.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: postgres-secrets</span><br><span class="line">  namespace: sonar</span><br><span class="line">  labels:</span><br><span class="line">    app: postgres</span><br><span class="line">data:</span><br><span class="line">  postgres-db: c29uYXJkYg==</span><br><span class="line">  postgres-user: c29uYXI=</span><br><span class="line">  postgres-password: c29uYXI=</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f pg-secrets.yaml</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get secrets -n sonar</span><br><span class="line">NAME                  TYPE                                  DATA      AGE</span><br><span class="line">ceph-secret           kubernetes.io/rbd                     1         8d</span><br><span class="line">default-token-fzbwt   kubernetes.io/service-account-token   3         16d</span><br><span class="line">postgres-secrets      Opaque                                3         3d</span><br><span class="line"></span><br><span class="line">$ kubectl describe secrets/postgres-secrets  -n sonar</span><br><span class="line">Name:         postgres-secrets</span><br><span class="line">Namespace:    sonar</span><br><span class="line">Labels:       app=postgres</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Type:  Opaque</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line">postgres-db:        7 bytes</span><br><span class="line">postgres-password:  5 bytes</span><br><span class="line">postgres-user:      5 bytes</span><br></pre></td></tr></table></figure><h3 id="部署-postgresql"><a class="markdownIt-Anchor" href="#部署-postgresql"></a> 部署 PostgreSQL</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">$ cat pg-statefulset.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: postgres</span><br><span class="line">  namespace: sonar</span><br><span class="line">  labels:</span><br><span class="line">    app: postgres</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">    - port: 5432</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: postgres</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: postgres</span><br><span class="line">  namespace: sonar</span><br><span class="line">  labels:</span><br><span class="line">    app: postgres</span><br><span class="line">spec:</span><br><span class="line">  serviceName: postgres</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: postgres</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 10</span><br><span class="line">      containers:</span><br><span class="line">      - name: postgresql</span><br><span class="line">        image: 10.0.77.16/library/postgres:10.3</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 5432</span><br><span class="line">          name: postgresql</span><br><span class="line">        env:</span><br><span class="line">        - name: POSTGRES_USER</span><br><span class="line">          valueFrom:</span><br><span class="line">            secretKeyRef:</span><br><span class="line">              name: postgres-secrets</span><br><span class="line">              key: postgres-user</span><br><span class="line">        - name: POSTGRES_DB</span><br><span class="line">          valueFrom:</span><br><span class="line">            secretKeyRef:</span><br><span class="line">              name: postgres-secrets</span><br><span class="line">              key: postgres-db</span><br><span class="line">        - name: POSTGRES_PASSWORD</span><br><span class="line">          valueFrom:</span><br><span class="line">            secretKeyRef:</span><br><span class="line">              name: postgres-secrets</span><br><span class="line">              key: postgres-password</span><br><span class="line">        - name: PGDATA</span><br><span class="line">          value: /var/lib/postgresql/data/pgdata</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: postgres-volume</span><br><span class="line">          mountPath: /var/lib/postgresql/data</span><br><span class="line">      imagePullSecrets: </span><br><span class="line">        - name: registrykey</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">  - metadata:</span><br><span class="line">      name: postgres-volume</span><br><span class="line">    spec:</span><br><span class="line">      accessModes:</span><br><span class="line">        - ReadWriteOnce</span><br><span class="line">      storageClassName: ceph-postgres</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 10Gi</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f pg-statefulset.yaml</span><br></pre></td></tr></table></figure><blockquote><p>volumeClaimTemplates中定义了PVC，并指定了<code>storageClassName: &quot;ceph-postgres&quot;</code>，会根据PVC动态创建Pod所需的PV。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pvc -n sonar</span><br><span class="line">NAME                         STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE</span><br><span class="line">postgres-volume-postgres-0   Bound     pvc-df69da97-2e7a-11e8-b7b9-0050568879ea   10Gi       RWO            ceph-postgres   5m</span><br><span class="line"></span><br><span class="line">$ kubectl get pv -n sonar</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                              STORAGECLASS    REASON    AGE</span><br><span class="line">pvc-df69da97-2e7a-11e8-b7b9-0050568879ea   10Gi       RWO            Delete           Bound     sonar/postgres-volume-postgres-0   ceph-postgres             7m</span><br><span class="line"></span><br><span class="line">$ kubectl describe persistentvolume  pvc-df69da97-2e7a-11e8-b7b9-0050568879ea</span><br><span class="line">Name:            pvc-df69da97-2e7a-11e8-b7b9-0050568879ea</span><br><span class="line">Labels:          &lt;none&gt;</span><br><span class="line">Annotations:     kubernetes.io/createdby=rbd-dynamic-provisioner</span><br><span class="line">                 pv.kubernetes.io/bound-by-controller=yes</span><br><span class="line">                 pv.kubernetes.io/provisioned-by=kubernetes.io/rbd</span><br><span class="line">StorageClass:    ceph-postgres</span><br><span class="line">Status:          Bound</span><br><span class="line">Claim:           sonar/postgres-volume-postgres-0</span><br><span class="line">Reclaim Policy:  Delete</span><br><span class="line">Access Modes:    RWO</span><br><span class="line">Capacity:        10Gi</span><br><span class="line">Message:         </span><br><span class="line">Source:</span><br><span class="line">    Type:          RBD (a Rados Block Device mount on the host that shares a pod<span class="string">'s lifetime)</span></span><br><span class="line"><span class="string">    CephMonitors:  [10.0.77.17:6789]</span></span><br><span class="line"><span class="string">    RBDImage:      kubernetes-dynamic-pvc-df734e53-2e7a-11e8-93af-00505688047e</span></span><br><span class="line"><span class="string">    FSType:        xfs</span></span><br><span class="line"><span class="string">    RBDPool:       rbd</span></span><br><span class="line"><span class="string">    RadosUser:     admin</span></span><br><span class="line"><span class="string">    Keyring:       /etc/ceph/keyring</span></span><br><span class="line"><span class="string">    SecretRef:     &amp;&#123;ceph-secret &#125;</span></span><br><span class="line"><span class="string">    ReadOnly:      false</span></span><br><span class="line"><span class="string">Events:            &lt;none&gt;</span></span><br></pre></td></tr></table></figure><h2 id="部署-sonarqube"><a class="markdownIt-Anchor" href="#部署-sonarqube"></a> 部署 Sonarqube</h2><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">$ cat sonar-rc.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: sonar</span><br><span class="line">  namespace: sonar</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 9000</span><br><span class="line">    protocol: TCP</span><br><span class="line">    nodePort: 30001</span><br><span class="line">  selector:</span><br><span class="line">    name: sonar</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ReplicationController</span><br><span class="line">metadata:</span><br><span class="line">  name: sonar</span><br><span class="line">  namespace: sonar</span><br><span class="line">  labels:</span><br><span class="line">    name: sonar</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    name: sonar</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: sonar</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: sonar</span><br><span class="line">        image: 10.0.77.16/library/sonarqube:6.7</span><br><span class="line">        env:</span><br><span class="line">        - name: SONARQUBE_JDBC_USERNAME</span><br><span class="line">          valueFrom:</span><br><span class="line">            secretKeyRef:</span><br><span class="line">              name: postgres-secrets</span><br><span class="line">              key: postgres-user</span><br><span class="line">        - name: SONARQUBE_JDBC_PASSWORD</span><br><span class="line">          valueFrom:</span><br><span class="line">            secretKeyRef:</span><br><span class="line">              name: postgres-secrets</span><br><span class="line">              key: postgres-password</span><br><span class="line">        - name: SONARQUBE_JDBC_URL</span><br><span class="line">          value: jdbc:postgresql://postgres:5432/sonardb</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 9000</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: "/opt/sonarqube/conf"</span><br><span class="line">          name: sonar-conf</span><br><span class="line">        - mountPath: "/opt/sonarqube/data"</span><br><span class="line">          name: sonar-data</span><br><span class="line">        - mountPath: "/opt/sonarqube/extensions"</span><br><span class="line">          name: sonar-extensions</span><br><span class="line">        - mountPath: "/opt/sonarqube/logs"</span><br><span class="line">          name: sonar-logs</span><br><span class="line">      volumes:</span><br><span class="line">        - name: sonar-conf</span><br><span class="line">          nfs:</span><br><span class="line">            server: 10.0.77.16</span><br><span class="line">            path: "/nfs/sonar/conf"</span><br><span class="line">        - name: sonar-data</span><br><span class="line">          nfs:</span><br><span class="line">            server: 10.0.77.16</span><br><span class="line">            path: "/nfs/sonar/data"</span><br><span class="line">        - name: sonar-extensions</span><br><span class="line">          nfs:</span><br><span class="line">            server: 10.0.77.16</span><br><span class="line">            path: "/nfs/sonar/extensions"</span><br><span class="line">        - name: sonar-logs</span><br><span class="line">          nfs:</span><br><span class="line">            server: 10.0.77.16</span><br><span class="line">            path: "/nfs/sonar/logs"</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f sonar-rc.yaml</span><br></pre></td></tr></table></figure><p>使用默认用户、密码 admin/admin 访问SonarQube <a href="http://k8s-node-ip:30001" target="_blank" rel="noopener">http://k8s-node-ip:30001</a></p><h2 id="sonarqube-plugins"><a class="markdownIt-Anchor" href="#sonarqube-plugins"></a> SonarQube Plugins</h2><p><a href="https://docs.sonarqube.org/display/PLUG" target="_blank" rel="noopener">Plugin Library</a></p><p>SonarQube v6.7预装好的Plugins如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -n sonar</span><br><span class="line">NAME          READY     STATUS    RESTARTS   AGE</span><br><span class="line">postgres-0    1/1       Running   0          15h</span><br><span class="line">sonar-wmvnf   1/1       Running   0          15h</span><br><span class="line"></span><br><span class="line">$ kubectl <span class="built_in">exec</span> -n sonar sonar-wmvnf -it -- bash</span><br><span class="line">root@sonar-wmvnf:/opt/sonarqube/extensions/plugins<span class="comment"># ls -l</span></span><br><span class="line">total 40464</span><br><span class="line">-rw-r--r-- 1 sonarqube sonarqube 2703958 Nov  6 08:12 sonar-csharp-plugin-6.5.0.3766.jar</span><br><span class="line">-rw-r--r-- 1 sonarqube sonarqube 1618672 Nov  6 08:12 sonar-flex-plugin-2.3.jar</span><br><span class="line">-rw-r--r-- 1 sonarqube sonarqube 6759535 Nov  6 15:31 sonar-java-plugin-4.15.0.12310.jar</span><br><span class="line">-rw-r--r-- 1 sonarqube sonarqube 3355702 Nov  6 08:12 sonar-javascript-plugin-3.2.0.5506.jar</span><br><span class="line">-rw-r--r-- 1 sonarqube sonarqube 3022870 Nov  6 16:50 sonar-php-plugin-2.11.0.2485.jar</span><br><span class="line">-rw-r--r-- 1 sonarqube sonarqube 4024311 Nov  6 08:12 sonar-python-plugin-1.8.0.1496.jar</span><br><span class="line">-rw-r--r-- 1 sonarqube sonarqube 3625962 Nov  6 08:12 sonar-scm-git-plugin-1.3.0.869.jar</span><br><span class="line">-rw-r--r-- 1 sonarqube sonarqube 6680471 Nov  6 08:12 sonar-scm-svn-plugin-1.6.0.860.jar</span><br><span class="line">-rw-r--r-- 1 sonarqube sonarqube 2250667 Nov  6 08:12 sonar-typescript-plugin-1.1.0.1079.jar</span><br><span class="line">-rw-r--r-- 1 sonarqube sonarqube 7368250 Nov  6 08:12 sonar-xml-plugin-1.4.3.1027.jar</span><br></pre></td></tr></table></figure><h2 id="sonarqube-scanner"><a class="markdownIt-Anchor" href="#sonarqube-scanner"></a> SonarQube Scanner</h2><p><a href="https://docs.sonarqube.org/display/SCAN/Analyzing+with+SonarQube+Scanner" target="_blank" rel="noopener">Analyzing with SonarQube Scanner</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@sonar-wmvnf:~<span class="comment"># cd /opt/sonarqube/extensions/downloads</span></span><br><span class="line">root@sonar-wmvnf:/opt/sonarqube/extensions/downloads<span class="comment"># wget https://sonarsource.bintray.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-3.1.0.1141-linux.zip</span></span><br><span class="line">root@sonar-wmvnf:/opt/sonarqube/extensions/downloads<span class="comment"># unzip sonar-scanner-cli-3.1.0.1141-linux.zip -d /opt/sonarqube</span></span><br><span class="line">root@sonar-wmvnf:/opt/sonarqube<span class="comment"># mv sonar-scanner-3.1.0.1141-linux sonar-scanner</span></span><br><span class="line">root@sonar-wmvnf:/opt/sonarqube<span class="comment"># chown -R sonarqube:sonarqube sonar-scanner/</span></span><br></pre></td></tr></table></figure><p>增加 SonarQube Scanner 的环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl cp sonar/sonar-wmvnf:/etc/profile ./</span><br><span class="line">tar: Removing leading `/<span class="string">' from member names</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ vi profile</span></span><br><span class="line"><span class="string">......</span></span><br><span class="line"><span class="string">/opt/sonarqube/sonar-scanner/bin</span></span><br><span class="line"><span class="string">......</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl cp ./profile sonar/sonar-wmvnf:/etc/</span><br><span class="line"></span><br><span class="line">root@sonar-wmvnf:/opt/sonarqube<span class="comment"># source /etc/profile</span></span><br><span class="line">root@sonar-wmvnf:/opt/sonarqube<span class="comment"># sonar-scanner -h</span></span><br><span class="line">INFO: </span><br><span class="line">INFO: usage: sonar-scanner [options]</span><br><span class="line">INFO: </span><br><span class="line">INFO: Options:</span><br><span class="line">INFO:  -D,--define &lt;arg&gt;     Define property</span><br><span class="line">INFO:  -h,--<span class="built_in">help</span>             Display <span class="built_in">help</span> information</span><br><span class="line">INFO:  -v,--version          Display version information</span><br><span class="line">INFO:  -X,--debug            Produce execution debug output</span><br></pre></td></tr></table></figure><p>配置 SonarQube Scanner 属性</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cat sonar-scanner.properties</span><br><span class="line"></span><br><span class="line">sonar.host.url=http://localhost:9000</span><br><span class="line">sonar.sourceEncoding=UTF-8</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl cp ./sonar-scanner.properties sonar/sonar-wmvnf:/opt/sonarqube/sonar-scanner/conf/sonar-scanner.properties</span><br></pre></td></tr></table></figure><p>配置 Python 代码分析</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ cat sonar-project.properties</span><br><span class="line">sonar.projectKey=my:python</span><br><span class="line">sonar.projectName=python</span><br><span class="line">sonar.projectVersion=1.0</span><br><span class="line">sonar.sources=.</span><br><span class="line">sonar.sourceEncoding=UTF-8</span><br><span class="line"></span><br><span class="line">sonar.language=py</span><br><span class="line"><span class="comment">#sonar.python.pylint=/usr/bin/pylint</span></span><br><span class="line"><span class="comment">#sonar.python.pylint_config=.pylintrc</span></span><br><span class="line"><span class="comment">#sonar.python.pylint.reportPath=./pylint-report.txt</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@sonar-wmvnf:/opt/sonarqube/conf<span class="comment"># mkdir python</span></span><br><span class="line"></span><br><span class="line">$ kubectl cp ./sonar-project.properties sonar/sonar-wmvnf:/opt/sonarqube/conf/python/sonar-project.properties</span><br><span class="line"></span><br><span class="line">root@sonar-wmvnf:/opt/sonarqube/conf<span class="comment"># chown -R sonarqube:sonarqube python</span></span><br></pre></td></tr></table></figure><p>执行分析</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">root@sonar-wmvnf:/opt/sonarqube/conf/python<span class="comment"># sonar-scanner -Dsonar.projectKey=my:python -Dsonar.sources=.</span></span><br><span class="line"></span><br><span class="line">INFO: Scanner configuration file: /opt/sonarqube/sonar-scanner/conf/sonar-scanner.properties</span><br><span class="line">INFO: Project root configuration file: /opt/sonarqube/conf/python/sonar-project.properties</span><br><span class="line">INFO: SonarQube Scanner 3.1.0.1141</span><br><span class="line">......</span><br><span class="line">......</span><br><span class="line">INFO: ANALYSIS SUCCESSFUL, you can browse http://localhost:9000/dashboard/index/my:python</span><br><span class="line">INFO: Note that you will be able to access the updated dashboard once the server has processed the submitted analysis report</span><br><span class="line">INFO: More about the report processing at http://localhost:9000/api/ce/task?id=AWJW_N6U1qxTjZHyQmBu</span><br><span class="line">INFO: Task total time: 4.424 s</span><br><span class="line">INFO: ------------------------------------------------------------------------</span><br><span class="line">INFO: EXECUTION SUCCESS</span><br><span class="line">INFO: ------------------------------------------------------------------------</span><br><span class="line">INFO: Total time: 6.144s</span><br><span class="line">INFO: Final Memory: 11M/335M</span><br><span class="line">INFO: ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><p><img src="/images/sonarPython.png" alt></p><h2 id="jenkins集成sonarqube"><a class="markdownIt-Anchor" href="#jenkins集成sonarqube"></a> Jenkins集成Sonarqube</h2><ul><li>[Analyzing with SonarQube Scanner for Jenkins](Analyzing with SonarQube Scanner for Jenkins)</li></ul><p><strong>Reference</strong></p><ul><li><a href="https://jimmysong.io/kubernetes-handbook/practice/using-ceph-for-persistent-storage.html" target="_blank" rel="noopener">使用Ceph做持久化存储创建MySQL集群</a></li><li><a href="http://rancher.com/running-highly-available-wordpress-mysql-kubernetes/" target="_blank" rel="noopener">Running Highly Available WordPress with MySQL on Kubernetes</a></li><li><a href="https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/nfs" target="_blank" rel="noopener">NFS PV©</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文中PostgreSQL使用 &lt;a href=&quot;https://acquaai.github.io/2019/09/16/using-ceph-rbd-persistent-storage/&quot;&gt;Ceph RBD&lt;/a&gt; 作为持久卷，SonarQube使用 NFS 作为持久卷。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;安装-postgresql&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#安装-postgresql&quot;&gt;&lt;/a&gt; 安装 PostgreSQL&lt;/h2&gt;
&lt;h3 id=&quot;创建-postgresql-secret&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#创建-postgresql-secret&quot;&gt;&lt;/a&gt; 创建 PostgreSQL Secret&lt;/h3&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; -n sonar | base64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;c29uYXI=&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; -n Passw0rd | base64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;UGFzc3cwcmQ=&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; -n sonardb | base64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;c29uYXJkYg==&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#与 echo -n 作用相同&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ tr --delete &lt;span class=&quot;string&quot;&gt;&#39;\n&#39;&lt;/span&gt; &amp;lt; mysql-root-pwd.txt &amp;gt; .strippedmysql-root-pwd.txt &amp;amp;&amp;amp; mv .strippedmysql-root-pwd.txt mysql-root-pwd.txt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;OR&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ kubectl create secret generic mysql-secrets --from-literal=root-user=root --from-literal=root-password=Passw0rd --from-literal=mysql-database=sonardb --from-literal=mysql-user=sonar --from-literal=mysql-password=sonar -n sonar&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;OR&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ kubectl create secret generic mysql-root-pwd --from-file=mysql-root-pwd.txt -n sonar&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>Ceph Distributed Storage System</title>
    <link href="https://acquaai.github.io/2018/03/13/ceph-cluster/"/>
    <id>https://acquaai.github.io/2018/03/13/ceph-cluster/</id>
    <published>2018-03-13T09:18:28.000Z</published>
    <updated>2019-08-28T14:00:52.036Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ceph-介绍"><a class="markdownIt-Anchor" href="#ceph-介绍"></a> Ceph 介绍</h2><p>Ceph 是一个符合 POSIX 的开源分布式存储系统，能提供 Ceph Object、Ceph RBD 和 Ceph FS 的存储能力。Ceph Cluster至少需要一个<code>Ceph Monitor</code> 和两个<code>OSD Daemon</code>，当运行 Ceph 文件系统客户端时，还必须要有元数据服务器（MDS）。</p><p>Components:</p><ul><li>Monitors: 基于 PAXOS 算法维护集群状态的各种图表，包括监视器图、OSD图、PG图、CRUSH图。Ceph 保存( <code>epoch</code> )发生在 Monitor、OSD 和 PG 上的每一次状态变更的历史信息。</li><li>OSDs: 存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其它 OSD 守护进程的心跳来向 Monitors 提供监控信息。当存储集群设定为 2个副本 时，至少需要 2个OSD守护进程，集群才能达到<code>active + clean</code>的状态，Ceph 默认为 3副本。</li><li>MDSs: 存储 Ceph FS 元数据。（针对 Object/RBD 存储不需要 MDS）MDS也可以在多节点部署实现冗余。MDS守护进程可以被配置为活跃或者被动状态，活跃的MDS为主MDS，其他的MDS处于备用状态，当主MDS节点故障时，备用MDS节点会接管其工作并被提升为主节点。</li></ul><a id="more"></a><p><img src="/images/cephstack.png" alt></p><p><img src="/images/cepharch.png" alt></p><p><strong>RADOS</strong></p><p>RADOS (Reliable Autonomic Distributed Object Store)自身就是一套完整的对象存储系统，包括 Ceph 的基础服务（Monitor、OSD、MDS），存储应用数据，同时提供 Ceph 的高可用性、高扩展性和高自动化特性。</p><p><strong>LibRados</strong></p><p>对 RADOS 进行抽象和封装，向上层提供不同的 API（RGW、RBD、FS）。提供 C/C++ 原生 API。</p><p><strong>APP的接口</strong></p><p>抽象 LibRados，便于应用和客户端使用。</p><ul><li>RGW: 提供与 S3 和 Swift 兼容的 RESTful API 的 gateway。通过 RGW 可以将 RADOS 响应转化为 HTTP 响应，反之亦然。</li><li>RBD: 提供一个标准的块设备接口服务。</li><li>CephFS: 提供一个 POSIX 兼容的分布式文件系统。</li></ul><p><strong>Client</strong></p><p>Ceph 应用接口的应用方式，如基于 LibRados 直接开发的对象存储应用，基于 RGW 开发的对象存储应用，基于RBD实现的云硬盘等。</p><p>一个 Ceph Cluster 逻辑上可以划分为多个 Pool，一个 Pool 由若干个逻辑 PG 组成。</p><p>一个文件会被切分为多个 Object，每个 Object 被映射到一个 PG，每个 PG 会根据 CRUSH 算法映射到一组 OSD（根据副本数），其中第一个 OSD（Primary OSD）为主，其它为备，OSD之间通过心跳来互相监控存活状态。一般来讲增加 PG 的数量能降低 OSD 负载，每个OSD大约分配 50～100 PG。</p><p>CRUSH: 是 Ceph 使用的数据分布算法，类似一致性哈希，使数据分配到预期的地方。</p><h2 id="部署-ceph-集群"><a class="markdownIt-Anchor" href="#部署-ceph-集群"></a> 部署 Ceph 集群</h2><table><thead><tr><th>IP</th><th>Hostname</th><th style="text-align:right">Roles</th></tr></thead><tbody><tr><td>10.0.77.16</td><td><a href="http://repo.k8s.com" target="_blank" rel="noopener">repo.k8s.com</a></td><td style="text-align:right">admin-node, deploy-node</td></tr><tr><td>10.0.77.17</td><td><a href="http://n1.k8s.com" target="_blank" rel="noopener">n1.k8s.com</a></td><td style="text-align:right">mon</td></tr><tr><td>10.0.77.18</td><td><a href="http://n2.k8s.com" target="_blank" rel="noopener">n2.k8s.com</a></td><td style="text-align:right">osd.1</td></tr><tr><td>10.0.77.19</td><td><a href="http://n3.k8s.com" target="_blank" rel="noopener">n3.k8s.com</a></td><td style="text-align:right">osd.2</td></tr></tbody></table><h3 id="ceph-yum-源"><a class="markdownIt-Anchor" href="#ceph-yum-源"></a> Ceph yum 源</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ~]# cat /etc/yum.repos.d/ceph.repo</span><br><span class="line"></span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=https://download.ceph.com/rpm-jewel/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">priority=2</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.ceph.com/keys/release.asc</span><br></pre></td></tr></table></figure><h3 id="安装-ceph-deploy"><a class="markdownIt-Anchor" href="#安装-ceph-deploy"></a> 安装 ceph-deploy</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ~]<span class="comment"># yum update -y &amp;&amp; yum -y install ceph-deploy</span></span><br></pre></td></tr></table></figure><h3 id="配置-ceph-用户"><a class="markdownIt-Anchor" href="#配置-ceph-用户"></a> 配置 Ceph 用户</h3><p><code>every node</code>新建 ceph 集群 k8sceph 用户，并确保该用户具有sudo权限。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ useradd -d /home/k8sceph -m k8sceph</span><br><span class="line">$ passwd k8sceph</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">"k8sceph ALL = (root) NOPASSWD:ALL"</span> | tee /etc/sudoers.d/k8sceph</span><br><span class="line">$ chmod 0440 /etc/sudoers.d/k8sceph</span><br><span class="line"></span><br><span class="line"><span class="comment">#禁用 TTY</span></span><br><span class="line">在 CentOS 和 RHEL 上执行 ceph-deploy 命令时可能会报错。如果你的 Ceph 节点默认设置了 requiretty ，执行 sudo visudo 禁用它，并找到 Defaults requiretty 选项，把它改为 Defaults:ceph !requiretty 或者直接注释掉，这样 ceph-deploy 就可以用之前创建的用户连接了。</span><br><span class="line"></span><br><span class="line">$ visudo -f /etc/sudoers</span><br><span class="line"></span><br><span class="line">Defaults:k8sceph     !requiretty</span><br><span class="line"></span><br><span class="line"><span class="comment">#优先级/首选项</span></span><br><span class="line">$ yum -y install yum-plugin-priorities</span><br></pre></td></tr></table></figure><h3 id="配置-ssh-免密登录"><a class="markdownIt-Anchor" href="#配置-ssh-免密登录"></a> 配置 SSH 免密登录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ~]<span class="comment"># ssh-keygen</span></span><br><span class="line">[root@repo ~]<span class="comment"># ssh-copy-id k8sceph@n1.k8s.com</span></span><br><span class="line">[root@repo ~]<span class="comment"># ssh-copy-id k8sceph@n2.k8s.com</span></span><br><span class="line">[root@repo ~]<span class="comment"># ssh-copy-id k8sceph@n3.k8s.com</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#修改 repo.k8s.com 控制节点上 ~/.ssh/config 文件，设置当不指定用户时登录到 n1~n3 的用户为 k8sceph.</span></span><br><span class="line">[root@repo ~]<span class="comment"># vi ~/.ssh/config</span></span><br><span class="line">Host c1</span><br><span class="line">   Hostname c1</span><br><span class="line">   User sdsceph</span><br><span class="line">Host c2</span><br><span class="line">   Hostname c2</span><br><span class="line">   User sdsceph</span><br><span class="line">Host c3</span><br><span class="line">   Hostname c3</span><br><span class="line">   User sdsceph</span><br><span class="line"></span><br><span class="line"><span class="comment">#repo.k8s.com 控制节点上 ssh 无密登录到 n1~n3 的测试</span></span><br><span class="line">[root@repo ~]<span class="comment"># ssh n1.k8s.com</span></span><br></pre></td></tr></table></figure><h3 id="配置-ntp-服务"><a class="markdownIt-Anchor" href="#配置-ntp-服务"></a> 配置 NTP 服务</h3><p><code>every node:</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ntpstat </span><br><span class="line">synchronised to NTP server (192.168.0.118) at stratum 4 </span><br><span class="line">   time correct to within 56 ms</span><br><span class="line">   polling server every 1024 s</span><br></pre></td></tr></table></figure><h2 id="安装-ceph"><a class="markdownIt-Anchor" href="#安装-ceph"></a> 安装 Ceph</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ceph-cluster]<span class="comment"># mkdir ceph-cluster &amp;&amp; cd ceph-cluster</span></span><br></pre></td></tr></table></figure><p>在管理节点上，进入刚创建的放置配置文件的目录，用 ceph-deploy 执行如下步骤。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ceph-cluster]<span class="comment"># ceph-deploy new n1.k8s.com</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#生成下面3个文件：</span></span><br><span class="line">[root@repo ceph-cluster]<span class="comment"># ls</span></span><br><span class="line">ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring</span><br></pre></td></tr></table></figure><p>把Ceph 配置文件里的ceph.conf里的默认副本数从 3 改成 2， 这样只有两个 OSD 也可以达到 active + clean 状态。把下面这行加入 [global] 段：</p><p><code>osd pool default size = 2</code></p><p>从管理节点执行各节点安装ceph：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ceph-cluster]<span class="comment"># ceph-deploy install repo.k8s.com n1.k8s.com n2.k8s.com n3.k8s.com</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#各个节点安装完后显示：</span></span><br><span class="line">Complete!</span><br><span class="line">Running <span class="built_in">command</span>: sudo ceph --version</span><br><span class="line">ceph version 10.2.10 (5dc1e4c05cb68dbf62ae6fce3f0700e4654fdbbe)</span><br></pre></td></tr></table></figure><p>ceph-deploy 将在各节点安装 Ceph 。 注：如果你执行过 ceph-deploy purge ，你必须重新执行这一步来安装 Ceph 。</p><p>配置初始 monitor(s)、并收集所有密钥，这里只有 n1：</p><blockquote><p>没有识别完整的主机名 <a href="http://n1.k8s.com" target="_blank" rel="noopener">n1.k8s.com</a>，无赖在/etc/hosts文中增加了 n1。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ceph-cluster]# ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure><p>完成上述操作后，当前目录里应该会出现这些密钥环：</p><ul><li>ceph.bootstrap-mds.keyring</li><li>ceph.bootstrap-mgr.keyring</li><li>ceph.bootstrap-osd.keyring</li><li>ceph.bootstrap-rgw.keyring</li><li>ceph.client.admin.keyring</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ceph-cluster]# ll</span><br><span class="line">total 268</span><br><span class="line">-rw------- 1 root root    113 Mar 13 16:02 ceph.bootstrap-mds.keyring</span><br><span class="line">-rw------- 1 root root     71 Mar 13 16:02 ceph.bootstrap-mgr.keyring</span><br><span class="line">-rw------- 1 root root    113 Mar 13 16:02 ceph.bootstrap-osd.keyring</span><br><span class="line">-rw------- 1 root root    113 Mar 13 16:02 ceph.bootstrap-rgw.keyring</span><br><span class="line">-rw------- 1 root root    129 Mar 13 16:02 ceph.client.admin.keyring</span><br><span class="line">-rw-r--r-- 1 root root    215 Mar 13 15:26 ceph.conf</span><br><span class="line">-rw-r--r-- 1 root root 245513 Mar 13 16:02 ceph-deploy-ceph.log</span><br><span class="line">-rw------- 1 root root     73 Mar 13 15:23 ceph.mon.keyring</span><br></pre></td></tr></table></figure><blockquote><p>只有在安装 Hammer 或更高版时才会创建 bootstrap-rgw 密钥环。<br>如果此步失败并输出类似于如下信息 “Unable to find /etc/ceph/ceph.client.admin.keyring”，请确认 ceph.conf 中为 monitor 指定的 IP 是 Public IP，而不是 Private IP。</p></blockquote><p>n1.k8s.com上看到ceph monitor进程已经启动:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@n1 ~]<span class="comment"># ps -ef|grep ceph-mon|grep -v grep</span></span><br><span class="line">ceph     17915     1  0 15:58 ?        00:00:00 /usr/bin/ceph-mon -f --cluster ceph --id n1 --setuser ceph --setgroup ceph</span><br></pre></td></tr></table></figure><p>接下来添加两个OSD到n2和n3。OSD是存储数据的节点，实际上需要为OSD提供独立的存储空间，如一个独立的磁盘。为 OSD 及其日志使用独立硬盘或分区，请参考<a href="http://docs.ceph.org.cn/rados/deployment/ceph-deploy-osd" target="_blank" rel="noopener">ceph-deploy osd</a>。</p><p>本学习使用系统本地磁盘创建目录提供给OSD使用。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ceph-cluster]<span class="comment"># ssh n2.k8s.com</span></span><br><span class="line">[k8sceph@n2 ~]$ sudo mkdir -p /ceph/osd0</span><br><span class="line">[k8sceph@n2 ~]$ sudo chown -R ceph:ceph /ceph/osd0</span><br><span class="line">[k8sceph@n2 ~]$ <span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line">[root@repo ceph-cluster]<span class="comment"># ssh n3.k8s.com</span></span><br><span class="line">[k8sceph@n3 ~]$ sudo mkdir -p /ceph/osd1</span><br><span class="line">[k8sceph@n3 ~]$ sudo chown -R ceph:ceph /ceph/osd1</span><br><span class="line">[k8sceph@n3 ~]$ <span class="built_in">exit</span></span><br></pre></td></tr></table></figure><p>从管理节点执行 ceph-deploy 来准备 OSD :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ceph-cluster]<span class="comment"># ceph-deploy osd prepare n2.k8s.com:/ceph/osd0 n3.k8s.com:/ceph/osd1</span></span><br></pre></td></tr></table></figure><p>激活 OSD :</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ceph-cluster]<span class="comment"># ceph-deploy osd activate n2.k8s.com:/ceph/osd0 n3.k8s.com:/ceph/osd1</span></span><br></pre></td></tr></table></figure><p>用 ceph-deploy 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样你每次执行 Ceph 命令行时就无需指定 monitor 地址和 ceph.client.admin.keyring 了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ceph-cluster]# chmod +r /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">[root@repo ceph-cluster]# ceph-deploy admin repo.k8s.com n1.k8s.com n2.k8s.com n3.k8s.com</span><br></pre></td></tr></table></figure><blockquote><p>ceph-deploy 和本地管理主机（ admin-node ）通信时，必须通过主机名可达。必要时可修改 /etc/hosts ，加入管理主机的名字。</p></blockquote><p>确保对 ceph.client.admin.keyring 有正确的操作权限:<br>$ sudo chmod +r /etc/ceph/ceph.client.admin.keyring</p><p>查看ceph集群中OSD节点的状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ceph-cluster]<span class="comment"># ceph osd tree</span></span><br><span class="line">ID WEIGHT  TYPE NAME     UP/DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line">-1 0.35339 root default                                    </span><br><span class="line">-2 0.17670     host n2                                     </span><br><span class="line"> 0 0.17670         osd.0      up  1.00000          1.00000 </span><br><span class="line">-3 0.17670     host n3                                     </span><br><span class="line"> 1 0.17670         osd.1      up  1.00000          1.00000</span><br></pre></td></tr></table></figure><p>查看集群健康状态:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ceph-cluster]<span class="comment"># ceph health</span></span><br><span class="line">HEALTH_OK</span><br></pre></td></tr></table></figure><p>如果在某些地方碰到麻烦，想从头再来，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">可以用下列命令清除配置：</span><br><span class="line">ceph-deploy purgedata &#123;ceph-node&#125; [&#123;ceph-node&#125;]</span><br><span class="line">ceph-deploy forgetkeys</span><br><span class="line"></span><br><span class="line">用下列命令可以连 Ceph 安装包一起清除：</span><br><span class="line">ceph-deploy purge &#123;ceph-node&#125; [&#123;ceph-node&#125;]</span><br></pre></td></tr></table></figure><h2 id="操作集群"><a class="markdownIt-Anchor" href="#操作集群"></a> 操作集群</h2><h3 id="对象存储测试"><a class="markdownIt-Anchor" href="#对象存储测试"></a> 对象存储测试</h3><p>要把对象存入 Ceph 存储集群，客户端必须做到：</p><ul><li>指定对象名</li><li>指定存储池</li></ul><p>查看集群中的存储池：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ceph-cluster]<span class="comment"># ceph osd lspools</span></span><br><span class="line">0 rbd,</span><br></pre></td></tr></table></figure><p>集群上只有rbd一个存储池，创建名为 k8s 的存储池:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ceph-cluster]<span class="comment"># ceph osd pool create k8s 100   # 100个 PG</span></span><br><span class="line">pool <span class="string">'k8s'</span> created</span><br></pre></td></tr></table></figure><p>先创建一个对象，用 rados put 命令加上对象名、一个有数据的测试文件路径、并指定存储池:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ~]<span class="comment"># echo &#123;Test-data&#125; &gt; testfile.txt</span></span><br><span class="line"></span><br><span class="line">rados put &#123;object-name&#125; &#123;file-path&#125; --pool=&#123;pool-name&#125;</span><br><span class="line"></span><br><span class="line">[root@repo ~]<span class="comment"># rados put test-object-1 testfile.txt --pool=k8s</span></span><br></pre></td></tr></table></figure><p>确认 Ceph 存储集群存储了此对象:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ~]<span class="comment"># rados -p k8s ls</span></span><br><span class="line"><span class="built_in">test</span>-object-1</span><br></pre></td></tr></table></figure><p>定位对象:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph osd map &#123;pool-name&#125; &#123;object-name&#125;</span><br><span class="line"></span><br><span class="line">[root@repo ~]<span class="comment"># ceph osd map k8s test-object-1</span></span><br><span class="line">osdmap e12 pool <span class="string">'k8s'</span> (1) object <span class="string">'test-object-1'</span> -&gt; pg 1.74dc35e2 (1.62) -&gt; up ([0,1], p0) acting ([0,1], p0)</span><br></pre></td></tr></table></figure><p>删除对象:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@repo ~]<span class="comment"># rados rm test-object-1 --pool=k8s</span></span><br></pre></td></tr></table></figure><h3 id="文件系统测试"><a class="markdownIt-Anchor" href="#文件系统测试"></a> 文件系统测试</h3><p><a href="http://docs.ceph.org.cn/rados/deployment/ceph-deploy-mds/" target="_blank" rel="noopener"><strong>Important:</strong></a> 必须部署至少一个元数据服务器才能使用 CephFS 文件系统，多个元数据服务器并行运行仍处于实验阶段，不要在生产环境下运行多个元数据服务器。</p><p>在 Ceph 管理节点上执行如下命令为集群增加一个MDS:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mds create &#123;host-name&#125;[:&#123;daemon-name&#125;] [&#123;host-name&#125;[:&#123;daemon-name&#125;] ...]</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy --overwrite-conf mds create n1.k8s.com</span><br></pre></td></tr></table></figure><p>1.创建 CephFS</p><p>CephFS需要使用两个Pool来分别存储数据和元数据，分别创建fs_data和fs_metadata两个Pool。</p><blockquote><p>PG 数量指定一般遵循的公式：</p></blockquote><blockquote><ul><li>集群 PG 总数 = ( OSD总数 * 100 ) / 数据最大副本数</li><li>单个存储池 PG 数 = ( OSD总数 * 100 ) / 数据最大副本数 / 存储池数</li></ul></blockquote><p>PG 的最终结果以最接近上述公式的 2<sup>N</sup> 向上取值，如：<br>PG = 2 * 100 / 2 / 2 = 50，向上取 2<sup>6</sup> 为 64，最接近 50，故 <code>PG = 64</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd pool create fs_data 128</span><br><span class="line">pool <span class="string">'fs_data'</span> created</span><br><span class="line"></span><br><span class="line">$ ceph osd pool create fs_metadata 128</span><br><span class="line">pool <span class="string">'fs_metadata'</span> created</span><br><span class="line"></span><br><span class="line">$ ceph osd lspools</span><br><span class="line">0 rbd,1 k8s,2 fs_data,3 fs_metadata,</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ceph fs new cephfs fs_metadata fs_data</span><br><span class="line">new fs with metadata pool 3 and data pool 2</span><br><span class="line"></span><br><span class="line">$ ceph fs ls</span><br><span class="line">name: cephfs, metadata pool: fs_metadata, data pools: [fs_data ]</span><br></pre></td></tr></table></figure><p>2.Mount CephFS</p><p>客户端访问Ceph FS有两种方式：</p><ul><li>Kernel内核驱动：Linux内核从2.6.34版本开始加入对CephFS的原声支持</li><li>Ceph FS FUSE： FUSE即Filesystem in Userspace</li></ul><p>3.用内核驱动挂载 Ceph 文件系统</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@mysql01 ~]$ uname -r</span><br><span class="line">3.10.0-693.5.2.el7.x86_64</span><br><span class="line"></span><br><span class="line">[root@mysql01 ~]$ mkdir /mnt/cephfs</span><br><span class="line">[root@mysql01 ~]$ mkdir /etc/ceph</span><br><span class="line"></span><br><span class="line">[root@mysql01 ~]$ scp -P 8086 root@10.0.77.16:/root/ceph-cluster/ceph.client.admin.keyring /etc/ceph</span><br><span class="line"></span><br><span class="line">[root@mysql01 ceph]$ mv ceph.client.admin.keyring admin.secret</span><br><span class="line">[root@mysql01 ceph]$ cat admin.secret</span><br><span class="line">AQCwhKdaIbKRHxAAYfCmoj+VRsfQXSs0784Kow==</span><br><span class="line"></span><br><span class="line">[root@mysql01 ~]$ mount -t ceph 10.0.77.17:6789,10.0.77.18:6789,10.0.77.19:6789:/ /mnt/cephfs -o name=admin,secretfile=admin.secret</span><br><span class="line"></span><br><span class="line">Filesystem                                         Size  Used Avail Use% Mounted on</span><br><span class="line">10.0.77.17:6789,10.0.77.18:6789,10.0.77.19:6789:/  362G   30G  333G   9% /mnt/cephfs</span><br></pre></td></tr></table></figure><blockquote><p><code>secretfile= option</code>时，CentOS7.3 - 3.10.0-693.5.2.el7.x86_64有bug，CentOS7.4 - 3.10.0-514.el7.x86_64时正常，直接使用<code>secret= option</code>时都正常。</p></blockquote><p><strong>Reference</strong></p><ul><li><a href="http://docs.ceph.org.cn/start/" target="_blank" rel="noopener">Ceph Documentation</a></li><li><a href="http://docs.ceph.com/docs/master/start/quick-cephfs/#" target="_blank" rel="noopener">CEPH FS QUICK START</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ceph-介绍&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#ceph-介绍&quot;&gt;&lt;/a&gt; Ceph 介绍&lt;/h2&gt;
&lt;p&gt;Ceph 是一个符合 POSIX 的开源分布式存储系统，能提供 Ceph Object、Ceph RBD 和 Ceph FS 的存储能力。Ceph Cluster至少需要一个&lt;code&gt;Ceph Monitor&lt;/code&gt; 和两个&lt;code&gt;OSD Daemon&lt;/code&gt;，当运行 Ceph 文件系统客户端时，还必须要有元数据服务器（MDS）。&lt;/p&gt;
&lt;p&gt;Components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monitors: 基于 PAXOS 算法维护集群状态的各种图表，包括监视器图、OSD图、PG图、CRUSH图。Ceph 保存( &lt;code&gt;epoch&lt;/code&gt; )发生在 Monitor、OSD 和 PG 上的每一次状态变更的历史信息。&lt;/li&gt;
&lt;li&gt;OSDs: 存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其它 OSD 守护进程的心跳来向 Monitors 提供监控信息。当存储集群设定为 2个副本 时，至少需要 2个OSD守护进程，集群才能达到&lt;code&gt;active + clean&lt;/code&gt;的状态，Ceph 默认为 3副本。&lt;/li&gt;
&lt;li&gt;MDSs: 存储 Ceph FS 元数据。（针对 Object/RBD 存储不需要 MDS）MDS也可以在多节点部署实现冗余。MDS守护进程可以被配置为活跃或者被动状态，活跃的MDS为主MDS，其他的MDS处于备用状态，当主MDS节点故障时，备用MDS节点会接管其工作并被提升为主节点。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>K8S持久化卷</title>
    <link href="https://acquaai.github.io/2018/03/10/k8s-pv/"/>
    <id>https://acquaai.github.io/2018/03/10/k8s-pv/</id>
    <published>2018-03-10T03:40:44.000Z</published>
    <updated>2019-08-28T14:00:52.041Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>声明：本文来自<a href="https://jimmysong.io/posts/kubernetes-persistent-volume/" target="_blank" rel="noopener">jimmysong</a>的文章，整理记录在此仅为加深对 K8S PV 学习和理解。</p></blockquote><p>在 K8S 中没有使用<strong>持久化卷</strong>是不完整的，PV作为子系统为管理员和用户提供了API。</p><h2 id="apis"><a class="markdownIt-Anchor" href="#apis"></a> APIs</h2><p><code>PersistentVolume</code>（PV）管理员配置的存储，作为 K8S 群集的一部分。类似节点是集群中的资源一样，PV 也是集群中的资源。 PV 是 Volume 之类的卷插件，具有独立于 使用PV的Pod 的生命周期。卷的生命比 Pod 中的所有容器都长，当容器重启时数据仍然得以保存。只有当 Pod 消亡时，卷将不复存在。Kubernetes 支持多种类型的卷，Pod 可以同时使用任意数量的卷。该API对象包含存储实现的细节，即 NFS、RBD 或云厂商的存储系统。</p><p><code>PersistentVolumeClaim</code>（PVC）用户对存储的请求，与 Pod 类似。Pod 消耗节点资源，PVC 消耗 PV 资源。PVC 请求特定的大小和访问模式。</p><h2 id="卷和声明claim的生命周期"><a class="markdownIt-Anchor" href="#卷和声明claim的生命周期"></a> 卷和声明(claim)的生命周期</h2><p>PV作为集群中的一项资源，PVC消费PV资源，也作为对资源的消费检查。</p><a id="more"></a><h3 id="配置provision"><a class="markdownIt-Anchor" href="#配置provision"></a> 配置（Provision）</h3><h4 id="静态-pv"><a class="markdownIt-Anchor" href="#静态-pv"></a> 静态 PV</h4><p>集群管理员创建的PV，供集群用户使用的实际存储细节，用于消费。</p><h4 id="动态-pv"><a class="markdownIt-Anchor" href="#动态-pv"></a> 动态 PV</h4><p>当管理员创建的静态PV不匹配用户的消费时，集群可能会尝试动态地为 PVC 创建卷。此配置基于 StorageClasses：PVC 必须请求存储类，且管理员必须创建并配置该类才能进行动态创建。声明该类为 “” 可以有效地禁用其动态配置。</p><p>要启用基于存储级别的动态存储配置，集群管理员需要启用 API server 上的 DefaultStorageClass 准入控制器。例如，通过确保 DefaultStorageClass 位于 API server 组件的 --admission-control 标志，使用逗号分隔的有序值列表中，可以完成此操作。</p><h3 id="绑定"><a class="markdownIt-Anchor" href="#绑定"></a> 绑定</h3><p>在动态配置的情况下，用户创建或已经创建了具有特定存储量的 PersistentVolumeClaim 以及某些访问模式。master 中的控制环路监视新的 PVC，寻找匹配的 PV（如果可能），并将它们绑定在一起。如果为新的 PVC 动态调配 PV，则该环路始终将该 PV 绑定到 PVC。否则，用户总会得到他们所请求的存储，但是容量可能超出要求的数量。一旦 PV 和 PVC 绑定后，PersistentVolumeClaim 绑定是排它性的，不管它们是如何绑定的。 PVC 跟 PV 绑定是一对一的映射。</p><p>如果没有匹配的卷，声明将无限期地保持未绑定状态。随着匹配卷的可用，声明将被绑定。例如，配置了许多 50Gi PV的集群将不会匹配请求 100Gi 的PVC。将100Gi PV 添加到群集时，可以绑定 PVC。</p><h3 id="使用"><a class="markdownIt-Anchor" href="#使用"></a> 使用</h3><p>用户进行了声明，并且该声明是绑定的，则只要用户需要，绑定的 PV 就属于该用户。用户通过在 Pod 的 volume 配置中包含 persistentVolumeClaim 来调度 Pod 并访问用户声明的 PV。</p><h3 id="持久化卷声明的保护"><a class="markdownIt-Anchor" href="#持久化卷声明的保护"></a> 持久化卷声明的保护</h3><p>PVC 保护的目的是确保由 pod 正在使用的 PVC 不会从系统中移除，避免数据丢失或损坏。</p><p><code>当 pod 状态为 Pending 并且 pod 已经分配给节点或 pod 为 Running 状态时，PVC 处于活动状态。</code></p><ul><li>A v1.9 or higher Kubernetes must be installed.</li><li>As PVC Protection is a Kubernetes v1.9 alpha feature it must be enabled:</li></ul><ol><li>Admission controller must be started with the PVC Protection plugin.</li><li>All Kubernetes components must be started with the PVCProtection alpha features enabled.</li></ol><h3 id="回收"><a class="markdownIt-Anchor" href="#回收"></a> 回收</h3><p>用户用完 volume 后，可以从允许回收资源的 API 中删除 PVC 对象。PersistentVolume 的回收策略告诉集群在存储卷声明释放后应如何处理该卷：<code>保留 | 回收 | 删除</code></p><h4 id="保留"><a class="markdownIt-Anchor" href="#保留"></a> 保留</h4><p>允许手动回收资源。当 PersistentVolumeClaim 被删除时，PersistentVolume 仍然存在，volume 被视为“已释放”。</p><p>由于前一个声明人的数据仍然存在，所以还不能马上进行其他声明。管理员可以通过以下步骤手动回收卷：</p><ol><li>删除 PersistentVolume。在删除 PV 后，外部基础架构中的关联存储资产（如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）仍然存在。</li><li>手动清理相关存储资产上的数据。</li><li>手动删除关联的存储资产，或者如果要重新使用相同的存储资产，再使用存储资产定义创建新的 PersistentVolume。</li></ol><h4 id="删除"><a class="markdownIt-Anchor" href="#删除"></a> 删除</h4><p>对于支持删除回收策略的卷插件，删除操作将从 Kubernetes 中删除 PersistentVolume 对象，并删除外部基础架构（如 AWS EBS、GCE PD、Azure Disk 或 Cinder 卷）中的关联存储资产。动态配置的卷继承其 StorageClass 的回收策略，默认为 Delete。管理员应该根据用户的期望来配置 StorageClass，否则就必须要在 PV 创建后进行编辑或修补。<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#persistentvolumeclaim-v1-core" target="_blank" rel="noopener">persistentVolumeReclaimPolicy</a></p><h4 id="回收-2"><a class="markdownIt-Anchor" href="#回收-2"></a> 回收</h4><p>如果存储卷插件支持，回收策略会在 volume 上执行基本擦除（rm -rf / thevolume / *），可被再次声明使用。</p><p>管理员可以使用 <a href="https://kubernetes.io/docs/reference/generated/kube-controller-manager/" target="_blank" rel="noopener">Kubernetes controller manager 命令行参数</a>来配置自定义回收站 pod 模板。自定义回收站 pod 模板必须包含 volumes 规范，volumes 部分的自定义回收站模块中指定的特定路径将被替换为正在回收的卷的特定路径。</p><h3 id="扩展持久化卷声明"><a class="markdownIt-Anchor" href="#扩展持久化卷声明"></a> 扩展持久化卷声明</h3><p>K8S v1.9版本中，支持扩展持久化卷类型：</p><ul><li>gcePersistentDisk</li><li>awsElasticBlockStore</li><li>Cinder</li><li>glusterfs</li><li>rbd</li></ul><p>管理员可以通过设置 ExpandPersistentVolumes 特性为 true来 允许扩展持久卷声明。管理员还应该启用<a href="https://kubernetes.io/docs/admin/admission-controllers/#persistentvolumeclaimresize" target="_blank" rel="noopener">PersistentVolumeClaimResize</a>准入控制插件来调整大小的卷的其他验证。</p><p>只要 PersistentVolumeClaimResize 准入插件开启，将只允许 allowVolumeExpansion 字段设置为 true 的存储类进行大小调整。</p><p>在任何情况下都不会创建新的 PersistentVolume 来满足声明。 Kubernetes 将尝试调整现有 volume 来满足声明的要求。</p><p>扩展卷包括 文件系统，只有在<code>ReadWrite</code>模式下使用 PersistentVolumeClaim 启动新Pod时，才会执行文件系统大小调整。 换句话说，如果正在扩展的卷在pod或部署中使用，则需要删除并重新创建pod。只有<code>XFS, Ext3, Ext4</code>支持文件系统大小调整。</p><h2 id="持久化卷类型"><a class="markdownIt-Anchor" href="#持久化卷类型"></a> 持久化卷类型</h2><p>PersistentVolume以插件的形式实现。Kubernetes目前支持的卷类型:</p><ul><li>GCEPersistentDisk</li><li>AWSElasticBlockStore</li><li>AzureFile</li><li>AzureDisk</li><li>FC (Fibre Channel)**</li><li>FlexVolume</li><li>Flocker</li><li>NFS</li><li>iSCSI</li><li>RBD (Ceph Block Device)</li><li>CephFS</li><li>Cinder (OpenStack block storage)</li><li>Glusterfs</li><li>VsphereVolume</li><li>Quobyte Volumes</li><li>HostPath (Single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster)</li><li>VMware Photon</li><li>Portworx Volumes</li><li>ScaleIO Volumes</li><li>StorageOS</li><li>Raw Block Support exists for these plugins only.</li></ul><h3 id="持久化卷类型示例"><a class="markdownIt-Anchor" href="#持久化卷类型示例"></a> 持久化卷类型示例</h3><h4 id="emptydir"><a class="markdownIt-Anchor" href="#emptydir"></a> emptyDir</h4><p>当 Pod 被分配给节点时，首先创建 emptyDir 卷，只要该 Pod 在该节点上运行，该卷就会存在，但它最初是空的。Pod 中的容器可以读取和写入 emptyDir 卷中的相同文件，尽管该卷可以挂载到每个容器中的相同或不同路径上。当从节点中删除 Pod 时，emptyDir 中的数据将被永久删除。</p><p>容器崩溃不会从节点中移除 pod，因此 emptyDir 卷中的数据在容器崩溃时是安全的。</p><p>emptyDir 的用法：</p><ul><li>暂存空间，例如用于基于磁盘的合并排序</li><li>用作长时间计算崩溃恢复时的检查点</li><li>Web服务器容器提供数据时，保存内容管理器容器提取的文件</li></ul><p>example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pd</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: k8s.gcr.io/test-webserver</span><br><span class="line">    name: test-container</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /cache</span><br><span class="line">      name: cache-volume</span><br><span class="line">  volumes:</span><br><span class="line">  - name: cache-volume</span><br><span class="line">    emptyDir: &#123;&#125;</span><br></pre></td></tr></table></figure><h4 id="hostpath"><a class="markdownIt-Anchor" href="#hostpath"></a> hostPath</h4><p>hostPath 卷将主机节点的文件系统中的文件或目录挂载到集群中。该功能大多数 Pod 都用不到(跨节点访问)。</p><p>hostPath 的用途如下：</p><ul><li>运行需要访问 Docker 内部的容器；使用 /var/lib/docker 的 hostPath</li><li>在容器中运行 cAdvisor；使用 /dev/cgroups 的 hostPath</li><li>允许 pod 指定给定的 hostPath 是否应该在 pod 运行之前存在，是否应该创建，以及它应该以什么形式存在</li></ul><p>注意：</p><ul><li>由于每个节点上的文件都不同，具有相同配置（例如从 podTemplate 创建的）的 pod 在不同节点上的行为可能会有所不同</li><li>当 Kubernetes 按照计划添加资源感知调度时，将无法考虑 hostPath 使用的资源</li><li>在底层主机上创建的文件或目录只能由 root 写入。您需要在特权容器中以 root 身份运行进程，或修改主机上的文件权限以便写入 hostPath 卷</li></ul><p>example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pd</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: k8s.gcr.io/test-webserver</span><br><span class="line">    name: test-container</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /test-pd</span><br><span class="line">      name: test-volume</span><br><span class="line">  volumes:</span><br><span class="line">  - name: test-volume</span><br><span class="line">    hostPath:</span><br><span class="line">      # directory location on host</span><br><span class="line">      path: /data</span><br><span class="line">      # this field is optional</span><br><span class="line">      type: Directory</span><br></pre></td></tr></table></figure><h4 id="cephfs"><a class="markdownIt-Anchor" href="#cephfs"></a> cephfs</h4><p>cephfs 卷允许将现有的 CephFS 卷挂载到容器中，与 emptyDir 不同的是，当删除 Pod 时cephfs 卷上的内容被保留，仅仅是卸载卷。与 awsElasticBlockStore 一样可以预先填充数据，并且可以在数据包之间&quot;切换&quot;数据。 CephFS 能被多个写设备同时挂载。</p><p><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/cephfs/" target="_blank" rel="noopener">CephFS example</a></p><h4 id="secret"><a class="markdownIt-Anchor" href="#secret"></a> secret</h4><p>secret 卷用于将敏感信息（如密码）传递到 pod。可以将 secret 存储在 Kubernetes API 中，并将它们挂载为文件，以供 Pod 使用，而无需直接连接到 Kubernetes。 secret 卷由 tmpfs（一个 RAM 支持的文件系统）支持，永远不会写入非易失性存储器。必须先在 Kubernetes API 中创建一个 secret，然后才能使用它。</p><p><a href="https://kubernetes.io/docs/concepts/configuration/secret/" target="_blank" rel="noopener">Secrets are described in more detail here.</a><br><a href="https://jimmysong.io/posts/kubernetes-secret-configuration/" target="_blank" rel="noopener">secret example</a></p><h4 id="nfs"><a class="markdownIt-Anchor" href="#nfs"></a> nfs</h4><p>nfs 卷允许 NFS（网络文件系统）共享挂载到容器中，与 emptyDir 不同的是，当删除 Pod 时 nfs 卷上的内容被保留，仅仅是卸载卷。与 cephfs 一样可以预先填充数据，并且可以在数据包之间&quot;切换&quot;数据。 nfs 能被多个写设备同时挂载。</p><p><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs" target="_blank" rel="noopener">NFS example</a></p><h4 id="projected"><a class="markdownIt-Anchor" href="#projected"></a> projected</h4><p><a href="https://kubernetes.io/docs/concepts/storage/volumes/#projected" target="_blank" rel="noopener">projected example</a></p><h4 id="rbd"><a class="markdownIt-Anchor" href="#rbd"></a> rbd</h4><p>rbd 卷允许将 Rados Block Device 卷挂载到容器中。与 emptyDir 不同的是，当删除 Pod 时 nfs 卷上的内容被保留，仅仅是卸载卷。与 cephfs 一样可以预先填充数据，并且可以在数据包之间&quot;切换&quot;数据。可以同时为多个用户以**<code>只读</code>**方式挂载，由单个用户以读写模式安装——不允许同时写入。</p><p><a href="https://github.com/kubernetes/examples/tree/master/staging/volumes/rbd" target="_blank" rel="noopener">RBD example</a></p><h2 id="持久化卷"><a class="markdownIt-Anchor" href="#持久化卷"></a> 持久化卷</h2><p>每个 PV 配置中都包含一个 sepc 规格字段和一个 status 卷状态字段。</p><h3 id="容量"><a class="markdownIt-Anchor" href="#容量"></a> 容量</h3><p>使用 PV 的容量属性设置存储容量。Kubernetes 资源模型<a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/resources.md" target="_blank" rel="noopener">capacity</a></p><h3 id="卷模式"><a class="markdownIt-Anchor" href="#卷模式"></a> 卷模式</h3><p>v1.9 之前，所有卷插件默认在持久卷上创建一个文件系统。在 v1.9 中，除文件系统之外，开始支持块设备。用户现在可以指定一个 volumeMode， volumeMode 的有效值可以是“Filesystem”或“Block”。如果未指定，volumeMode 将默认为“Filesystem”。</p><h3 id="访问模式"><a class="markdownIt-Anchor" href="#访问模式"></a> 访问模式</h3><p>PersistentVolume 以卷资源提供者支持的模式挂载到主机上，如下：</p><ul><li>（RWO）ReadWriteOnce——该卷可以被单个节点以读/写模式挂载</li><li>（ROX）ReadOnlyMany——该卷可以被多个节点以只读模式挂载</li><li>（RWX）ReadWriteMany——该卷可以被多个节点以读/写模式挂载</li></ul><blockquote><p><strong>!!!</strong> 一个卷一次只能使用一种访问模式挂载，即使它支持很多访问模式。例如，GCEPersistentDisk 可以由单个节点作为 ReadWriteOnce 模式挂载，或由多个节点以 ReadOnlyMany 模式挂载，但不能同时挂载。</p></blockquote><p>卷资源提供者支持的访问模式：</p><table><thead><tr><th style="text-align:left">Volume Plugin</th><th style="text-align:center">ReadWriteOnce</th><th style="text-align:center">ReadOnlyMany</th><th style="text-align:center">ReadWriteMany</th></tr></thead><tbody><tr><td style="text-align:left">AWSElasticBlockStore</td><td style="text-align:center">✓</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">AzureFile</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td></tr><tr><td style="text-align:left">AzureDisk</td><td style="text-align:center">✓</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">CephFS</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td></tr><tr><td style="text-align:left">Cinder</td><td style="text-align:center">✓</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">FC</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">FlexVolume</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">Flocker</td><td style="text-align:center">✓</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">GCEPersistentDisk</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">Glusterfs</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td></tr><tr><td style="text-align:left">HostPath</td><td style="text-align:center">✓</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">iSCSI</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">PhotonPersistentDisk</td><td style="text-align:center">✓</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">Quobyte</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td></tr><tr><td style="text-align:left">NFS</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td></tr><tr><td style="text-align:left">RBD</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">VsphereVolume</td><td style="text-align:center">✓</td><td style="text-align:center">-</td><td style="text-align:center">- (works when pods are collocated)</td></tr><tr><td style="text-align:left">PortworxVolume</td><td style="text-align:center">✓</td><td style="text-align:center">-</td><td style="text-align:center">✓</td></tr><tr><td style="text-align:left">ScaleIO</td><td style="text-align:center">✓</td><td style="text-align:center">✓</td><td style="text-align:center">-</td></tr><tr><td style="text-align:left">StorageOS</td><td style="text-align:center">✓</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr></tbody></table><h3 id="类"><a class="markdownIt-Anchor" href="#类"></a> 类</h3><p>PV 可以具有一个类，通过将 storageClassName 属性设置为 <a href="https://kubernetes.io/docs/concepts/storage/storage-classes/" target="_blank" rel="noopener">StorageClass</a> 的名称来指定该类。一个特定类别的 PV 只能绑定到请求该类别的 PVC。没有 storageClassName 的 PV 就没有类，它只能绑定到不需要特定类的 PVC。</p><h3 id="回收策略"><a class="markdownIt-Anchor" href="#回收策略"></a> 回收策略</h3><p>当前的回收策略包括：</p><ul><li>Retain（保留）——手动回收</li><li>Recycle（回收）——基本擦除（rm -rf /thevolume/*）</li><li>Delete（删除）——关联的存储资产（例如 AWS EBS、GCE PD、Azure Disk 和 OpenStack Cinder 卷）将被删除</li></ul><p>当前只有 NFS 和 HostPath 支持回收策略。AWS EBS、GCE PD、Azure Disk 和 Cinder 卷支持删除策略。</p><h3 id="挂载选项"><a class="markdownIt-Anchor" href="#挂载选项"></a> 挂载选项</h3><p>Kubernetes 管理员可以指定在节点上为挂载持久卷指定挂载选项，<strong>但不是所有的</strong>持久化卷类型都支持挂载选项。挂载选项没有校验，如果挂载选项无效则挂载失败。</p><h3 id="状态"><a class="markdownIt-Anchor" href="#状态"></a> 状态</h3><p>命令行会显示绑定到 PV 的 PVC 的名称。卷可以处于以下的某种状态：</p><ul><li>Available（可用）——一块空闲资源还没有被任何声明绑定</li><li>Bound（已绑定）——卷已经被声明绑定</li><li>Released（已释放）——声明被删除，但是资源还未被集群重新声明</li><li>Failed（失败）——该卷的自动回收失败</li></ul><h2 id="persistentvolumeclaim"><a class="markdownIt-Anchor" href="#persistentvolumeclaim"></a> PersistentVolumeClaim</h2><p>每个 PVC 中同样包含一个 sepc 规格字段和一个 status 声明状态字段。</p><h3 id="访问模式-2"><a class="markdownIt-Anchor" href="#访问模式-2"></a> 访问模式</h3><p>在请求具有特定访问模式的存储时，声明使用与卷相同的约定。</p><h3 id="卷模式-2"><a class="markdownIt-Anchor" href="#卷模式-2"></a> 卷模式</h3><p>声明使用与卷相同的约定，指示将卷作为文件系统或块设备使用。</p><h3 id="资源"><a class="markdownIt-Anchor" href="#资源"></a> 资源</h3><p>像 pod 一样，声明可以请求特定数量的资源。在这种情况下，请求是用于存储的。相同的资源模型适用于卷和声明。</p><h3 id="选择器"><a class="markdownIt-Anchor" href="#选择器"></a> 选择器</h3><p>声明可以指定一个标签选择器来进一步过滤该组卷。只有标签与选择器匹配的卷可以绑定到声明。选择器由两个字段组成：</p><ul><li>matchLabels：volume 必须有具有该值的标签</li><li>matchExpressions：这是一个要求列表，通过指定关键字，值列表以及与关键字和值相关的运算符组成。有效的运算符包括 In、NotIn、Exists 和 DoesNotExist。</li></ul><p>所有来自 matchLabels 和 matchExpressions 的要求都被&quot;与&quot;（and）——它们必须全部满足才能匹配。</p><h3 id="类-2"><a class="markdownIt-Anchor" href="#类-2"></a> 类</h3><p>声明可以通过使用属性 storageClassName 指定 StorageClass 的名称来请求特定的类。只有所请求的类与 PVC 具有相同 storageClassName 的 PV 才能绑定到 PVC。</p><p>PVC 不一定要请求类。其 storageClassName 设置为 “” 的 PVC 始终被解释为没有请求类的 PV，因此只能绑定到没有类的 PV（没有注解或 “”）。没有 storageClassName 的 PVC 根据是否打开DefaultStorageClass 准入控制插件，集群对其进行不同处理。</p><ul><li>如果打开了准入控制插件，管理员可以指定一个默认的 StorageClass。所有没有 StorageClassName 的 PVC 将被绑定到该默认的 PV。通过在 StorageClass 对象中将注解 <a href="http://storageclassclass.ubernetes.io/is-default-class" target="_blank" rel="noopener">storageclassclass.ubernetes.io/is-default-class</a> 设置为 “true” 来指定默认的 StorageClass。如果管理员没有指定缺省值，那么集群会响应 PVC 创建，就好像关闭了准入控制插件一样。如果指定了多个默认值，则准入控制插件将禁止所有 PVC 创建。</li><li>如果准入控制插件被关闭，则没有默认 StorageClass 的概念。所有没有 storageClassName 的 PVC 只能绑定到没有类的 PV。在这种情况下，没有 storageClassName 的 PVC 的处理方式与 storageClassName 设置为 “” 的 PVC 的处理方式相同。</li></ul><p>当 PVC 指定了 selector，除了请求一个 StorageClass 之外，这些需求被“与”在一起：只有被请求的类的 PV 具有和被请求的标签才可以被绑定到 PVC。</p><blockquote><p>目前具有非空 selector 的 PVC 不能为其动态配置 PV。</p></blockquote><h2 id="声明作为卷"><a class="markdownIt-Anchor" href="#声明作为卷"></a> 声明作为卷</h2><p>通过声明卷来访问存储。声明必须与使用声明的 pod 存在于相同的命名空间中。集群在 pod 的命名空间中查找声明，并使用它来获取支持声明的 PersistentVolume，该卷将被挂载到主机的 pod 上。</p><h3 id="命名空间注意点"><a class="markdownIt-Anchor" href="#命名空间注意点"></a> 命名空间注意点</h3><p>PersistentVolumes 绑定是唯一的，并且由于 PersistentVolumeClaims 是命名空间对象，因此只能在一个命名空间内挂载具有“多个”模式（ROX、RWX）的声明。</p><h2 id="原始块卷支持"><a class="markdownIt-Anchor" href="#原始块卷支持"></a> 原始块卷支持</h2><p>**<code>原始块卷的静态配置</code>**在 v1.9 中作为 alpha 功能引入。由于这个改变，需要一些新的 API 字段来使用该功能。目前，Fibre Channl 是支持该功能的唯一插件。</p><h3 id="原始块卷作为持久化卷"><a class="markdownIt-Anchor" href="#原始块卷作为持久化卷"></a> 原始块卷作为持久化卷</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: block-pv</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 10Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  volumeMode: Block</span><br><span class="line">  persistentVolumeReclaimPolicy: Retain</span><br><span class="line">  fc:</span><br><span class="line">    targetWWNs: [&quot;50060e801049cfd1&quot;]</span><br><span class="line">    lun: 0</span><br><span class="line">    readOnly: false</span><br></pre></td></tr></table></figure><h3 id="持久化卷声明请求原始块卷"><a class="markdownIt-Anchor" href="#持久化卷声明请求原始块卷"></a> 持久化卷声明请求原始块卷</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: block-pvc</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  volumeMode: Block</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 10Gi</span><br></pre></td></tr></table></figure><h3 id="pod-规格配置中为容器添加原始块设备"><a class="markdownIt-Anchor" href="#pod-规格配置中为容器添加原始块设备"></a> Pod 规格配置中为容器添加原始块设备</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-with-block-volume</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: fc-container</span><br><span class="line">      image: fedora:26</span><br><span class="line">      command: [&quot;/bin/sh&quot;, &quot;-c&quot;]</span><br><span class="line">      args: [ &quot;tail -f /dev/null&quot; ]</span><br><span class="line">      volumeDevices:</span><br><span class="line">        - name: data</span><br><span class="line">          devicePath: /dev/xvda</span><br><span class="line">  volumes:</span><br><span class="line">    - name: data</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: block-pvc</span><br></pre></td></tr></table></figure><blockquote><p>注意：在为 Pod 增加原始块设备时，在容器中指定设备路径而不是挂载路径。</p></blockquote><h3 id="绑定块卷"><a class="markdownIt-Anchor" href="#绑定块卷"></a> 绑定块卷</h3><p>如果用户通过使用 PersistentVolumeClaim 规范中的 volumeMode 字段来请求原始块卷，则绑定规则与之前不识别该模式规范的版本略有不同。<br>静态设置的卷的卷绑定矩阵：</p><table><thead><tr><th style="text-align:left">PV volumeMode</th><th style="text-align:left">PVC volumeMode</th><th style="text-align:right">Result</th></tr></thead><tbody><tr><td style="text-align:left">unspecified</td><td style="text-align:left">unspecified</td><td style="text-align:right">BIND</td></tr><tr><td style="text-align:left">unspecified</td><td style="text-align:left">Block</td><td style="text-align:right">NO BIND</td></tr><tr><td style="text-align:left">unspecified</td><td style="text-align:left">Filesystem</td><td style="text-align:right">BIND</td></tr><tr><td style="text-align:left">Block</td><td style="text-align:left">unspecified</td><td style="text-align:right">NO BIND</td></tr><tr><td style="text-align:left">Block</td><td style="text-align:left">Block</td><td style="text-align:right">BIND</td></tr><tr><td style="text-align:left">Block</td><td style="text-align:left">Filesystem</td><td style="text-align:right">NO BIND</td></tr><tr><td style="text-align:left">Filesystem</td><td style="text-align:left">Filesystem</td><td style="text-align:right">BIND</td></tr><tr><td style="text-align:left">Filesystem</td><td style="text-align:left">Block</td><td style="text-align:right">NO BIND</td></tr><tr><td style="text-align:left">Filesystem</td><td style="text-align:left">unspecified</td><td style="text-align:right">BIND</td></tr></tbody></table><h2 id="编写可移植配置"><a class="markdownIt-Anchor" href="#编写可移植配置"></a> 编写可移植配置</h2><p>持久卷配置模板规范：</p><ul><li>不要在配置组合中包含 PersistentVolumeClaim 对象（与 Deployment、ConfigMap等一起）。</li><li>不要在配置中包含 PersistentVolume 对象，用户实例化配置可能没有创建 PersistentVolume 的权限。</li><li>为用户在实例化模板时提供存储类名称的选项。</li><li>如果用户提供存储类名称，则将该值放入 persistentVolumeClaim.storageClassName 字段中。如果集群具有由管理员启用的 StorageClass，这将导致 PVC 匹配正确的存储类别。</li><li>如果用户未提供存储类名称，则将 persistentVolumeClaim.storageClassName 字段保留为 nil。</li><li>这将使用集群中默认的 StorageClass 为用户自动配置 PV。许多集群环境都有默认的 StorageClass，或者管理员可以创建自己的默认 StorageClass。</li><li>请注意集群中一段时间之后仍未绑定的 PVC，并向用户展示它们，因为这表示集群可能没有动态存储支持（在这种情况下用户应创建匹配的 PV），或集群没有存储系统（在这种情况下用户不能部署需要 PVC 的配置）。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;声明：本文来自&lt;a href=&quot;https://jimmysong.io/posts/kubernetes-persistent-volume/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;jimmysong&lt;/a&gt;的文章，整理记录在此仅为加深对 K8S PV 学习和理解。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在 K8S 中没有使用&lt;strong&gt;持久化卷&lt;/strong&gt;是不完整的，PV作为子系统为管理员和用户提供了API。&lt;/p&gt;
&lt;h2 id=&quot;apis&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#apis&quot;&gt;&lt;/a&gt; APIs&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;PersistentVolume&lt;/code&gt;（PV）管理员配置的存储，作为 K8S 群集的一部分。类似节点是集群中的资源一样，PV 也是集群中的资源。 PV 是 Volume 之类的卷插件，具有独立于 使用PV的Pod 的生命周期。卷的生命比 Pod 中的所有容器都长，当容器重启时数据仍然得以保存。只有当 Pod 消亡时，卷将不复存在。Kubernetes 支持多种类型的卷，Pod 可以同时使用任意数量的卷。该API对象包含存储实现的细节，即 NFS、RBD 或云厂商的存储系统。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PersistentVolumeClaim&lt;/code&gt;（PVC）用户对存储的请求，与 Pod 类似。Pod 消耗节点资源，PVC 消耗 PV 资源。PVC 请求特定的大小和访问模式。&lt;/p&gt;
&lt;h2 id=&quot;卷和声明claim的生命周期&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#卷和声明claim的生命周期&quot;&gt;&lt;/a&gt; 卷和声明(claim)的生命周期&lt;/h2&gt;
&lt;p&gt;PV作为集群中的一项资源，PVC消费PV资源，也作为对资源的消费检查。&lt;/p&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>NFS Share</title>
    <link href="https://acquaai.github.io/2018/03/10/nfs/"/>
    <id>https://acquaai.github.io/2018/03/10/nfs/</id>
    <published>2018-03-10T03:27:25.000Z</published>
    <updated>2019-08-28T14:00:52.043Z</updated>
    
    <content type="html"><![CDATA[<h2 id="server"><a class="markdownIt-Anchor" href="#server"></a> Server</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ setsebool -P virt_use_nfs 1 (OR SELinux status: disabled)</span><br><span class="line">$ yum -y install nfs-utils libnfsidmap</span><br><span class="line">$ systemctl <span class="built_in">enable</span> rpcbind nfs-server</span><br><span class="line">$ systemctl start rpcbind nfs-server rpc-statd nfs-idmapd</span><br><span class="line"></span><br><span class="line">$ cat /etc/expots</span><br><span class="line">/nfs/sonar/conf 10.0.77.0/27(rw,sync,no_root_squash)</span><br><span class="line">/nfs/sonar/data 10.0.77.0/27(rw,sync,no_root_squash)</span><br><span class="line">/nfs/sonar/extensions 10.0.77.0/27(rw,sync,no_root_squash)</span><br><span class="line">/nfs/sonar/logs 10.0.77.0/27(rw,sync,no_root_squash)</span><br><span class="line"></span><br><span class="line">$ chmod -R 777 /nfs</span><br><span class="line">$ exportfs -r</span><br></pre></td></tr></table></figure><a id="more"></a><blockquote><ul><li>rw: 该主机对该共享目录有读写权限</li><li>sync: 资料同步写入到内存与硬盘中</li><li>no_root_squash: 客户机用root访问该共享文件夹时，不映射root用户</li><li>root_squash: 客户机用root用户访问该共享文件夹时，将root用户映射成匿名用户</li><li>no_subtree_check: 不检查父目录权限</li><li>subtree_check: 如果共享/usr/bin之类的子目录时，强制NFS检查父目录的权限（默认）</li><li>anonuid: 将客户机上的用户映射成指定的本地用户ID的用户</li><li>anongid: 将客户机上的用户映射成属于指定的本地用户组ID</li><li>secure: NFS通过1024以下的安全TCP/IP端口发送</li><li>insecure: NFS通过1024以上的端口发送</li><li>wdelay: 如果多个用户要写入NFS目录，则归组写入（默认）</li><li>no_wdelay: 如果多个用户要写入NFS目录，则立即写入，当使用async时，无需此设置</li></ul></blockquote><h2 id="client"><a class="markdownIt-Anchor" href="#client"></a> Client</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ yum -y install nfs-utils</span><br><span class="line">$ systemctl start rpcbind.service</span><br><span class="line"></span><br><span class="line">$ showmount -e 10.0.77.16</span><br><span class="line">Export list <span class="keyword">for</span> 10.0.77.16:</span><br><span class="line">/nfs/sonar/logs       10.0.77.0/27</span><br><span class="line">/nfs/sonar/extensions 10.0.77.0/27</span><br><span class="line">/nfs/sonar/data       10.0.77.0/27</span><br><span class="line">/nfs/sonar/conf       10.0.77.0/27</span><br><span class="line"></span><br><span class="line">$ mount -t nfs 10.0.77.16:/nfs/sonar/logs /nfs</span><br><span class="line"></span><br><span class="line"><span class="comment">#NFS默认是用UDP协议，使用TCP协议传输更稳定</span></span><br><span class="line">$ mount -t nfs 10.0.77.16:/nfs/sonar/logs /nfs -o proto=tcp -o nolock</span><br><span class="line"></span><br><span class="line"><span class="comment">#开机自动挂载</span></span><br><span class="line">$ cat /etc/fstab  </span><br><span class="line">10.0.77.16:/nfs/sonar/logs    /nfs    nfs4 rw,hard,intr,proto=tcp,port=2049,noauto    0  0</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;server&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#server&quot;&gt;&lt;/a&gt; Server&lt;/h2&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ setsebool -P virt_use_nfs 1 (OR SELinux status: disabled)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ yum -y install nfs-utils libnfsidmap&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ systemctl &lt;span class=&quot;built_in&quot;&gt;enable&lt;/span&gt; rpcbind nfs-server&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ systemctl start rpcbind nfs-server rpc-statd nfs-idmapd&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ cat /etc/expots&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/nfs/sonar/conf 10.0.77.0/27(rw,sync,no_root_squash)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/nfs/sonar/data 10.0.77.0/27(rw,sync,no_root_squash)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/nfs/sonar/extensions 10.0.77.0/27(rw,sync,no_root_squash)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/nfs/sonar/logs 10.0.77.0/27(rw,sync,no_root_squash)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ chmod -R 777 /nfs&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ exportfs -r&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>VNC Server</title>
    <link href="https://acquaai.github.io/2018/03/10/vnc/"/>
    <id>https://acquaai.github.io/2018/03/10/vnc/</id>
    <published>2018-03-10T03:20:21.000Z</published>
    <updated>2019-08-28T14:00:52.044Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ yum -y install tigervnc-server</span><br><span class="line">$ su - acqua</span><br><span class="line">$ vncpasswd</span><br><span class="line">$ cp /lib/systemd/system/vncserver@.service  /etc/systemd/system/vncserver@:1.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># port 5900+display</span></span><br><span class="line">$ vi /etc/systemd/system/vncserver@\:1.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># Quick HowTo:</span></span><br><span class="line"><span class="comment"># 1. Copy this file to /etc/systemd/system/vncserver@.service</span></span><br><span class="line"><span class="comment"># 2. Replace &lt;USER&gt; with the actual user name and edit vncserver</span></span><br><span class="line"><span class="comment">#    parameters appropriately</span></span><br><span class="line"><span class="comment">#   ("User=&lt;USER&gt;" and "/home/&lt;USER&gt;/.vnc/%H%i.pid")</span></span><br><span class="line"><span class="comment"># 3. Run `systemctl daemon-reload`</span></span><br><span class="line"><span class="comment"># 4. Run `systemctl enable vncserver@:&lt;display&gt;.service`</span></span><br></pre></td></tr></table></figure><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=Remote desktop service (VNC)</span><br><span class="line">After=syslog.target network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=forking</span><br><span class="line">User=acqua</span><br><span class="line"></span><br><span class="line">ExecStartPre=/bin/sh -c <span class="string">'/usr/bin/vncserver -kill %i &gt; /dev/null 2&gt;&amp;1 || :'</span></span><br><span class="line">ExecStart=/sbin/runuser -l acqua -c <span class="string">"/usr/bin/vncserver %i -geometry 1280x1024"</span></span><br><span class="line">PIDFile=/home/acqua/.vnc/%H%i.pid</span><br><span class="line">ExecStop=/bin/sh -c <span class="string">'/usr/bin/vncserver -kill %i &gt; /dev/null 2&gt;&amp;1 || :'</span></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br><span class="line">$ systemctl daemon-reload</span><br><span class="line">$ systemctl start vncserver@:1</span><br><span class="line">$ systemctl status vncserver@:1</span><br><span class="line">$ systemctl <span class="built_in">enable</span> vncserver@:1</span><br><span class="line">$ ss -nultp |grep vnc</span><br><span class="line"></span><br><span class="line">$ firewall-cmd --add-port=5901/tcp</span><br><span class="line">$ firewall-cmd --add-port=5901/tcp --permanent</span><br></pre></td></tr></table></figure><p>VNC Viewer -&gt; IP:5901 来访问。</p>]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ yum -y install tigervnc-server&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ su - acqua&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ vncpasswd&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ cp /lib/systemd/system/vncserver@.service  /etc/systemd/system/vncserver@:1.service&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# port 5900+display&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ vi /etc/systemd/system/vncserver@\:1.service&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Quick HowTo:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 1. Copy this file to /etc/systemd/system/vncserver@.service&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 2. Replace &amp;lt;USER&amp;gt; with the actual user name and edit vncserver&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#    parameters appropriately&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#   (&quot;User=&amp;lt;USER&amp;gt;&quot; and &quot;/home/&amp;lt;USER&amp;gt;/.vnc/%H%i.pid&quot;)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 3. Run `systemctl daemon-reload`&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 4. Run `systemctl enable vncserver@:&amp;lt;display&amp;gt;.service`&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>K8S集群部署</title>
    <link href="https://acquaai.github.io/2018/02/09/k8s-cluster/"/>
    <id>https://acquaai.github.io/2018/02/09/k8s-cluster/</id>
    <published>2018-02-09T11:21:59.000Z</published>
    <updated>2019-08-28T15:18:19.775Z</updated>
    
    <content type="html"><![CDATA[<h2 id="what-is-kubernetes"><a class="markdownIt-Anchor" href="#what-is-kubernetes"></a> What is Kubernetes?</h2><p><a href="https://kubernetes.io" target="_blank" rel="noopener">Kubernetes</a>是一个为容器应用自动部署、扩展和管理的开源系统（容器编排系统）。</p><p><strong>Architecture</strong></p><p><img src="/images/k8s-arch.png" alt></p><a id="more"></a><p><a href="https://github.com/acquaai/Kubernetes/blob/master/k8s-cluster-with-binary/k8s-with-binary.md" target="_blank" rel="noopener">K8S集群部署完整文档</a></p><p><strong>Reference</strong><br><a href="https://jimmysong.io/posts/kubernetes-installation-document/" target="_blank" rel="noopener">Jimmy Song的博客</a><br><a href="https://blog.frognew.com/2017/04/install-ha-kubernetes-1.6-cluster.html#13-etcd%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2" target="_blank" rel="noopener">青蛙小白的博客</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;what-is-kubernetes&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#what-is-kubernetes&quot;&gt;&lt;/a&gt; What is Kubernetes?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://kubernetes.io&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kubernetes&lt;/a&gt;是一个为容器应用自动部署、扩展和管理的开源系统（容器编排系统）。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/k8s-arch.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>Kubernetes Security</title>
    <link href="https://acquaai.github.io/2018/02/09/k8s-sec1/"/>
    <id>https://acquaai.github.io/2018/02/09/k8s-sec1/</id>
    <published>2018-02-09T01:27:33.000Z</published>
    <updated>2019-08-28T14:00:52.041Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Reference</strong></p><ol><li><a href="https://docs.docker.com/engine/security/security/" target="_blank" rel="noopener">Docker security</a></li><li><a href="http://www.freebuf.com/column/152398.html" target="_blank" rel="noopener">Docker容器全面的安全防护</a></li></ol><h2 id="docker宿主机安全"><a class="markdownIt-Anchor" href="#docker宿主机安全"></a> Docker宿主机安全</h2><ul><li>Minimal Install</li><li>sudo yum update -y</li><li>按需安装附加包/服务</li></ul><a id="more"></a><ul><li>SSH Service</li></ul><p><strong>采用认证登录</strong><br>认证服务器(C端)生成一对公/私钥，私钥放在C端，公钥上传到S端。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa</span><br><span class="line"></span><br><span class="line">[dockeruser@repo ~]$ mkidr ~/.ssh</span><br><span class="line">[dockeruser@repo ~]$ sudo chmod 700 ~/.ssh</span><br><span class="line"></span><br><span class="line">$ scp ~/.ssh/id_rsa.pub dockeruser@10.0.77.16:~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p><strong>sshd_config</strong><br>SSH禁用root登录和使用密码的身份验证。</p><p>PermitRootLogin yes =&gt; no<br>PasswordAuthentication yes =&gt; no</p><p><strong>使用非<code>:22</code>默认端口</strong></p><ul><li>关闭无用服务/端口</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nmap -sU -sS -p 1-65535 localhost</span><br><span class="line"></span><br><span class="line">Starting Nmap 6.40 ( http://nmap.org ) at 2018-02-09 09:21 CST</span><br><span class="line">Nmap scan report <span class="keyword">for</span> localhost (127.0.0.1)</span><br><span class="line">Host is up (0.0000060s latency).</span><br><span class="line">Other addresses <span class="keyword">for</span> localhost (not scanned): 127.0.0.1</span><br><span class="line">Not shown: 131068 closed ports</span><br><span class="line">PORT     STATE SERVICE</span><br><span class="line">25/tcp   open  smtp</span><br><span class="line">8086/tcp open  d<span class="_">-s</span>-n</span><br><span class="line"></span><br><span class="line">Nmap <span class="keyword">done</span>: 1 IP address (1 host up) scanned <span class="keyword">in</span> 1.74 seconds</span><br><span class="line"></span><br><span class="line">$ sudo systemctl stop postfix</span><br><span class="line">$ sudo yum remove postfix</span><br></pre></td></tr></table></figure><h2 id="docker安全"><a class="markdownIt-Anchor" href="#docker安全"></a> Docker安全</h2><h3 id="api"><a class="markdownIt-Anchor" href="#api"></a> API</h3><ul><li><p><a href="https://www.kubernetes.org.cn/1995.html" target="_blank" rel="noopener">APIServer认证、授权、准入控制</a></p></li><li><p><a href="https://www.kubernetes.org.cn/2781.html" target="_blank" rel="noopener">NetworkPolicy</a></p></li></ul><p>默认情况下，K8S没有限制集群内Pod间的通信，NetworkPolicy于Pod间创建防火墙。</p><h3 id="运行参数"><a class="markdownIt-Anchor" href="#运行参数"></a> 运行参数</h3><ul><li><p><code>-default-ulimit</code><br>限制进程和文件数 -default-ulimit nproc=X:X -default-ulimit nofile=X:X</p></li><li><p><code>-icc=false/true</code><br>管理容器间通信，建议设置为 false，使用<code>--link</code> 参数允许容器间通信。</p></li><li><p><code>-iptables=true</code><br>启用iptables规则</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://docs.docker.com/engine/security/security/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Docker security&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.freebuf.com/column/152398.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Docker容器全面的安全防护&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;docker宿主机安全&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#docker宿主机安全&quot;&gt;&lt;/a&gt; Docker宿主机安全&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Minimal Install&lt;/li&gt;
&lt;li&gt;sudo yum update -y&lt;/li&gt;
&lt;li&gt;按需安装附加包/服务&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
  <entry>
    <title>CI/CDs</title>
    <link href="https://acquaai.github.io/2018/02/03/ci-cd/"/>
    <id>https://acquaai.github.io/2018/02/03/ci-cd/</id>
    <published>2018-02-03T08:35:11.000Z</published>
    <updated>2019-08-28T14:00:52.037Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>持续集成（Continuous Integration）的目的是让产品可以快速迭代，同时还能保持高质量。核心措施是代码集成到主干之前，必须通过自动化测试。只要有一个测试用例失败，就不能集成。优点就是能快速发现错误，防止分支大幅偏离主干。</p></li><li><p>持续交付（Continuous Delivery）是指频繁的将软件的新版本交付给 QA或用户 以供评审。如果评审通过，代码就进入生产阶段。持续交付可以看作持续集成的下一步。它强调的是，不管怎么更新软件是随时随地可以交付的。</p></li><li><p>持续部署（Continuous Deployment）是持续交付的下一步，指的是代码通过评审以后，自动部署到生产环境。持续部署的目标是，代码在任何时刻都是可部署的，可以进入生产阶段。持续部署的前提是能自动化完成测试、构建、部署等步骤。</p></li></ul><a id="more"></a><p>持续部署与持续交付的区别如下图所示：<a href="http://www.ruanyifeng.com/blog/2015/09/continuous-integration.html" target="_blank" rel="noopener">- - -摘自阮一峰老师日志</a><br><img src="/images/k_CD.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;
&lt;p&gt;持续集成（Continuous Integration）的目的是让产品可以快速迭代，同时还能保持高质量。核心措施是代码集成到主干之前，必须通过自动化测试。只要有一个测试用例失败，就不能集成。优点就是能快速发现错误，防止分支大幅偏离主干。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;持续交付（Continuous Delivery）是指频繁的将软件的新版本交付给 QA或用户 以供评审。如果评审通过，代码就进入生产阶段。持续交付可以看作持续集成的下一步。它强调的是，不管怎么更新软件是随时随地可以交付的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;持续部署（Continuous Deployment）是持续交付的下一步，指的是代码通过评审以后，自动部署到生产环境。持续部署的目标是，代码在任何时刻都是可部署的，可以进入生产阶段。持续部署的前提是能自动化完成测试、构建、部署等步骤。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="DevOps" scheme="https://acquaai.github.io/categories/DevOps/"/>
    
    
  </entry>
  
</feed>
